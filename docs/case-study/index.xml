<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Case-studies on Bruno Nicenboim</title>
    <link>/case-study/</link>
    <description>Recent content in Case-studies on Bruno Nicenboim</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 14 Jun 2025 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/case-study/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Case study: Mispecified models of reaction times and choice</title>
      <link>/case-study/mispecification/mispecified/</link>
      <pubDate>Sat, 14 Jun 2025 00:00:00 +0000</pubDate>
      
      <guid>/case-study/mispecification/mispecified/</guid><description>


&lt;pre class=&#34;r fold-hide&#34;&gt;&lt;code&gt;papaja::r_refs(&amp;quot;r-references.bib&amp;quot;)

## Global options
options(max.print = &amp;quot;75&amp;quot;,
        width = 80,
        tibble.width = 80)
options(scipen = 1, digits = 2)
knitr::opts_chunk$set(echo = TRUE,
                    cache = TRUE,
                    prompt = FALSE,
                    cache.lazy = FALSE,
                    tidy = FALSE,
                    comment = NA,
                    message = FALSE,
                    warning = TRUE)
knitr::opts_knit$set(width = 80)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Packages in use
library(rtdists)
library(tidytable) # faster replacement of dplyr
library(ggplot2)
library(latex2exp) # for math symbols in ggplots
library(rstan)
library(bayesplot)
library(posterior)
library(bridgesampling)
library(loo)
options(mc.cores = parallel::detectCores())
source(&amp;quot;aux.R&amp;quot;)
# Plots
bayesplot_theme_set(theme_light())
theme_set(theme_light())
theme_update(plot.title = element_text(hjust = 0.5))
options(ggplot2.continuous.colour = scale_color_viridis_c)
options(ggplot2.continuous.fill = scale_fill_viridis_c)
options(ggplot2.discrete.colour = scale_color_viridis_d)
options(ggplot2.discrete.fill = scale_fill_viridis_d)
color_scheme_set(&amp;quot;viridis&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To illustrate the effect of mispecified models in model comparison, I will use simulated response times and choice data. These data could be from an experiment representing either a motion detection task, where participants decide the direction of moving dots, or a lexical decision task, which involves determining whether letter strings are real words or nonsensical sequences.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;TL;DR;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;For both BF and CV:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A model that is closer to the truth is not necessarily the one with the best predictions.&lt;/li&gt;
&lt;li&gt;A flexible theory-agnostic model might yield the best predictions even if it doesn’t resemble the generative process of the data.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For BF:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The comparison of (cognitive) models that entail very different generative processes but can mimic the data well (for different reasons) is strongly prior dependent.&lt;/li&gt;
&lt;li&gt;Even the selection of a model that relatively closely resembles the data generative process and has a better fit to the data can be (but not always) also strongly prior dependent.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For CV:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Unless there is a clear gain in predictions, CV will be undecided. This is regardless of how close a model is to the true generative process and how good the fit is.&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;true-data-generating-process-with-the-linear-ballistic-accumulator-model&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;True data generating process with the Linear Ballistic Accumulator Model&lt;/h1&gt;
&lt;p&gt;I’m going to assume that the true generative model for this simulated experiment is a Linear Ballistic Accumulator model &lt;span class=&#34;citation&#34;&gt;(LBA: &lt;a href=&#34;/case-study/mispecification/mispecified/#ref-brownSimplestCompleteModel2008&#34;&gt;Brown and Heathcote 2008&lt;/a&gt;)&lt;/span&gt; with a Gamma distribution for the accumulation rates &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;/case-study/mispecification/mispecified/#ref-Terry2015&#34;&gt;Terry et al. 2015&lt;/a&gt;)&lt;/span&gt;. The LBA model conceptualizes decision-making as a race among several accumulators, each gathering evidence for a different decision option. The first accumulator to reach a predetermined threshold dictates the decision.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:drawing&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;/case-study/mispecification/mispecified//case-study/mispecification/mispecified_files/figure-html/drawing-1.png&#34; alt=&#34;The figure depicts the parameters for one linear ballistic accumulator.&#34; width=&#34;672&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: The figure depicts the parameters for one linear ballistic accumulator.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The key parameters of the model are the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(v_i\)&lt;/span&gt;: The rate of evidence accumulation (drift rate) for choice &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;. This parameter represents how quickly evidence supporting choice &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; is gathered.&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(p_i\)&lt;/span&gt;: The starting point of evidence accumulation for choice &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;. This parameter accounts for any initial bias or prior evidence in favor of choice &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;: Time since the start of the decision process.&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(b_i\)&lt;/span&gt;: The evidence threshold that must be reached for decision &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; to be made. Once the accumulated evidence &lt;span class=&#34;math inline&#34;&gt;\(d_i(t)\)&lt;/span&gt; for any choice &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; exceeds &lt;span class=&#34;math inline&#34;&gt;\(b_i\)&lt;/span&gt;, a decision is made in favor of that choice.&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(D_i(t)\)&lt;/span&gt;: The amount of evidence accumulated for choice &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; at time &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;, defined as &lt;span class=&#34;math inline&#34;&gt;\(D_i(t) = v_i \cdot t + p_i\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The LBA model assumes that the rates of evidence accumulation (&lt;span class=&#34;math inline&#34;&gt;\(v_i\)&lt;/span&gt;) for different choices are independent and can vary between trials. The model also allows for variability in the starting points (&lt;span class=&#34;math inline&#34;&gt;\(p_i\)&lt;/span&gt;), reflecting differences in initial bias or predisposition towards certain choices.&lt;/p&gt;
&lt;div id=&#34;true-values-for-the-parameters&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;True values for the Parameters&lt;/h2&gt;
&lt;p&gt;Below, I define the true values for the model’s parameters under four distinct conditions.&lt;/p&gt;
&lt;pre class=&#34;r fold-hide&#34;&gt;&lt;code&gt;df_pars &amp;lt;- tribble(
  ~difficulty,    ~emphasis,   ~A,   ~b, ~scale_v1, ~shape_v1, ~scale_v2, ~shape_v2, ~t0,
   &amp;quot;easy&amp;quot;,         &amp;quot;accuracy&amp;quot;,  .5,   5.1,   5,       6,         12,       1,         .1,
   &amp;quot;hard&amp;quot;,         &amp;quot;accuracy&amp;quot;,  .5,   5.1,  4.2,      6,         12,       1,         .1,
   &amp;quot;easy&amp;quot;,         &amp;quot;speed&amp;quot;,    3.9,   4.1,   5,       6,         12,       1,         .1,
   &amp;quot;hard&amp;quot;,         &amp;quot;speed&amp;quot;,    3.9,   4.1,  4.2,      6,         12,       1,         .1
)
df_pars&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tidytable: 4 × 9
  difficulty emphasis     A     b scale_v1 shape_v1 scale_v2 shape_v2    t0
  &amp;lt;chr&amp;gt;      &amp;lt;chr&amp;gt;    &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
1 easy       accuracy   0.5   5.1      5          6       12        1   0.1
2 hard       accuracy   0.5   5.1      4.2        6       12        1   0.1
3 easy       speed      3.9   4.1      5          6       12        1   0.1
4 hard       speed      3.9   4.1      4.2        6       12        1   0.1&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;simulated-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Simulated data&lt;/h2&gt;
&lt;p&gt;I generate data from a LBA with Gamma likelihood using the &lt;code&gt;rtdists&lt;/code&gt; package. The differences in emphasis do not alter the parameters that relate to the speed or noise in the accumulation process. Instead, they affect the parameters that control the distance that needs to be accumulated. This is achieved by increasing the likelihood that the initial position (&lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;) is near the threshold (by increasing &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;) and by decreasing the distance to the threshold (&lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt;). In datasets like this, the emphasis on accuracy could be the result of instructions from the experimenter or the intrinsic motivation of the participant.&lt;/p&gt;
&lt;pre class=&#34;r fold-hide&#34;&gt;&lt;code&gt;set.seed(123)
N_cond &amp;lt;- 400
df_sim &amp;lt;- df_pars |&amp;gt;
  pmap_df(function(A,b,t0, scale_v1, scale_v2, shape_v1, shape_v2, difficulty, emphasis, ...) {
    rLBA(N_cond, A = A, b = b, t0 = t0,
         scale_v = c(scale_v1, scale_v2),
         shape_v = c(shape_v1, shape_v2),
         distribution = &amp;quot;gamma&amp;quot; ) |&amp;gt;
      mutate(rt = rt * 1000, # to ms
             difficulty = difficulty,
             emphasis = emphasis)
  }) |&amp;gt; mutate(diff = ifelse(difficulty == &amp;quot;hard&amp;quot;,1,-1),
               emph = ifelse(emphasis ==&amp;quot;speed&amp;quot;,1,-1),
               resp = ifelse(response ==1, &amp;quot;correct&amp;quot;,&amp;quot;incorrect&amp;quot;),
               acc= ifelse(response ==1, 1, 0))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The simulated data mimic several patterns observed in real-world data, including (i) a predominance of correct over incorrect responses, (ii) a positive skew in response times, (iii) the standard deviation of response times increasing alongside the mean, and (iv) an increase in difficulty resulting in both more incorrect responses and prolonged response times. Additionally, it reflects a speed-accuracy trade-off, where faster decisions tend to be less accurate and slower decisions are more accurate. When speed is emphasized, response times for errors are shorter compared to when accuracy is emphasized, attributable to the shorter distance to the decision threshold.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_sim |&amp;gt; summarize(correct = mean(response==1),
                    rt_correct = mean(rt[response==1]),
                    sd_correct = sd(rt[response==1]),
                    rt_incorrect = mean(rt[response==2]),
                    sd_incorrect = sd(rt[response==2]),
                    .by = c(&amp;quot;emphasis&amp;quot;,&amp;quot;difficulty&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tidytable: 4 × 7
  emphasis difficulty correct rt_correct sd_correct rt_incorrect sd_incorrect
  &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;        &amp;lt;dbl&amp;gt;      &amp;lt;dbl&amp;gt;      &amp;lt;dbl&amp;gt;        &amp;lt;dbl&amp;gt;        &amp;lt;dbl&amp;gt;
1 accuracy easy         0.89        281.       79.0         305.        112. 
2 accuracy hard         0.848       311.       95.0         323.        145. 
3 speed    easy         0.805       179.       61.2         164.         47.4
4 speed    hard         0.78        184.       58.2         171.         50.3&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(df_sim, aes(x = rt, y = resp)) + 
  geom_violin(draw_quantiles = c(.025,.5,.975), scale = &amp;quot;count&amp;quot;, alpha = .5) +
  geom_jitter(alpha = .2, width = 0, height = .25) +
  facet_grid(emphasis ~ difficulty) +
  xlab(&amp;quot;Response time [ms]&amp;quot;) +
  ylab(&amp;quot;Accuracy&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/case-study/mispecification/mispecified//case-study/mispecification/mispecified_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;case-1.-speed-emphasis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Case 1. Speed emphasis&lt;/h2&gt;
&lt;p&gt;Next, I investigate what would happen if all the data obtained is from a participant who emphazises speed.&lt;/p&gt;
&lt;pre class=&#34;r fold-hide&#34;&gt;&lt;code&gt;set.seed(123)
df_sim_speed &amp;lt;- df_sim |&amp;gt;
  filter(emphasis == &amp;quot;speed&amp;quot;) |&amp;gt;
  group_by(difficulty) |&amp;gt;
  mutate(train = rbinom(n(), 1, .9))
df_sim_speed_train &amp;lt;-  df_sim_speed |&amp;gt;
  filter(train == 1)
dsim_speed_list &amp;lt;- list(N = nrow(df_sim_speed_train),
                  rt =df_sim_speed_train$rt,
                  response = df_sim_speed_train$response,
                  K = 1,
                  X = model.matrix(~ 0 + diff, df_sim_speed_train),
                  only_prior = 0)

fits_speed &amp;lt;- list()&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;case-1.a.-log-normal-race-model-vs.-theory-agnostic-models&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Case 1.a. Log-Normal race model vs. theory-agnostic models&lt;/h3&gt;
&lt;p&gt;I start with three models under consideration:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1. Log-Normal Race (LNR) model&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Similarly to the LBA, the LNR model describes the decision-making process as a race between several accumulators, each representing a different decision alternative &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;/case-study/mispecification/mispecified/#ref-HeathcoteLove2012&#34;&gt;Heathcote and Love 2012&lt;/a&gt;; &lt;a href=&#34;/case-study/mispecification/mispecified/#ref-RouderEtAl2015&#34;&gt;Rouder et al. 2015&lt;/a&gt;)&lt;/span&gt;. The time it takes for each accumulator to reach a decision threshold is assumed to follow a log-normal distribution. Unlike the LBA, there is no moving starting point, and all the accumulators start from the same initial point. Furthermore, the threshold and accumulation rate cannot be disentangled: a manipulation that affects the rate or the decision threshold will affect the location of the distribution in the same way &lt;span class=&#34;citation&#34;&gt;(also see &lt;a href=&#34;/case-study/mispecification/mispecified/#ref-RouderEtAl2015&#34;&gt;Rouder et al. 2015&lt;/a&gt;)&lt;/span&gt;. Another important observation is that the decision time (&lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt;) won’t have a log-normal distribution when the distance to the threshold is not log-normally distributed or constant.&lt;/p&gt;
&lt;p&gt;Following &lt;span class=&#34;citation&#34;&gt;Rouder et al. (&lt;a href=&#34;/case-study/mispecification/mispecified/#ref-RouderEtAl2015&#34;&gt;2015&lt;/a&gt;)&lt;/span&gt;, we assume that the noise parameter is the same for each accumulator, since this means that contrasts between finishing time distributions are captured completely by contrasts of the locations of the log-normal distributions.&lt;/p&gt;
&lt;!-- In a race of accumulators model, the assumption is that the time $T$ taken for each accumulator $i$ of evidence to reach the threshold at distance $D$ is simply defined by --&gt;
&lt;!-- \begin{equation} --&gt;
&lt;!-- t_i = d_i/v_i --&gt;
&lt;!-- \end{equation} --&gt;
&lt;!-- where the denominator vV$ is the rate (velocity, sometimes also called  drift rate) of  evidence accumulation. --&gt;
&lt;!-- The log-normal race model assumes that the  rate in each trial is sampled from a log-normal distribution: --&gt;
&lt;!-- \begin{equation} --&gt;
&lt;!-- V_i \sim \mathit{LogNormal}(\mu_vi, \sigma_vi) --&gt;
&lt;!-- \end{equation} --&gt;
&lt;!-- The observed reaction time corresponds to the sum of a non decision time $T_{0}$ --&gt;
&lt;!-- and the decision time which is the shortest time taken for an accumulator. The choice made corresponds to the accumulator that reach the threshold faster. --&gt;
&lt;span class=&#34;math display&#34;&gt;\[\begin{aligned}
\mu_1 &amp;amp;= \alpha_1 \cdot \text{diff}_n \cdot \beta_1\\
\mu_2 &amp;amp;= \alpha_2 \cdot \text{diff}_n \cdot \beta_2
\end{aligned}\]&lt;/span&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
(rt_n; \text{response}_n ) \sim \text{LNR}(\{\mu_1;\mu_2\}, \sigma, T_0)
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;with &lt;span class=&#34;math inline&#34;&gt;\(\text{diff}\)&lt;/span&gt; sum coded to &lt;span class=&#34;math inline&#34;&gt;\(-1\)&lt;/span&gt; for easy and &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; for hard.&lt;/p&gt;
&lt;p&gt;For this model, I set the following (very weak and not that good) priors:&lt;/p&gt;
&lt;span class=&#34;math display&#34;&gt;\[\begin{aligned}
\alpha &amp;amp;\sim \text{Normal}(0, 100) \\
\beta &amp;amp;\sim \text{Normal}(0, 10) \\
\sigma &amp;amp;\sim \text{Normal}_+(0, 100) \\
T_{0} &amp;amp;\sim \text{Normal}(150, 100) \quad \text{with } T_0 &amp;gt; \min(rt)
\end{aligned}\]&lt;/span&gt;
&lt;p&gt;This is implemented in Stan &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;/case-study/mispecification/mispecified/#ref-Stan2023&#34;&gt;Stan Development Team 2023b&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;stan&#34;&gt;&lt;code&gt;functions{
  real lognormalrace2_lpdf(real rt, int response,
                           real T_nd,
                           array[] real mu,
                           real sigma){
    real T = rt - T_nd;
    real lp = 0;
    if(response==1)
      lp += lognormal_lpdf(T | mu[1], sigma) +
        lognormal_lccdf(T | mu[2], sigma);
    else
      lp += lognormal_lpdf(T | mu[2], sigma) +
        lognormal_lccdf(T | mu[1], sigma);
    return lp;
  }
  array[] real  lognormalrace2_rng(real T_nd,
                          array[] real mu,
                          real sigma){
    real rt1 = lognormal_rng(mu[1], sigma);
    real rt2 = lognormal_rng(mu[2], sigma);
    array[2] real out;
    if(rt1 &amp;lt; rt2)
      out = {rt1 + T_nd, 1.0};
    else
      out = {rt2 + T_nd, 2.0};
    return out;
      }
}
data {
  int&amp;lt;lower = 1&amp;gt; N;
  int&amp;lt;lower = 1&amp;gt; K;
  matrix[N, K] X;
  vector[N] rt; //ms
  array[N] int response;
  int only_prior;
}
parameters {
  array[2] real alpha;
  array[2] vector[K] beta;
  real&amp;lt;lower = 0&amp;gt; sigma;
  real&amp;lt;lower = 0, upper = min(rt)&amp;gt; T_nd;
}
transformed parameters{
  array[N] real log_lik;
  if(only_prior != 1)
    for(n in 1:N){
      log_lik[n] = lognormalrace2_lpdf(rt[n]| response[n],
                                       T_nd,
                                       {alpha[1] + X[n] * beta[1],
                                        alpha[2] + X[n] * beta[2]},
                                       sigma);
    }
}
model {
  target += normal_lpdf(alpha | 0, 100);
  for(k in 1:K) target += normal_lpdf(beta[k] | 0, 10);
  target += normal_lpdf(sigma | 0, 100)
    - normal_lccdf(0 | 0, 100);
  target += normal_lpdf(T_nd | 150, 100)
    - log_diff_exp(normal_lcdf(min(rt) | 150, 100),
                   normal_lcdf(0 | 150, 100));
  if(only_prior!=1)
    target += sum(log_lik);
}
generated quantities {
  array[N] real pred_rt;
  array[N] int pred_response;
  for(n in 1:N){
    array[2] real out;
    out = lognormalrace2_rng(T_nd,
                             {alpha[1] + X[n] * beta[1],
                              alpha[2] + X[n] * beta[2]},
                             sigma);
    pred_rt[n] = out[1];
    pred_response[n] = to_int(out[2]);
    }
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fits_speed$LNRace &amp;lt;- stan(&amp;quot;./LogNormalRace_badpriors.stan&amp;quot;,
                          data = dsim_speed_list,
                          warmup = 1000,
                          iter = 10000)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print_model(fits_speed$LNRace)   &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 6 × 7
  variable      mean `2.5%` `97.5%`  rhat ess_bulk ess_tail
  &amp;lt;chr&amp;gt;        &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;
1 alpha[1]   4.4      4.3     4.5     1.0   18019.   19575.
2 alpha[2]   5.1      5.0     5.2     1.0   32999.   24096.
3 beta[1,1]  0.046   -0.017   0.11    1.0   33745.   23947.
4 beta[2,1] -0.00022 -0.085   0.084   1.0   31334.   22407.
5 sigma      0.82     0.73    0.91    1.0   16692.   18267.
6 T_nd      94.      88.     99.      1.0   15933.   17341.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;2. Theory-agnostic model&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This model simultaneously addresses response time and binary choice data without any commitment to any theory. It incorporates predictors to explain variability in both outcomes, treating response times as normally distributed and choices as following a Bernoulli distribution.&lt;/p&gt;
&lt;span class=&#34;math display&#34;&gt;\[\begin{aligned}
rt &amp;amp;\sim \text{Normal}(\alpha + \text{diff} \cdot \beta_1, \sigma)\\
acc &amp;amp;\sim \text{Bernoulli}(logit(\text{prob}) + \text{diff} \cdot \beta_2)
\end{aligned}\]&lt;/span&gt;
&lt;p&gt;I define the following regularizing priors.&lt;/p&gt;
&lt;span class=&#34;math display&#34;&gt;\[\begin{aligned}
\alpha &amp;amp;\sim \text{Normal}(500, 50) \\
\text{prob} &amp;amp;\sim \text{Beta}(900, 100) \\
\beta_1 &amp;amp;\sim \text{Normal}(0, 50) \\
\beta_2 &amp;amp;\sim \text{Normal}(0, 0.5) \\
\sigma &amp;amp;\sim \text{Normal}_+(100, 100)
\end{aligned}\]&lt;/span&gt;
&lt;p&gt;This is implemented in Stan &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;/case-study/mispecification/mispecified/#ref-Stan2023&#34;&gt;Stan Development Team 2023b&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;stan&#34;&gt;&lt;code&gt;data {
  int&amp;lt;lower = 1&amp;gt; N;
  int&amp;lt;lower = 1&amp;gt; K;
  matrix[N, K] X;
  vector[N] rt; //ms
  array[N] int response;
  int only_prior;
}
transformed data {
  array[N] int acc;
  for(n in 1:N)
    if(response[n] == 1)
      acc[n] = 1;
    else
      acc[n] = 0;
}
parameters {
  real alpha;
  real&amp;lt;lower=0, upper=1&amp;gt; prob;
  array[2] vector[K] beta;
  real&amp;lt;lower = 0&amp;gt; sigma;
}
transformed parameters{
  array[N] real log_lik;
  if(only_prior != 1)
    for(n in 1:N){
      log_lik[n] = normal_lpdf(rt[n] | alpha + X[n] * beta[1], sigma) +
        bernoulli_logit_lpmf(acc[n] | logit(prob) + X[n] * beta[2]);
    }
}
model {
  target += normal_lpdf(alpha | 500, 50);
  target += beta_lpdf(prob | 900, 100);
  
  target += normal_lpdf(beta[1] | 0, 50);
  target += normal_lpdf(beta[2] | 0, .5);
  target += normal_lpdf(sigma | 100, 100)
    - normal_lccdf(0 | 100, 100);
  if(only_prior!=1)
    target += sum(log_lik);
}
generated quantities {
  array[N] real pred_rt;
  array[N] int pred_response;
  for(n in 1:N){
    int pacc = bernoulli_logit_rng(logit(prob) + X[n] * beta[2]);
    if(pacc==1)
      pred_response[n] = 1;
    else
      pred_response[n] = 2;
    
    pred_rt[n] = normal_rng(alpha + X[n] * beta[1], sigma);
    }
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fits_speed$agnostic &amp;lt;- stan(&amp;quot;./Agnostic.stan&amp;quot;, 
                            data = dsim_speed_list,
                            warmup = 1000,
                            iter = 10000)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print_model(fits_speed$agnostic) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 5 × 7
  variable    mean `2.5%` `97.5%`  rhat ess_bulk ess_tail
  &amp;lt;chr&amp;gt;      &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;
1 alpha     180.   176.    184.     1.0   45771.   29305.
2 prob        0.85   0.83    0.87   1.0   42409.   28075.
3 beta[1,1]   2.1   -2.2     6.4    1.0   46537.   29313.
4 beta[2,1]  -0.10  -0.30    0.10   1.0   45803.   29146.
5 sigma      59.    56.     62.     1.0   41886.   27427.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;3. Theory-agnostic model with Log-likelihood&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This model is similar to the previous one, but treats response times as shifted log-normally distributed.&lt;/p&gt;
&lt;span class=&#34;math display&#34;&gt;\[\begin{aligned}
(rt - T_{\text{shift}}) &amp;amp;\sim \text{LogNormal}(\alpha + \text{diff} \cdot \beta_1, \sigma)\\
acc &amp;amp;\sim \text{Bernoulli}(logit(\text{prob}) + \text{diff} \cdot \beta_2)
\end{aligned}\]&lt;/span&gt;
&lt;p&gt;I define the following very weak priors.&lt;/p&gt;
&lt;span class=&#34;math display&#34;&gt;\[\begin{aligned}
\alpha &amp;amp;\sim \text{Normal}(0, 100) \\
\text{prob} &amp;amp;\sim \text{Beta}(1, 1) \\
\beta_1 &amp;amp;\sim \text{Normal}(0, 10) \\
\beta_2 &amp;amp;\sim \text{Normal}(0, 10) \\
\sigma &amp;amp;\sim \text{Normal}_+(0, 100) \\
T_{\text{shift}} &amp;amp;\sim \text{Normal}_+(150, 100) \quad \text{with }  T_{\text{shift}} &amp;gt; min(rt)
\end{aligned}\]&lt;/span&gt;
&lt;p&gt;This is implemented in Stan &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;/case-study/mispecification/mispecified/#ref-Stan2023&#34;&gt;Stan Development Team 2023b&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;stan&#34;&gt;&lt;code&gt;data {
  int&amp;lt;lower = 1&amp;gt; N;
  int&amp;lt;lower = 1&amp;gt; K;
  matrix[N, K] X;
  vector[N] rt; //ms
  array[N] int response;
  int only_prior;
}
transformed data {
  array[N] int acc;
  for(n in 1:N)
    if(response[n] == 1)
      acc[n] = 1;
    else
      acc[n] = 0;
}
parameters {
  real alpha;
  real&amp;lt;lower=0, upper=1&amp;gt; prob;
  array[2] vector[K] beta;
  real&amp;lt;lower = 0&amp;gt; sigma;
  real&amp;lt;lower = 0, upper = min(rt)&amp;gt; T_shift;
}
transformed parameters{
  array[N] real log_lik;
  if(only_prior != 1)
    for(n in 1:N){
      log_lik[n] = lognormal_lpdf(rt[n] - T_shift | alpha + X[n] * beta[1], sigma) +
        bernoulli_logit_lpmf(acc[n] | logit(prob) + X[n] * beta[2]);
    }
}
model {
  target += beta_lpdf(prob | 1, 1);
  target += normal_lpdf(alpha | 0, 100);
  for(k in 1:K) target += normal_lpdf(beta[k] | 0, 10);
  target += normal_lpdf(sigma | 0, 100)
    - normal_lccdf(0 | 0, 100);
  target += normal_lpdf(T_shift | 150, 100)
    - log_diff_exp(normal_lcdf(min(rt) | 150, 100),
                   normal_lcdf(0 | 150, 100));
if(only_prior!=1)
    target += sum(log_lik);
}
generated quantities {
  array[N] real pred_rt;
  array[N] int pred_response;
  for(n in 1:N){
    int pacc = bernoulli_logit_rng(logit(prob) + X[n] * beta[2]);
    if(pacc==1)
      pred_response[n] = 1;
    else
      pred_response[n] = 2;
    
    pred_rt[n] = lognormal_rng(alpha + X[n] * beta[1], sigma) + T_shift;
    }
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fits_speed$agnosticLog &amp;lt;- stan(&amp;quot;./AgnosticLog.stan&amp;quot;,
                               data = dsim_speed_list,
                               warmup = 1000,
                               iter = 10000)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print_model(fits_speed$agnosticLog)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 6 × 7
  variable    mean `2.5%` `97.5%`  rhat ess_bulk ess_tail
  &amp;lt;chr&amp;gt;      &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;
1 alpha      4.3    4.2     4.4     1.0   14197.   17094.
2 prob       0.79   0.75    0.81    1.0   33289.   24355.
3 beta[1,1]  0.031 -0.016   0.078   1.0   32152.   22840.
4 beta[2,1] -0.075 -0.25    0.10    1.0   32800.   23617.
5 sigma      0.63   0.56    0.71    1.0   14817.   17980.
6 T_shift   89.    81.     95.      1.0   13848.   16188.&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;relationship-between-the-models-and-true-generating-process&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Relationship between the models and true generating process&lt;/h4&gt;
&lt;p&gt;The LNR model is the closest to the true generating process, but the priors are relatively ill-defined, much weaker than what we actually know. The theory-agnostic models are very flexible, with the second one allowing for a closer fit to the positively skewed response time.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;posterior-predictive-checks&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Posterior predictive checks&lt;/h4&gt;
&lt;p&gt;Posterior predictive checks for the general shape of the distribution of RTs show that no fit is perfect. However, the fit to the proportion of predicted correct vs incorrect responses (area of the violin plots) seems to be approximately fine.&lt;/p&gt;
&lt;!-- ```{r} --&gt;
&lt;!-- dens_speed &lt;- map2(fits_speed, names(fits_speed), --&gt;
&lt;!--                    ~ ppc_dens_overlay_grouped(df_sim_speed_train$rt, --&gt;
&lt;!--                                               yrep = extract(.x,pars = &#34;pred_rt&#34;)[[1]][1:200,, drop = FALSE], --&gt;
&lt;!--                                               group = df_sim_speed_train$difficulty) + --&gt;
&lt;!--                      ggtitle(.y)) --&gt;
&lt;!-- walk(dens_speed, ~ plot(.x)) --&gt;
&lt;!-- ``` --&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;violins &amp;lt;- map2(fits_speed, names(fits_speed), ~ violin_plot(df_sim_speed_train, .x)  + ggtitle(.y) )

walk(violins, ~ plot(.x))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/case-study/mispecification/mispecified//case-study/mispecification/mispecified_files/figure-html/violin-acc-1.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/case-study/mispecification/mispecified//case-study/mispecification/mispecified_files/figure-html/violin-acc-2.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/case-study/mispecification/mispecified//case-study/mispecification/mispecified_files/figure-html/violin-acc-3.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;!-- Quantile probability plot showing   0.1, 0.3, 0.5, 0.7, and 0.9 response times quantiles plotted against  proportion of incorrect responses (left) and proportion of correct responses (right). Word frequency are grouped according to the two difficulty condtions. --&gt;
&lt;!-- ```{r} --&gt;
&lt;!-- qpf_speed &lt;- map2(fits_speed, names(fits_speed) ~ --&gt;
&lt;!--                                do_qpf(.x, --&gt;
&lt;!--                                       df_sim_speed_train, --&gt;
&lt;!--                                       cond = difficulty)) --&gt;
&lt;!-- walk(qpf_speed, ~ plot(.x + ggtitle(names(fits_speed)))) --&gt;
&lt;!-- ``` --&gt;
&lt;/div&gt;
&lt;div id=&#34;model-comparison&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Model comparison&lt;/h4&gt;
&lt;p&gt;I implement model comparison with Bayes Factor (BF) using bridge sampling as well as with PSIS-LOO CV approximation using the log-score rule (&lt;span class=&#34;math inline&#34;&gt;\(\widehat{elpd}\)&lt;/span&gt;). When the model comparison is reported, the first model is the best model and it’s used as reference for the next models.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lm_speed &amp;lt;- map(fits_speed, ~ bridge_sampler(.x))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;loo_speed &amp;lt;- map(fits_speed, ~ loo(.x))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bf_compare(lm_speed) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;                 BF logBF
agnosticLog 1.0e+00     0
LNRace      1.5e+14    33
agnostic    1.0e+82   189&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;loo_compare(loo_speed)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;            elpd_diff se_diff
agnosticLog    0.0       0.0 
LNRace       -28.2       5.3 
agnostic    -175.8      28.3 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Both BF and &lt;span class=&#34;math inline&#34;&gt;\(\widehat{elpdf}\)&lt;/span&gt;-CV agree that the best model is the theory-agnostic with a log-normal likelihood. &lt;strong&gt;This shows that the model that is closer to the truth is not necessarily the one with the best predictions. A flexible theory-agnostic model might yield the best predictions even if it doesn’t resemble the generative process. Crucially, this is true for both BF and CV.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We add another cognitive model under consideration.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;case-1.b-another-competitor-fast-guess-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Case 1.b Another competitor: Fast guess model&lt;/h3&gt;
&lt;p&gt;Ollman’s &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;/case-study/mispecification/mispecified/#ref-Ollman1966&#34;&gt;1966&lt;/a&gt;)&lt;/span&gt; fast-guess model assumes that the behavior in this task (and in any other choice task) is governed by two distinct cognitive processes: (i) a guessing mode, and (ii) a task-engaged mode. In the guessing mode, responses are fast and accuracy is at chance level. In the task-engaged mode, responses are slower and accuracy approaches 100%. This means that intermediate values of response times and accuracy can only be achieved by mixing responses from the two modes. Further assumptions of this model are that response times depend on the difficulty of the choice, and that the probability of being on one of the two states depend on the speed incentives during the instructions.&lt;/p&gt;
&lt;p&gt;To simplify matters, I follow the implementation of &lt;span class=&#34;citation&#34;&gt;Nicenboim, Schad, and Vasishth (&lt;a href=&#34;/case-study/mispecification/mispecified/#ref-Nicenboim2024Bayesian&#34;&gt;2024&lt;/a&gt;)&lt;/span&gt;, I ignore the possibility that the accuracy of the choice is also affected by the difficulty of the choice. Also, I ignore the possibility that subjects might be biased to one specific response in the guessing mode.&lt;/p&gt;
&lt;p&gt;The fast-guess model makes the assumption that during a task, a single subject would behave in these two ways: They would be engaged in the task a proportion of the trials and would guess on the rest of the trials. This means that for a single subject, there is an underlying probability of being engaged in the task, &lt;span class=&#34;math inline&#34;&gt;\(p_{task}\)&lt;/span&gt;, that determines whether they are actually choosing (&lt;span class=&#34;math inline&#34;&gt;\(z=1\)&lt;/span&gt;) or guessing (&lt;span class=&#34;math inline&#34;&gt;\(z=0\)&lt;/span&gt;):&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
z_n \sim \mathit{Bernoulli}(p_{task})
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The value of the parameter &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt; in every trial determines the behavior of the subject. This means that the distribution that we observe is a mixture of the two distributions presented before:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
rt_n \sim
\begin{cases}
\mathit{LogNormal}(\alpha + \beta \cdot diff_n, \sigma), &amp;amp; \text{ if } z_n =1 \\
\mathit{LogNormal}(\gamma, \sigma_2), &amp;amp; \text{ if } z_n=0
\end{cases}

\end{equation}\]&lt;/span&gt;
&lt;span class=&#34;math display&#34; id=&#34;eq:dismix3&#34;&gt;\[\begin{equation}
acc_n \sim
\begin{cases}
\mathit{Bernoulli}(p_{correct}), &amp;amp; \text{ if } z_n =1 \\
\mathit{Bernoulli}(0.5), &amp;amp; \text{ if } z_n=0
\end{cases}
\tag{1}
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;I use the following priors&lt;/p&gt;
&lt;span class=&#34;math display&#34;&gt;\[\begin{aligned}
\alpha &amp;amp;\sim \mathit{Normal}(6, 1)\\
\beta &amp;amp;\sim \mathit{Normal}(0, .1)\\
\sigma &amp;amp;\sim \mathit{Normal}_+(.5, .2)\\
\gamma &amp;amp;\sim \mathit{Normal}(6, 1), \text{for } \gamma &amp;lt; \alpha \\
\sigma_2 &amp;amp;\sim \mathit{Normal}_+(.5, .2)\\
p_{task} &amp;amp;\sim \mathit{Beta}(8, 2)
\end{aligned}\]&lt;/span&gt;
&lt;p&gt;&lt;strong&gt;Crucially, this model should capture the pattern of fast errors, but because of the wrong reasons!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This is implemented in Stan &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;/case-study/mispecification/mispecified/#ref-Stan2023&#34;&gt;Stan Development Team 2023b&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;stan&#34;&gt;&lt;code&gt;data {
  int&amp;lt;lower = 1&amp;gt; N;
  int&amp;lt;lower = 1&amp;gt; K;
  matrix[N, K] X;
  vector[N] rt; //ms
  array[N] int response;
  int only_prior;
}
transformed data {
  array[N] int acc;
  for(n in 1:N)
    if(response[n] == 1)
      acc[n] = 1;
    else
      acc[n] = 0;
}
parameters {
  real alpha;
  vector[K] beta;
  real&amp;lt;lower = 0&amp;gt; sigma;
  real&amp;lt;upper = alpha&amp;gt; gamma;
  real&amp;lt;lower = 0&amp;gt; sigma2;
  real&amp;lt;lower = 0, upper = 1&amp;gt; p_correct;
  real&amp;lt;lower = 0, upper = 1&amp;gt; p_task;
}
transformed parameters{
  array[N] real log_lik;
  if(only_prior != 1)
    for(n in 1:N)
      log_lik[n] = log_sum_exp(log(p_task) +
                            lognormal_lpdf(rt[n] | alpha +  X[n] * beta, sigma) +
                            bernoulli_lpmf(acc[n] | p_correct),
                            log1m(p_task) +
                            lognormal_lpdf(rt[n] | gamma, sigma2) +
                            bernoulli_lpmf(acc[n] | .5));

}
model {
  target += normal_lpdf(alpha | 6, 1);
  target += normal_lpdf(beta | 0, .3);
  target += normal_lpdf(sigma | .5, .2)
    - normal_lccdf(0 | .5, .2);
  target += normal_lpdf(gamma | 6, 1) -
    normal_lcdf(alpha | 6, 1);
  target += normal_lpdf(sigma2 | .5, .2)
    - normal_lccdf(0 | .5, .2);
  target += beta_lpdf(p_correct | 995, 5);
  target += beta_lpdf(p_task | 8, 2);
  if(only_prior != 1)
    target += sum(log_lik);
}
generated quantities {
  array[N] real pred_rt;
  array[N] int pred_response;
  for(n in 1:N){
    int ontask = bernoulli_rng(p_task);
    if(ontask == 1){
      pred_response[n] = bernoulli_rng(p_correct) == 1 ? 1 : 2;
      pred_rt[n] = lognormal_rng(alpha + X[n] * beta, sigma);
    } else {
      pred_response[n] = bernoulli_rng(.5) == 1 ? 1 : 2;
      pred_rt[n] = lognormal_rng(gamma, sigma2);

    }
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fits_speed$FG &amp;lt;- stan(&amp;quot;./FastGuess.stan&amp;quot;,  
                      data = dsim_speed_list,
                      warmup = 1000,
                      iter = 10000)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print_model(fits_speed$FG)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 7 × 7
  variable   mean `2.5%` `97.5%`  rhat ess_bulk ess_tail
  &amp;lt;chr&amp;gt;     &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;
1 alpha     5.2    5.2     5.2     1.0   20792.   22252.
2 beta[1]   0.017 -0.017   0.050   1.0   40853.   27096.
3 sigma     0.31   0.29    0.33    1.0   37401.   27191.
4 gamma     5.1    5.0     5.1     1.0   27563.   24863.
5 sigma2    0.24   0.22    0.27    1.0   33117.   27203.
6 p_correct 0.99   0.99    1.0     1.0   35452.   22009.
7 p_task    0.57   0.50    0.63    1.0   33932.   28178.&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;model-comparison-1&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Model comparison&lt;/h4&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lm_speed$FG &amp;lt;- bridge_sampler(fits_speed$FG)
loo_speed$FG &amp;lt;- loo(fits_speed$FG)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bf_compare(lm_speed) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;                 BF logBF
agnosticLog 1.0e+00     0
FG          5.5e+09    22
LNRace      1.5e+14    33
agnostic    1.0e+82   189&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;loo_compare(loo_speed)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;            elpd_diff se_diff
agnosticLog    0.0       0.0 
LNRace       -28.2       5.3 
FG           -31.9       8.3 
agnostic    -175.8      28.3 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Both BF and &lt;span class=&#34;math inline&#34;&gt;\(\hat{elpdf}\)&lt;/span&gt; agree that the best model is the theory-agnostic with a log-normal likelihood. What if we only considering the cognitive models?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bf_compare(lm_speed[!startsWith(names(lm_speed),&amp;quot;agnostic&amp;quot;)])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;          BF logBF
FG         1     0
LNRace 27227    10&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;loo_compare(loo_speed[!startsWith(names(lm_speed),&amp;quot;agnostic&amp;quot;)])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;       elpd_diff se_diff
LNRace  0.0       0.0   
FG     -3.6      11.3   &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;BF shows a clear advantage for the Fast Guess model. This is a bit unsettling because the Fast Guess model is clearly different from the true generating process. &lt;span class=&#34;math inline&#34;&gt;\(\hat{elpdf}\)&lt;/span&gt;-CV cannot distinguish between the models.&lt;/p&gt;
&lt;p&gt;But the LogNormal race model had terrible priors, what if they are more realistic?&lt;/p&gt;
&lt;span class=&#34;math display&#34;&gt;\[\begin{aligned}
\alpha &amp;amp;\sim \text{Normal}(5.2, 1) \\
\beta &amp;amp;\sim \text{Normal}(0, .2) \\
\sigma &amp;amp;\sim \text{Normal}_+(0.5, 0.25) \\
T_{0} &amp;amp;\sim \text{Normal}(150, 100) \quad \text{with a minimum value of min(rt)}
\end{aligned}\]&lt;/span&gt;
&lt;p&gt;This is implemented in Stan &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;/case-study/mispecification/mispecified/#ref-Stan2023&#34;&gt;Stan Development Team 2023b&lt;/a&gt;)&lt;/span&gt; and the model comparison is repeated.&lt;/p&gt;
&lt;pre class=&#34;stan&#34;&gt;&lt;code&gt;functions{
  real lognormalrace2_lpdf(real rt, int response,
                           real T_nd,
                           array[] real mu,
                           real sigma){
    real T = rt - T_nd;
    real lp = 0;
    if(response==1)
      lp += lognormal_lpdf(T | mu[1], sigma) +
        lognormal_lccdf(T | mu[2], sigma);
    else
      lp += lognormal_lpdf(T | mu[2], sigma) +
        lognormal_lccdf(T | mu[1], sigma);
    return lp;
  }
  array[] real  lognormalrace2_rng(real T_nd,
                          array[] real mu,
                          real sigma){
    real rt1 = lognormal_rng(mu[1], sigma);
    real rt2 = lognormal_rng(mu[2], sigma);
    array[2] real out;
    if(rt1 &amp;lt; rt2)
      out = {rt1 + T_nd, 1.0};
    else
      out = {rt2 + T_nd, 2.0};
    return out;
      }
}
data {
  int&amp;lt;lower = 1&amp;gt; N;
  int&amp;lt;lower = 1&amp;gt; K;
  matrix[N, K] X;
  vector[N] rt; //ms
  array[N] int response;
  int only_prior;
}
parameters {
  array[2] real alpha;
  array[2] vector[K] beta;
  real&amp;lt;lower = 0&amp;gt; sigma;
  real&amp;lt;lower = 0, upper = min(rt)&amp;gt; T_nd;
}
transformed parameters{
  array[N] real log_lik;
  if(only_prior != 1)
    for(n in 1:N){
      log_lik[n] = lognormalrace2_lpdf(rt[n]| response[n],
                                       T_nd,
                                       {alpha[1] + X[n] * beta[1],
                                        alpha[2] + X[n] * beta[2]},
                                       sigma);
    }
}
model {
  target += normal_lpdf(alpha | 5.2, 1);
  for(k in 1:K) target += normal_lpdf(beta[k] | 0, 0.2);
  target += normal_lpdf(sigma | 0.5, 0.25)
    - normal_lccdf(0 | 0.5, .25);
  target += normal_lpdf(T_nd | 150, 100)
    - log_diff_exp(normal_lcdf(min(rt) | 150, 100),
                   normal_lcdf(0 | 150, 100));
  if(only_prior!=1)
    target += sum(log_lik);
}
generated quantities {
  array[N] real pred_rt;
  array[N] int pred_response;
  for(n in 1:N){
    array[2] real out;
    out = lognormalrace2_rng(T_nd,
                             {alpha[1] + X[n] * beta[1],
                              alpha[2] + X[n] * beta[2]},
                             sigma);
    pred_rt[n] = out[1];
    pred_response[n] = to_int(out[2]);
    }
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lm_speed$LNRace_reg_priors &amp;lt;- bridge_sampler(fits_speed$LNRace_reg_priors)
loo_speed$LNRace_reg_priors &amp;lt;- loo(fits_speed$LNRace_reg_priors)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bf_compare(lm_speed) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;                       BF logBF
agnosticLog       1.0e+00     0
LNRace_reg_priors 4.6e+06    15
FG                5.5e+09    22
LNRace            1.5e+14    33
agnostic          1.0e+82   189&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;loo_compare(loo_speed) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;                  elpd_diff se_diff
agnosticLog          0.0       0.0 
LNRace_reg_priors  -28.2       5.3 
LNRace             -28.2       5.3 
FG                 -31.9       8.3 
agnostic          -175.8      28.3 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The best model is still the theory-agnostic flexible model with a log-normal likelihood.&lt;/p&gt;
&lt;p&gt;Considering only the cognitive models, we see the differences between BF and &lt;span class=&#34;math inline&#34;&gt;\(\hat{elpd}\)&lt;/span&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bf_compare(lm_speed[!startsWith(names(lm_speed),&amp;quot;agnostic&amp;quot;)])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;                        BF logBF
LNRace_reg_priors        1   0.0
FG                    1197   7.1
LNRace            32586467  17.3&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;loo_compare(loo_speed[!startsWith(names(lm_speed),&amp;quot;agnostic&amp;quot;)])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;                  elpd_diff se_diff
LNRace_reg_priors  0.0       0.0   
LNRace             0.0       0.3   
FG                -3.7      11.1   &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With better priors, the LNR model is prefered according to the BF. This shows that the comparison of (cognitive) models that entail very different generative but can mimic the data well (for different reasons) can be very strongly prior dependent. In contrast, &lt;span class=&#34;math inline&#34;&gt;\(\hat{elpd}\)&lt;/span&gt;-CV is less enthusiastic in selecting a model.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;case-1.c-yet-another-competitor-a-more-flexible-implementation-of-the-lnr-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Case 1.c Yet another competitor: A more flexible implementation of the LNR model&lt;/h3&gt;
&lt;p&gt;A more flexible implementation of the Log Normal race model relaxes the assumption that the noise parameter is the same for all the accumulators. This supposed to allow it to capture more flexible patterns in the data.&lt;/p&gt;
&lt;p&gt;I implement it with the following likelihood:&lt;/p&gt;
&lt;span class=&#34;math display&#34;&gt;\[\begin{aligned}
\mu_1 &amp;amp;= \alpha_1 \cdot diff_n \cdot \beta_1\\
\mu_2 &amp;amp;= \alpha_2 \cdot diff_n \cdot \beta_2\\
\mu_3 &amp;amp;= \alpha_3 \cdot diff_n \cdot \beta_3\\
\mu_4 &amp;amp;= \alpha_4 \cdot diff_n \cdot \beta_4\\
(rt_n; response_n ) &amp;amp;\sim \text{LNR}(\{\mu_1;\mu_2\}, \{\exp(mu_3); \exp(mu_4)\}, T_0)
\end{aligned}\]&lt;/span&gt;
&lt;p&gt;We try two flavors,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;with very uninformative priors&lt;/li&gt;
&lt;/ul&gt;
&lt;span class=&#34;math display&#34;&gt;\[\begin{aligned}
\alpha_{1;2} &amp;amp;\sim \text{Normal}(0, 100) \\
\alpha_{3;4} &amp;amp;\sim \text{Normal}(0, 2) \\
\beta &amp;amp;\sim \text{Normal}(0, 10) \\
T_{0} &amp;amp;\sim \text{Normal}(150, 100) \quad \text{with a minimum value of min(rt)}
\end{aligned}\]&lt;/span&gt;
&lt;pre class=&#34;stan&#34;&gt;&lt;code&gt;functions{
  real lognormalrace2_lpdf(real rt, int response,
                           real T_nd,
                           array[] real mu,
                           array[] real sigma){
    real T = rt - T_nd;
    real lp = 0;
    if(response==1)
      lp += lognormal_lpdf(T | mu[1], sigma[1]) +
        lognormal_lccdf(T | mu[2], sigma[2]);
    else
      lp += lognormal_lpdf(T | mu[2], sigma[1]) +
        lognormal_lccdf(T | mu[1], sigma[2]);
    return lp;
  }
  array[] real  lognormalrace2_rng(real T_nd,
                          array[] real mu,
                          array[] real sigma){
    real rt1 = lognormal_rng(mu[1], sigma[1]);
    real rt2 = lognormal_rng(mu[2], sigma[2]);
    array[2] real out;
    if(rt1 &amp;lt; rt2)
      out = {rt1 + T_nd, 1.0};
    else
      out = {rt2 + T_nd, 2.0};
    return out;
      }
}
data {
  int&amp;lt;lower = 1&amp;gt; N;
  int&amp;lt;lower = 1&amp;gt; K;
  matrix[N, K] X;
  vector[N] rt; //ms
  array[N] int response;
  int only_prior;
}
parameters {
  array[4] real alpha;
  array[4] vector[K] beta;
  real&amp;lt;lower = 0, upper = min(rt)&amp;gt; T_nd;
}
transformed parameters{
  array[N] real log_lik;
  if(only_prior != 1)
    for(n in 1:N){
      log_lik[n] = lognormalrace2_lpdf(rt[n]| response[n],
                                       T_nd,
                                        {alpha[1] + X[n] * beta[1],
                                         alpha[2] + X[n] * beta[2]},
                                       {exp(alpha[3] + X[n] * beta[3]),
                                        exp(alpha[4] + X[n] * beta[4])});
    }
}
model {
  target += normal_lpdf(alpha[1:2] | 0, 100);
  target += normal_lpdf(alpha[3:4] | 0, 2);
  for(k in 1:K) target += normal_lpdf(beta[k] | 0, 10);
  target += normal_lpdf(T_nd | 150, 100)
    - log_diff_exp(normal_lcdf(min(rt) | 150, 100),
                   normal_lcdf(0 | 150, 100));
  if(only_prior!=1)
    target += sum(log_lik);
}
generated quantities {
  array[N] real pred_rt;
  array[N] int pred_response;
  for(n in 1:N){
    array[2] real out;
    out = lognormalrace2_rng(T_nd,
                             {alpha[1] + X[n] * beta[1],
                              alpha[2] + X[n] * beta[2]},
                             {exp(alpha[3] + X[n] * beta[3]),
                              exp(alpha[4] + X[n] * beta[4])});
    pred_rt[n] = out[1];
    pred_response[n] = to_int(out[2]);
    }
}&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;and with regularizing priors.&lt;/li&gt;
&lt;/ul&gt;
&lt;span class=&#34;math display&#34;&gt;\[\begin{aligned}
\alpha_{1;2} &amp;amp;\sim \text{Normal}(5.2, 1) \\
\alpha_{3;4} &amp;amp;\sim \text{Normal}(\log(0.5), 1) \\
\beta &amp;amp;\sim \text{Normal}(0, .2) \\
T_{0} &amp;amp;\sim \text{Normal}(150, 100) \quad \text{with a minimum value of min(rt)}
\end{aligned}\]&lt;/span&gt;
&lt;pre class=&#34;stan&#34;&gt;&lt;code&gt;functions{
  real lognormalrace2_lpdf(real rt, int response,
                           real T_nd,
                           array[] real mu,
                           array[] real sigma){
    real T = rt - T_nd;
    real lp = 0;
    if(response==1)
      lp += lognormal_lpdf(T | mu[1], sigma[1]) +
        lognormal_lccdf(T | mu[2], sigma[2]);
    else
      lp += lognormal_lpdf(T | mu[2], sigma[1]) +
        lognormal_lccdf(T | mu[1], sigma[2]);
    return lp;
  }
  array[] real  lognormalrace2_rng(real T_nd,
                          array[] real mu,
                          array[] real sigma){
    real rt1 = lognormal_rng(mu[1], sigma[1]);
    real rt2 = lognormal_rng(mu[2], sigma[2]);
    array[2] real out;
    if(rt1 &amp;lt; rt2)
      out = {rt1 + T_nd, 1.0};
    else
      out = {rt2 + T_nd, 2.0};
    return out;
      }
}
data {
  int&amp;lt;lower = 1&amp;gt; N;
  int&amp;lt;lower = 1&amp;gt; K;
  matrix[N, K] X;
  vector[N] rt; //ms
  array[N] int response;
  int only_prior;
}
parameters {
  array[4] real alpha;
  array[4] vector[K] beta;
  real&amp;lt;lower = 0, upper = min(rt)&amp;gt; T_nd;
}
transformed parameters{
  array[N] real log_lik;
  if(only_prior != 1)
    for(n in 1:N){
      log_lik[n] = lognormalrace2_lpdf(rt[n]| response[n],
                                       T_nd,
                                        {alpha[1] + X[n] * beta[1],
                                         alpha[2] + X[n] * beta[2]},
                                       {exp(alpha[3] + X[n] * beta[3]),
                                        exp(alpha[4] + X[n] * beta[4])});
    }
}
model {
  target += normal_lpdf(alpha[1:2] | 5.2, 1);
  target += normal_lpdf(alpha[3:4] | log(0.5), 1);
  for(k in 1:K) target += normal_lpdf(beta[k] | 0, .2);
  target += normal_lpdf(T_nd | 150, 100)
    - log_diff_exp(normal_lcdf(min(rt) | 150, 100),
                   normal_lcdf(0 | 150, 100));
  if(only_prior!=1)
    target += sum(log_lik);
}
generated quantities {
  array[N] real pred_rt;
  array[N] int pred_response;
  for(n in 1:N){
    array[2] real out;
    out = lognormalrace2_rng(T_nd,
                             {alpha[1] + X[n] * beta[1],
                              alpha[2] + X[n] * beta[2]},
                             {exp(alpha[3] + X[n] * beta[3]),
                              exp(alpha[4] + X[n] * beta[4])});
    pred_rt[n] = out[1];
    pred_response[n] = to_int(out[2]);
    }
}&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;posterior-predictive-checks-1&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Posterior predictive checks&lt;/h4&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit_plot &amp;lt;- fits_speed[!endsWith(names(fits_speed),&amp;quot;reg_priors&amp;quot;)]
violins &amp;lt;- map2(fit_plot, names(fit_plot), ~ violin_plot(df_sim_speed_train, .x)  + ggtitle(.y) )

walk(violins, ~ plot(.x))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/case-study/mispecification/mispecified//case-study/mispecification/mispecified_files/figure-html/unnamed-chunk-38-1.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/case-study/mispecification/mispecified//case-study/mispecification/mispecified_files/figure-html/unnamed-chunk-38-2.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/case-study/mispecification/mispecified//case-study/mispecification/mispecified_files/figure-html/unnamed-chunk-38-3.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/case-study/mispecification/mispecified//case-study/mispecification/mispecified_files/figure-html/unnamed-chunk-38-4.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/case-study/mispecification/mispecified//case-study/mispecification/mispecified_files/figure-html/unnamed-chunk-38-5.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dens_speed &amp;lt;- map2(
  fit_plot, 
  names(fit_plot), ~ ppc_dens_overlay_grouped(df_sim_speed_train$rt,
                                           yrep = extract(.x,pars = &amp;quot;pred_rt&amp;quot;)[[1]][1:200,, drop = FALSE],
                                           group = df_sim_speed_train$difficulty) +

  coord_cartesian(xlim= c(0, 1500)) + ggtitle(.y))
walk(dens_speed, ~ plot(.x ))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/case-study/mispecification/mispecified//case-study/mispecification/mispecified_files/figure-html/unnamed-chunk-39-1.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/case-study/mispecification/mispecified//case-study/mispecification/mispecified_files/figure-html/unnamed-chunk-39-2.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/case-study/mispecification/mispecified//case-study/mispecification/mispecified_files/figure-html/unnamed-chunk-39-3.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/case-study/mispecification/mispecified//case-study/mispecification/mispecified_files/figure-html/unnamed-chunk-39-4.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/case-study/mispecification/mispecified//case-study/mispecification/mispecified_files/figure-html/unnamed-chunk-39-5.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;!-- ```{r, message = FALSE} --&gt;
&lt;!-- qpf_speed &lt;- map(fit_plot, ~ do_qpf(.x, df_sim_speed_train, cond = difficulty) + --&gt;
&lt;!--                  coord_cartesian(ylim= c(100, 500))) --&gt;
&lt;!-- walk(qpf_speed, ~ plot(.x + ggtitle(names(fit_plot)))) --&gt;
&lt;!-- ``` --&gt;
&lt;/div&gt;
&lt;div id=&#34;model-comparison-2&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Model comparison&lt;/h4&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bf_compare(lm_speed)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;                          BF logBF
LNRace_fl_reg_priors 1.0e+00   0.0
agnosticLog          3.6e+02   5.9
LNRace_fl            1.2e+06  14.0
LNRace_reg_priors    1.6e+09  21.2
FG                   2.0e+12  28.3
LNRace               5.3e+16  38.5
agnostic             3.6e+84 194.7&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bf_compare(lm_speed[c(&amp;quot;LNRace_fl_reg_priors&amp;quot;,&amp;quot;agnosticLog&amp;quot;)])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;                      BF logBF
LNRace_fl_reg_priors   1   0.0
agnosticLog          359   5.9&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bf_compare(lm_speed[c(&amp;quot;LNRace_fl&amp;quot;,&amp;quot;agnosticLog&amp;quot;)])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;              BF logBF
agnosticLog    1   0.0
LNRace_fl   3237   8.1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;loo_compare(loo_speed)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;                     elpd_diff se_diff
agnosticLog             0.0       0.0 
LNRace_fl_reg_priors   -2.3       4.5 
LNRace_fl              -2.4       4.5 
LNRace_reg_priors     -28.2       5.3 
LNRace                -28.2       5.3 
FG                    -31.9       8.3 
agnostic             -175.8      28.3 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Only with good priors, the flexible version of the LNR model is the best model according the Bayes Factor. CV cannot really distinguish betwen them.&lt;/p&gt;
&lt;p&gt;Considering only the cognitive models, there is slightly more agreement between the model comparison methods. The flexible version of the LNR model is the superior model for both methods.&lt;/p&gt;
&lt;p&gt;This shows that even the selection of a model that relatively closely resembles the data generative process can be strongly prior dependent for the BF.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bf_compare(lm_speed[!startsWith(names(lm_speed),&amp;quot;agnostic&amp;quot;)])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;                          BF logBF
LNRace_fl_reg_priors 1.0e+00     0
LNRace_fl            1.2e+06    14
LNRace_reg_priors    1.6e+09    21
FG                   2.0e+12    28
LNRace               5.3e+16    39&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;loo_compare(loo_speed[!startsWith(names(lm_speed),&amp;quot;agnostic&amp;quot;)])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;                     elpd_diff se_diff
LNRace_fl_reg_priors   0.0       0.0  
LNRace_fl             -0.1       0.1  
LNRace_reg_priors    -25.9       5.8  
LNRace               -25.9       5.7  
FG                   -29.6       8.2  &lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;visualization-of-elpd-fast-guess-vs-flexible-lnr&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Visualization of elpd: Fast Guess vs Flexible LNR&lt;/h4&gt;
&lt;p&gt;The plots show the difference in pointwise predictive accuracy of the Flexible LNR vs Fast Guess models. A more positive value indicates an advantage for the Flexible LNR.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_sim_speed_train &amp;lt;- ungroup(df_sim_speed_train) %&amp;gt;%
  mutate(diff_elpd_LNRF_FG =  loo_speed$LNRace_fl$pointwise[,&amp;quot;elpd_loo&amp;quot;] -  loo_speed$FG $pointwise[,&amp;quot;elpd_loo&amp;quot;],
         diff_elpd_LNRF_AL =  loo_speed$LNRace_fl$pointwise[,&amp;quot;elpd_loo&amp;quot;] -  loo_speed$FG$pointwise[,&amp;quot;elpd_loo&amp;quot;])


ggplot(df_sim_speed_train,
       aes(x = rt, y = diff_elpd_LNRF_FG)) +
  geom_jitter(alpha = .5, width = 0, height = .1 ) +
  facet_grid(difficulty ~ resp) +
  geom_hline(yintercept = 0, linetype = &amp;quot;dashed&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/case-study/mispecification/mispecified//case-study/mispecification/mispecified_files/figure-html/unnamed-chunk-45-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;case-2.-accuracy-emphasis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Case 2. Accuracy emphasis&lt;/h2&gt;
&lt;p&gt;I fit again all the models for a subset of the data where accuracy is emphasized.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(123)
df_sim_acc &amp;lt;- df_sim |&amp;gt;
  filter(emphasis ==&amp;quot;accuracy&amp;quot;) |&amp;gt;
  group_by(difficulty) |&amp;gt;
  mutate(train = rbinom(n(),1,.9))
df_sim_acc_train &amp;lt;-  df_sim_acc |&amp;gt;
  filter(train==1)
dsim_acc_list &amp;lt;- list(N = nrow(df_sim_acc_train),
                      rt =df_sim_acc_train$rt,
                      response = df_sim_acc_train$response,
                      K = 1,
                      X = model.matrix(~ 0 + diff, df_sim_acc_train),
                      only_prior = 0)

fits_acc &amp;lt;- list()
fits_acc$LNRace &amp;lt;- stan(&amp;quot;./LogNormalRace_badpriors.stan&amp;quot;,
                        data = dsim_acc_list,
                        warmup = 1000,
                        iter = 10000)

fits_acc$LNRace_reg_priors &amp;lt;- stan(&amp;quot;./LogNormalRace.stan&amp;quot;,
                                   data = dsim_acc_list,
                                   warmup = 1000,
                                   iter = 10000)

fits_acc$agnostic &amp;lt;- stan(&amp;quot;./Agnostic.stan&amp;quot;,
                          data = dsim_acc_list,
                          warmup = 1000,
                          iter = 10000)

fits_acc$agnosticLog &amp;lt;- stan(&amp;quot;./AgnosticLog.stan&amp;quot;,
                             data = dsim_acc_list,
                             warmup = 1000,
                             iter = 10000)

fits_acc$FG &amp;lt;- stan(&amp;quot;./FastGuess.stan&amp;quot;,
                    data = dsim_acc_list,
                    warmup = 1000,
                    iter = 10000)

fits_acc$LNRace_fl &amp;lt;- stan(&amp;quot;./LogNormalRace_fl_badpriors.stan&amp;quot;,
                           data = dsim_acc_list,
                           control = list(adapt_delta = .9,
                                          max_treedepth = 12),
                           warmup = 1000,
                           iter = 10000)

fits_acc$LNRace_fl_reg_priors &amp;lt;- stan(&amp;quot;./LogNormalRace_fl.stan&amp;quot;,
                                      data = dsim_acc_list,
                                      control = list(adapt_delta = .9,
                                                     max_treedepth = 12),
                                      warmup = 1000,
                                      iter = 10000)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;posterior-predictive-check&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Posterior predictive check&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit_plot &amp;lt;- fits_acc[!endsWith(names(fits_acc),&amp;quot;reg_priors&amp;quot;)]

violins &amp;lt;- map2(fit_plot, names(fit_plot), ~ violin_plot(df_sim_acc_train, .x)  + ggtitle(.y) )

walk(violins, ~ plot(.x))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/case-study/mispecification/mispecified//case-study/mispecification/mispecified_files/figure-html/unnamed-chunk-47-1.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/case-study/mispecification/mispecified//case-study/mispecification/mispecified_files/figure-html/unnamed-chunk-47-2.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/case-study/mispecification/mispecified//case-study/mispecification/mispecified_files/figure-html/unnamed-chunk-47-3.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/case-study/mispecification/mispecified//case-study/mispecification/mispecified_files/figure-html/unnamed-chunk-47-4.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/case-study/mispecification/mispecified//case-study/mispecification/mispecified_files/figure-html/unnamed-chunk-47-5.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;model-comparison-3&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Model comparison&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lm_acc &amp;lt;- map(fits_acc, ~ bridge_sampler(.x))
loo_acc&amp;lt;- map(fits_acc, ~ loo(.x))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bf_compare(lm_acc) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;                          BF logBF
agnosticLog          1.0e+00     0
FG                   8.2e+05    14
LNRace_reg_priors    2.2e+06    15
LNRace_fl_reg_priors 5.0e+06    15
LNRace_fl            6.9e+12    30
LNRace               1.4e+14    33
agnostic             2.5e+73   169&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;loo_compare(loo_acc)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;                     elpd_diff se_diff
agnosticLog             0.0       0.0 
FG                    -23.2       7.7 
LNRace_fl_reg_priors  -25.0       7.2 
LNRace_fl             -25.1       7.2 
LNRace                -28.0       8.2 
LNRace_reg_priors     -28.0       8.2 
agnostic             -181.7      42.1 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As before, the most flexible theory-agnostic model is selected by both methods. What if we only considering the cognitive models?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bf_compare(lm_acc[!startsWith(names(lm_acc),&amp;quot;agnostic&amp;quot;)])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;                          BF logBF
FG                   1.0e+00  0.00
LNRace_reg_priors    2.7e+00  0.98
LNRace_fl_reg_priors 6.1e+00  1.81
LNRace_fl            8.4e+06 15.95
LNRace               1.7e+08 18.96&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;loo_compare(loo_acc[!startsWith(names(lm_acc),&amp;quot;agnostic&amp;quot;)])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;                     elpd_diff se_diff
FG                    0.0       0.0   
LNRace_fl_reg_priors -1.8      11.0   
LNRace_fl            -1.9      11.0   
LNRace               -4.8      11.5   
LNRace_reg_priors    -4.8      11.5   &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here, the Fast Guess model, which shouldn’t even be able to capture the long errors, is the one selected as the best model. The summary of the posterior of the model below shows that this is achieved because the location of the errors is just a bit faster than non-error, and has a much larger scale parameter:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print_model(fits_acc$FG)   &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 7 × 7
  variable   mean `2.5%` `97.5%`  rhat ess_bulk ess_tail
  &amp;lt;chr&amp;gt;     &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;
1 alpha     5.7    5.6     5.7     1.0   42748.   26049.
2 beta[1]   0.051  0.029   0.074   1.0   45729.   27276.
3 sigma     0.23   0.21    0.25    1.0   33397.   26122.
4 gamma     5.6    5.6     5.7     1.0   49136.   29303.
5 sigma2    0.37   0.32    0.42    1.0   34566.   25136.
6 p_correct 0.99   0.99    1.0     1.0   36978.   23317.
7 p_task    0.75   0.70    0.79    1.0   39981.   26841.&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;case-3.-considering-both-speed-and-accuracy-emphasis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Case 3. Considering both speed and accuracy emphasis&lt;/h2&gt;
&lt;p&gt;Now we consider the data with both speed and accuracy emphasis and all the models are fit again.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(123)

df_sim &amp;lt;- df_sim |&amp;gt;
  group_by(difficulty) |&amp;gt;
  mutate(train = rbinom(n(),1,.9))
df_sim_train &amp;lt;-  df_sim |&amp;gt;
  filter(train==1)

dsim_list &amp;lt;- list(N = nrow(df_sim_train),
                  rt =df_sim_train$rt,
                  response = df_sim_train$response,
                  K = 2,
                  X = model.matrix(~ 0 + diff + emph , df_sim_train),
                  only_prior = 0)

fits &amp;lt;- list()
fits$LNRace &amp;lt;- stan(&amp;quot;./LogNormalRace_badpriors.stan&amp;quot;,
                    data = dsim_list,
                    warmup = 1000,
                    iter = 10000)

fits$LNRace_reg_priors &amp;lt;- stan(&amp;quot;./LogNormalRace.stan&amp;quot;,
                               data = dsim_list,
                               warmup = 1000,
                               iter = 10000)
 
fits$agnostic &amp;lt;- stan(&amp;quot;./Agnostic.stan&amp;quot;,
                      data = dsim_list,
                      warmup = 1000,
                      iter = 10000)

fits$agnosticLog &amp;lt;- stan(&amp;quot;./AgnosticLog.stan&amp;quot;,
                         data = dsim_list,
                         warmup = 1000,
                         iter = 10000)

fits$FG &amp;lt;- stan(&amp;quot;./FastGuess.stan&amp;quot;,
                data = dsim_list,
                warmup = 1000,
                iter = 10000)

fits$LNRace_fl &amp;lt;- stan(&amp;quot;./LogNormalRace_fl_badpriors.stan&amp;quot;,
                       data = dsim_list,
                       warmup = 1000,
                       iter = 10000,
                       control = list(adapt_delta = .9,
                                      max_treedepth = 12))
fits$LNRace_fl_reg_priors &amp;lt;- stan(&amp;quot;./LogNormalRace_fl.stan&amp;quot;,
                                  data = dsim_list,
                                  warmup = 1000,
                                  iter = 10000,
                                  control = list(adapt_delta = .9,
                                                 max_treedepth = 12))&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;posterior-predictive-check-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Posterior predictive check&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit_plot &amp;lt;- fits[!endsWith(names(fits),&amp;quot;reg_priors&amp;quot;)]
violins &amp;lt;- map2(fit_plot, names(fit_plot), ~ violin_plot(df_sim_train, .x)  + ggtitle(.y) )

walk(violins, ~ plot(.x))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/case-study/mispecification/mispecified//case-study/mispecification/mispecified_files/figure-html/unnamed-chunk-55-1.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/case-study/mispecification/mispecified//case-study/mispecification/mispecified_files/figure-html/unnamed-chunk-55-2.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/case-study/mispecification/mispecified//case-study/mispecification/mispecified_files/figure-html/unnamed-chunk-55-3.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/case-study/mispecification/mispecified//case-study/mispecification/mispecified_files/figure-html/unnamed-chunk-55-4.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/case-study/mispecification/mispecified//case-study/mispecification/mispecified_files/figure-html/unnamed-chunk-55-5.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;model-comparison-4&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Model comparison&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lm &amp;lt;- map(fits, ~ bridge_sampler(.x))
loo&amp;lt;- map(fits, ~ loo(.x))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The result of the comparison of the original models (no flexible LogNormal Race model) is similar to the speed emphasis comparison. A theory-agnostic model makes the better predictions.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bf_compare(lm[!startsWith(names(lm),&amp;quot;LNRace_fl&amp;quot;)])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;                        BF logBF
agnosticLog        1.0e+00     0
LNRace_reg_priors  1.1e+23    53
LNRace             2.6e+34    79
FG                 2.0e+39    91
agnostic          1.3e+166   382&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;loo_compare(loo[!startsWith(names(loo),&amp;quot;LNRace_fl&amp;quot;)])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;                  elpd_diff se_diff
agnosticLog          0.0       0.0 
LNRace_reg_priors  -73.2       8.4 
LNRace             -73.3       8.4 
FG                 -94.0      14.4 
agnostic          -393.4      67.1 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The model closer to the true generative process makes the better predictions according to the BF.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bf_compare(lm) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;                           BF logBF
LNRace_fl_reg_priors  1.0e+00     0
LNRace_fl             8.0e+09    23
agnosticLog           1.8e+14    33
LNRace_reg_priors     1.9e+37    86
LNRace                4.7e+48   112
FG                    3.7e+53   123
agnostic             2.3e+180   415&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;loo_compare(loo)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;                     elpd_diff se_diff
LNRace_fl               0.0       0.0 
LNRace_fl_reg_priors   -0.1       0.3 
agnosticLog           -19.7      13.5 
LNRace_reg_priors     -92.9      14.9 
LNRace                -93.0      14.9 
FG                   -113.7      18.8 
agnostic             -413.1      63.6 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Interestingly, now the prior dependency is less strong, even without regularing priors the model with a generative process closer to the truth is the best model:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bf_compare(lm[!endsWith(names(lm),&amp;quot;reg_priors&amp;quot;)])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;                  BF logBF
LNRace_fl    1.0e+00     0
agnosticLog  2.3e+04    10
LNRace       5.9e+38    89
FG           4.6e+43   101
agnostic    2.9e+170   392&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;visualization-of-elpd&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Visualization of elpd&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Flexible LNRace vs FG&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_sim_train &amp;lt;- ungroup(df_sim_train) %&amp;gt;%
  mutate(diff_elpd_LNRF_FG =  loo$LNRace_fl$pointwise[,&amp;quot;elpd_loo&amp;quot;] -  loo$FG $pointwise[,&amp;quot;elpd_loo&amp;quot;])


ggplot(df_sim_train,
       aes(x = rt, y = diff_elpd_LNRF_FG)) +
  geom_jitter(alpha = .5, width = 0, height = .1 ) +
  facet_grid(difficulty ~ resp + emphasis) +
  geom_hline(yintercept = 0, linetype = &amp;quot;dashed&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/case-study/mispecification/mispecified//case-study/mispecification/mispecified_files/figure-html/unnamed-chunk-62-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Flexible LNRace vs Theory-agnostic Log&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_sim_train &amp;lt;- ungroup(df_sim_train) %&amp;gt;%
  mutate(diff_elpd_LNRF_AL =  loo$LNRace_fl$pointwise[,&amp;quot;elpd_loo&amp;quot;] -  loo$agnosticLog $pointwise[,&amp;quot;elpd_loo&amp;quot;])


ggplot(df_sim_train,
       aes(x = rt, y = diff_elpd_LNRF_AL)) +
  geom_jitter(alpha = .5, width = 0, height = .1 ) +
  facet_grid(difficulty ~ resp + emphasis) +
  geom_hline(yintercept = 0, linetype = &amp;quot;dashed&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/case-study/mispecification/mispecified//case-study/mispecification/mispecified_files/figure-html/unnamed-chunk-63-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In the context of both Bayes Factor analysis and Cross-Validation using the log-score rule, we notice the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It is not always the case that a model which aligns more closely with the underlying truth will exhibit superior predictive capabilities. This shows how the best model in terms of predictions is not necessarily the one that is most faithful to the actual generative process.&lt;/li&gt;
&lt;li&gt;Models that are designed to be flexible and not bound to any specific theoretical framework may deliver the most accurate predictions, despite potentially lacking resemblance to the actual process that generates the data.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Specific to Bayes Factor:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;When comparing cognitive models that have significantly different underlying generative processes but are capable of closely mimicking the observed data (albeit for varied reasons), the outcome of such comparisons can be highly dependent on the priors. This suggests a strong influence of the initial assumptions on the comparative analysis of models.&lt;/li&gt;
&lt;li&gt;The process of selecting a model that appears to closely match the process generating the observed data can also be heavily influenced by the choice of priors, although this is not always the case.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Specific to Cross-Validation:&lt;/p&gt;
&lt;p&gt;Cross-Validation remains inconclusive unless there is a demonstrable improvement in prediction quality. This stance is maintained irrespective of the proximity of a model to the true generative process, showing how CV prioritizes predictive performance over theoretical fidelity to the generative mechanism.&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;I used R &lt;span class=&#34;citation&#34;&gt;(Version 4.5.0; &lt;a href=&#34;/case-study/mispecification/mispecified/#ref-R-base&#34;&gt;R Core Team 2024&lt;/a&gt;)&lt;/span&gt; and the R-packages &lt;em&gt;bayesplot&lt;/em&gt; &lt;span class=&#34;citation&#34;&gt;(Version 1.12.0; &lt;a href=&#34;/case-study/mispecification/mispecified/#ref-R-bayesplot&#34;&gt;Gabry et al. 2019&lt;/a&gt;)&lt;/span&gt;, &lt;em&gt;bridgesampling&lt;/em&gt; &lt;span class=&#34;citation&#34;&gt;(Version 1.1.5; &lt;a href=&#34;/case-study/mispecification/mispecified/#ref-R-bridgesampling&#34;&gt;Gronau, Singmann, and Wagenmakers 2020&lt;/a&gt;)&lt;/span&gt;, &lt;em&gt;ggplot2&lt;/em&gt; &lt;span class=&#34;citation&#34;&gt;(Version 3.5.2; &lt;a href=&#34;/case-study/mispecification/mispecified/#ref-R-ggplot2&#34;&gt;Wickham 2016&lt;/a&gt;)&lt;/span&gt;, &lt;em&gt;latex2exp&lt;/em&gt; &lt;span class=&#34;citation&#34;&gt;(Version 0.9.6; &lt;a href=&#34;/case-study/mispecification/mispecified/#ref-R-latex2exp&#34;&gt;Meschiari 2022&lt;/a&gt;)&lt;/span&gt;, &lt;em&gt;loo&lt;/em&gt; &lt;span class=&#34;citation&#34;&gt;(Version 2.8.0; &lt;a href=&#34;/case-study/mispecification/mispecified/#ref-R-loo_a&#34;&gt;Vehtari, Gelman, and Gabry 2017&lt;/a&gt;; &lt;a href=&#34;/case-study/mispecification/mispecified/#ref-R-loo_b&#34;&gt;Yao et al. 2017&lt;/a&gt;)&lt;/span&gt;, &lt;em&gt;posterior&lt;/em&gt; &lt;span class=&#34;citation&#34;&gt;(Version 1.6.1; &lt;a href=&#34;/case-study/mispecification/mispecified/#ref-R-posterior&#34;&gt;Vehtari et al. 2021&lt;/a&gt;)&lt;/span&gt;, &lt;em&gt;rstan&lt;/em&gt; &lt;span class=&#34;citation&#34;&gt;(Version 2.36.0.9000; &lt;a href=&#34;/case-study/mispecification/mispecified/#ref-R-rstan&#34;&gt;Stan Development Team 2023a&lt;/a&gt;)&lt;/span&gt;, &lt;em&gt;rtdists&lt;/em&gt; &lt;span class=&#34;citation&#34;&gt;(Version 0.11.5; &lt;a href=&#34;/case-study/mispecification/mispecified/#ref-R-rtdists&#34;&gt;Singmann et al. 2022&lt;/a&gt;)&lt;/span&gt;, &lt;em&gt;StanHeaders&lt;/em&gt; &lt;span class=&#34;citation&#34;&gt;(Version 2.36.0.9000; &lt;a href=&#34;/case-study/mispecification/mispecified/#ref-R-StanHeaders&#34;&gt;Stan Development Team 2020&lt;/a&gt;)&lt;/span&gt; and &lt;em&gt;tidytable&lt;/em&gt; &lt;span class=&#34;citation&#34;&gt;(Version 0.11.2; &lt;a href=&#34;/case-study/mispecification/mispecified/#ref-R-tidytable&#34;&gt;Fairbanks 2023&lt;/a&gt;)&lt;/span&gt; for all our analyses.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34; entry-spacing=&#34;0&#34;&gt;
&lt;div id=&#34;ref-brownSimplestCompleteModel2008&#34; class=&#34;csl-entry&#34;&gt;
Brown, Scott D., and Andrew Heathcote. 2008. &lt;span&gt;“The Simplest Complete Model of Choice Response Time: &lt;span&gt;Linear&lt;/span&gt; Ballistic Accumulation.”&lt;/span&gt; &lt;em&gt;Cognitive Psychology&lt;/em&gt; 57 (3): 153–78. &lt;a href=&#34;https://doi.org/10.1016/j.cogpsych.2007.12.002&#34;&gt;https://doi.org/10.1016/j.cogpsych.2007.12.002&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-R-tidytable&#34; class=&#34;csl-entry&#34;&gt;
Fairbanks, Mark. 2023. &lt;em&gt;Tidytable: Tidy Interface to ’Data.table’&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=tidytable&#34;&gt;https://CRAN.R-project.org/package=tidytable&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-R-bayesplot&#34; class=&#34;csl-entry&#34;&gt;
Gabry, Jonah, Daniel Simpson, Aki Vehtari, Michael Betancourt, and Andrew Gelman. 2019. &lt;span&gt;“Visualization in Bayesian Workflow.”&lt;/span&gt; &lt;em&gt;J. R. Stat. Soc. A&lt;/em&gt; 182: 389–402. &lt;a href=&#34;https://doi.org/10.1111/rssa.12378&#34;&gt;https://doi.org/10.1111/rssa.12378&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-R-bridgesampling&#34; class=&#34;csl-entry&#34;&gt;
Gronau, Quentin F., Henrik Singmann, and Eric-Jan Wagenmakers. 2020. &lt;span&gt;“&lt;span class=&#34;nocase&#34;&gt;bridgesampling&lt;/span&gt;: An &lt;span&gt;R&lt;/span&gt; Package for Estimating Normalizing Constants.”&lt;/span&gt; &lt;em&gt;Journal of Statistical Software&lt;/em&gt; 92 (10): 1–29. &lt;a href=&#34;https://doi.org/10.18637/jss.v092.i10&#34;&gt;https://doi.org/10.18637/jss.v092.i10&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-HeathcoteLove2012&#34; class=&#34;csl-entry&#34;&gt;
Heathcote, Andrew, and Jonathon Love. 2012. &lt;span&gt;“Linear Deterministic Accumulator Models of Simple Choice.”&lt;/span&gt; &lt;em&gt;Frontiers in Psychology&lt;/em&gt; 3: 292. &lt;a href=&#34;https://doi.org/10.3389/fpsyg.2012.00292&#34;&gt;https://doi.org/10.3389/fpsyg.2012.00292&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-R-latex2exp&#34; class=&#34;csl-entry&#34;&gt;
Meschiari, Stefano. 2022. &lt;em&gt;Latex2exp: Use LaTeX Expressions in Plots&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=latex2exp&#34;&gt;https://CRAN.R-project.org/package=latex2exp&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-Nicenboim2024Bayesian&#34; class=&#34;csl-entry&#34;&gt;
Nicenboim, Bruno, D. Schad, and S. Vasishth. 2024. &lt;span&gt;“An Introduction to Bayesian Data Analysis for Cognitive Science.”&lt;/span&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Ollman1966&#34; class=&#34;csl-entry&#34;&gt;
Ollman, Robert. 1966. &lt;span&gt;“Fast Guesses in Choice Reaction Time.”&lt;/span&gt; &lt;em&gt;Psychonomic Science&lt;/em&gt; 6 (4): 155–56. https://doi.org/&lt;a href=&#34;https://doi.org/10.3758/BF03328004&#34;&gt;https://doi.org/10.3758/BF03328004&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-R-base&#34; class=&#34;csl-entry&#34;&gt;
R Core Team. 2024. &lt;em&gt;R: A Language and Environment for Statistical Computing&lt;/em&gt;. Vienna, Austria: R Foundation for Statistical Computing. &lt;a href=&#34;https://www.R-project.org/&#34;&gt;https://www.R-project.org/&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-RouderEtAl2015&#34; class=&#34;csl-entry&#34;&gt;
Rouder, Jeffrey N., Jordan M. Province, Richard D. Morey, Pablo Gomez, and Andrew Heathcote. 2015. &lt;span&gt;“The Lognormal Race: &lt;span&gt;A&lt;/span&gt; Cognitive-Process Model of Choice and Latency with Desirable Psychometric Properties.”&lt;/span&gt; &lt;em&gt;Psychometrika&lt;/em&gt; 80 (2): 491–513. &lt;a href=&#34;https://doi.org/10.1007/s11336-013-9396-3&#34;&gt;https://doi.org/10.1007/s11336-013-9396-3&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-R-rtdists&#34; class=&#34;csl-entry&#34;&gt;
Singmann, Henrik, Scott Brown, Matthew Gretton, and Andrew Heathcote. 2022. &lt;em&gt;Rtdists: Response Time Distributions&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=rtdists&#34;&gt;https://CRAN.R-project.org/package=rtdists&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-R-StanHeaders&#34; class=&#34;csl-entry&#34;&gt;
Stan Development Team. 2020. &lt;span&gt;“&lt;span&gt;StanHeaders&lt;/span&gt;: Headers for the &lt;span&gt;R&lt;/span&gt; Interface to &lt;span&gt;Stan&lt;/span&gt;.”&lt;/span&gt; &lt;a href=&#34;https://mc-stan.org/&#34;&gt;https://mc-stan.org/&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-R-rstan&#34; class=&#34;csl-entry&#34;&gt;
———. 2023a. &lt;span&gt;“&lt;span&gt;RStan&lt;/span&gt;: The &lt;span&gt;R&lt;/span&gt; Interface to &lt;span&gt;Stan&lt;/span&gt;.”&lt;/span&gt; &lt;a href=&#34;https://mc-stan.org/&#34;&gt;https://mc-stan.org/&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-Stan2023&#34; class=&#34;csl-entry&#34;&gt;
———. 2023b. &lt;span&gt;“Stan Modeling Language Users Guide and Reference Manual, Version 2.32.”&lt;/span&gt; &lt;a href=&#34;https://mc-stan.org/docs/2_32/reference-manual/index.html;%20https://mc-stan.org/docs/2_32/stan-users-guide/index.html&#34;&gt;https://mc-stan.org/docs/2_32/reference-manual/index.html; https://mc-stan.org/docs/2_32/stan-users-guide/index.html&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-Terry2015&#34; class=&#34;csl-entry&#34;&gt;
Terry, Andrew, A. A. J. Marley, Avinash Barnwal, E.-J. Wagenmakers, Andrew Heathcote, and Scott D. Brown. 2015. &lt;span&gt;“Generalising the Drift Rate Distribution for Linear Ballistic Accumulators.”&lt;/span&gt; &lt;em&gt;Journal of Mathematical Psychology&lt;/em&gt; 68-69 (October): 49–58. &lt;a href=&#34;https://doi.org/10.1016/j.jmp.2015.09.002&#34;&gt;https://doi.org/10.1016/j.jmp.2015.09.002&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-R-loo_a&#34; class=&#34;csl-entry&#34;&gt;
Vehtari, Aki, Andrew Gelman, and Jonah Gabry. 2017. &lt;span&gt;“Practical Bayesian Model Evaluation Using Leave-One-Out Cross-Validation and WAIC.”&lt;/span&gt; &lt;em&gt;Statistics and Computing&lt;/em&gt; 27: 1413–32. &lt;a href=&#34;https://doi.org/10.1007/s11222-016-9696-4&#34;&gt;https://doi.org/10.1007/s11222-016-9696-4&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-R-posterior&#34; class=&#34;csl-entry&#34;&gt;
Vehtari, Aki, Andrew Gelman, Daniel Simpson, Bob Carpenter, and Paul-Christian Bürkner. 2021. &lt;span&gt;“Rank-Normalization, Folding, and Localization: An Improved Rhat for Assessing Convergence of MCMC (with Discussion).”&lt;/span&gt; &lt;em&gt;Bayesian Analysis&lt;/em&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-R-ggplot2&#34; class=&#34;csl-entry&#34;&gt;
Wickham, Hadley. 2016. &lt;em&gt;Ggplot2: Elegant Graphics for Data Analysis&lt;/em&gt;. Springer-Verlag New York. &lt;a href=&#34;https://ggplot2.tidyverse.org&#34;&gt;https://ggplot2.tidyverse.org&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-R-loo_b&#34; class=&#34;csl-entry&#34;&gt;
Yao, Yuling, Aki Vehtari, Daniel Simpson, and Andrew Gelman. 2017. &lt;span&gt;“Using Stacking to Average Bayesian Predictive Distributions.”&lt;/span&gt; &lt;em&gt;Bayesian Analysis&lt;/em&gt;. &lt;a href=&#34;https://doi.org/10.1214/17-BA1091&#34;&gt;https://doi.org/10.1214/17-BA1091&lt;/a&gt;.
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Case study: Mixture modeling of nasalization in Basque</title>
      <link>/case-study/mixture/mixture-nasal/</link>
      <pubDate>Fri, 13 Jun 2025 00:00:00 +0000</pubDate>
      
      <guid>/case-study/mixture/mixture-nasal/</guid><description>


&lt;pre class=&#34;r fold-hide&#34;&gt;&lt;code&gt;## Global options
options(max.print = &amp;quot;75&amp;quot;,
        width = 80,
        tibble.width = 80)
options(scipen = 1, digits = 2)
knitr::opts_chunk$set(echo = TRUE,
                    cache = TRUE,
                    prompt = FALSE,
                    cache.lazy = FALSE,
                    tidy = FALSE,
                    comment = NA,
                    message = FALSE,
                    warning = TRUE)
knitr::opts_knit$set(width = 80)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)&lt;/code&gt;&lt;/pre&gt;
&lt;style&gt;
.exercise {
  border: 2px solid #007ACC;
  background-color: #f0f8ff;
  padding: 1em;
  margin: 1em 0;
  border-radius: 6px;
}

.exercise h3 {
  margin-top: 0;
  color: #005a9c;
}

.note {
  border-left: 5px solid #1e90ff;
  background-color: #eef6fc;
  padding: 1em;
  margin: 1em 0;
}

.warning {
  border-left: 5px solid #d9534f;
  background-color: #fdf2f2;
  padding: 1em;
  margin: 1em 0;
}
&lt;/style&gt;
&lt;div id=&#34;introduction&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;This case study examines how &lt;em&gt;nasalance&lt;/em&gt; varies across three etymological categories of Basque aspirates: &lt;strong&gt;oral&lt;/strong&gt;, &lt;strong&gt;contact-nasalized&lt;/strong&gt; (i.e., following a nasal consonant), and historically &lt;strong&gt;nasalized&lt;/strong&gt; aspirates that are reported to be in flux.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;set-up&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Set-up&lt;/h1&gt;
&lt;div id=&#34;packages&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Packages&lt;/h2&gt;
&lt;p&gt;The analysis relies on &lt;strong&gt;tidytable&lt;/strong&gt; for fast data manipulation, &lt;strong&gt;cmdstanr&lt;/strong&gt; for Stan interfacing, and a handful of tidy-Bayes helpers for visualisation and model comparison.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidytable) # nice alternative to dplyr
library(cmdstanr)
library(posterior)
library(bayesplot)
library(brms)
library(ggplot2)
library(loo)
options(mc.cores = 4)
knitr::write_bib(.packages(), &amp;quot;r-references.bib&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;files&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Files&lt;/h2&gt;
&lt;p&gt;We’ll use the following files&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;./nas_data.RDS&#34;&gt;&lt;code&gt;nas_data.RDS&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;./beta2.stan&#34;&gt;&lt;code&gt;beta2.stan&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;./mix.stan&#34;&gt;&lt;code&gt;mix.stan&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;./nonmix.stan&#34;&gt;&lt;code&gt;nonmix.stan&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;./mix_s.stan&#34;&gt;&lt;code&gt;mix_s.stan&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;./mix_s2.stan&#34;&gt;&lt;code&gt;mix_s2.stan&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;./mixture-nasal.Rmd&#34;&gt;&lt;code&gt;mixture-nasal.Rmd&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;data-from-iñigo-urrestarazu-porta&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data from Iñigo Urrestarazu-Porta&lt;/h2&gt;
&lt;p&gt;Basque speakers of the Zuberoan variety were recorded with a nasometer. The etymology has three relevant levels:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;contact&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;nasalized&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;oral&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r fold-hide&#34;&gt;&lt;code&gt;nas_data &amp;lt;- readRDS(&amp;quot;nas_data.RDS&amp;quot;) |&amp;gt;
  rename(
    etymology = E,
    root = R,
    speaker = S,
    nasalance = N
  ) |&amp;gt;
  filter(!is.na(nasalance)) |&amp;gt;
  filter(etymology != &amp;quot;assimilated&amp;quot;) |&amp;gt;
  mutate(
    E = as.factor(etymology) |&amp;gt; as.numeric(),
    R = as.factor(root) |&amp;gt; as.numeric(),
    speaker = as.factor(speaker) |&amp;gt; as.numeric()
  )&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nas_data&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tidytable: 699 × 6
   speaker root     etymology nasalance     E     R
     &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;         &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
 1       1 diharu   nasalized     0.102     2    26
 2       1 ihakin   oral          0.136     3    37
 3       1 ihaute   nasalized     0.170     2    41
 4       1 ihintz   nasalized     0.219     2    44
 5       1 ahate    nasalized     0.232     2     8
 6       1 zehe     nasalized     0.144     2    64
 7       1 kahixka  nasalized     0.300     2    46
 8       1 ehe-no   nasalized     0.128     2    28
 9       1 ehe-no   nasalized     0.128     2    28
10       1 halahola oral          0.215     3    36
# ℹ 689 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The quick summary below confirms the expected ranking: &lt;em&gt;contact&lt;/em&gt; tokens are the most nasal, &lt;em&gt;oral&lt;/em&gt; tokens the least, and the &lt;em&gt;nasalized&lt;/em&gt; class falls somewhere in between.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(summary &amp;lt;- nas_data |&amp;gt;
  summarize(
    mean = mean(nasalance),
    sd = sd(nasalance),
    SE = sd(nasalance) / sqrt(n()),
    lq = quantile(nasalance, 0.025),
    hq = quantile(nasalance, 0.975),
    .by = c(&amp;quot;etymology&amp;quot;, &amp;quot;E&amp;quot;)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tidytable: 3 × 7
  etymology     E  mean     sd      SE     lq    hq
  &amp;lt;chr&amp;gt;     &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
1 contact       1 0.543 0.115  0.0145  0.310  0.749
2 nasalized     2 0.233 0.176  0.00863 0.0536 0.719
3 oral          3 0.166 0.0993 0.00668 0.0536 0.460&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r fold-hide&#34;&gt;&lt;code&gt;ggplot(nas_data, aes(x = nasalance, fill = etymology, color = etymology)) +
  geom_density(alpha = 0.5, linewidth = 1.2) +
  labs(
    title = &amp;quot;Distribution of nasalance by etymology&amp;quot;,
    x = &amp;quot;Nasalance (N)&amp;quot;,
    y = &amp;quot;Density&amp;quot;
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/case-study/mixture/mixture-nasal//case-study/mixture/mixture-nasal_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We are modelling proportions of nasalance, with a categorical predictor (etymological category or group), to try to understand if &lt;em&gt;some&lt;/em&gt; aspirates /h/ are nasalized in Basque.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Assumptions:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Orals&lt;/strong&gt; should have low proportion values (~20%).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Contact&lt;/strong&gt; refers to aspirates after a nasal consonant, so they should have high values (~50–60%).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Nasalized&lt;/strong&gt; aspirates are being lost in the language:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Some speakers never produce them as such (N ~20%).&lt;/li&gt;
&lt;li&gt;Even among those who maintain the distinction, some lexical items have lost nasality.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The core research question is therefore whether the &lt;em&gt;nasalized&lt;/em&gt; category is best described by a &lt;strong&gt;mixture&lt;/strong&gt; of two latent sub-processes.&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;modeling-the-mixture-of-two-components&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Modeling the mixture of two components&lt;/h1&gt;
&lt;p&gt;We focus here on a two-component mixture model &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;/case-study/mixture/mixture-nasal/#ref-NicenboimEtAl2025&#34;&gt;Nicenboim, Schad, and Vasishth 2025&lt;/a&gt;)&lt;/span&gt;, where each component represents a distinct cognitive process. The latent vector &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{z}\)&lt;/span&gt; indicates which component an observation belongs to:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34; id=&#34;eq:mixz&#34;&gt;\[\begin{equation}
\begin{aligned}
z_n \sim \mathit{Bernoulli}(\theta)\\
y_n \sim
\begin{cases}
p_1(\Theta_1), &amp;amp; \text{ if } z_n =1 \\
p_2(\Theta_2), &amp;amp; \text{ if } z_n=0
\end{cases}
\end{aligned}
\tag{1}
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In this specific case, &lt;span class=&#34;math inline&#34;&gt;\(y_n\)&lt;/span&gt; is the observed nasalance of a historically nasalized root, and the latent binary indicator &lt;span class=&#34;math inline&#34;&gt;\(z_n\)&lt;/span&gt; determines whether the token was produced via the &lt;em&gt;contact-like&lt;/em&gt; process &lt;span class=&#34;math inline&#34;&gt;\((p_1)\)&lt;/span&gt; or the &lt;em&gt;oral-like&lt;/em&gt; process &lt;span class=&#34;math inline&#34;&gt;\((p_2)\)&lt;/span&gt;.&lt;/p&gt;
&lt;div id=&#34;data-simulation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data simulation&lt;/h2&gt;
&lt;p&gt;In the next few chunks we generate two artificial data sets so that we can stress-test our models: one &lt;strong&gt;true mixture&lt;/strong&gt; and one &lt;strong&gt;true non-mixture&lt;/strong&gt;. We ignore subject‑level variability for now.&lt;/p&gt;
&lt;div id=&#34;a-custom-beta-parameterisation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;A custom Beta parameterisation&lt;/h3&gt;
&lt;p&gt;Before simulating, we re-express the Beta distribution in terms of a more intuitive mean/&lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; parameterisation. The helper functions &lt;code&gt;beta_pars()&lt;/code&gt; and &lt;code&gt;rbeta2()&lt;/code&gt; &lt;em&gt;only&lt;/em&gt; wrap the algebra–they do not affect inference later on, but they make simulation vastly more readable.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{var} &amp;lt; \mu (1 - \mu), \quad
\nu = \frac{\mu(1 - \mu)}{\text{var}} - 1, \quad
\alpha = \mu \nu, \quad \beta = (1 - \mu) \nu
\]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;beta_pars &amp;lt;- function(mean, sd) {
  var &amp;lt;- sd^2
  alpha &amp;lt;- ((1 - mean) / var - 1 / mean) * mean^2
  beta &amp;lt;- alpha * (1 / mean - 1)
  list(alpha = alpha, beta = beta)
}

rbeta2 &amp;lt;- function(n, mean, sd) {
  pars &amp;lt;- beta_pars(mean, sd)
  rbeta(n = n, shape1 = pars$alpha, shape2 = pars$beta)
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;simulate-data-from-a-mixture-process&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Simulate data from a &lt;em&gt;mixture&lt;/em&gt; process&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;prob_nas&lt;/code&gt; below is the root-level probability that a historically nasalised aspirate is realised via a “high-nasalance mechanism” (as in contact condition). By varying this single probability you can dial-in anything from a pure-contact to a pure-oral realisation.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(99)
N &amp;lt;- 100

true &amp;lt;- list(
  mu_oral = 0.166,
  sigma_oral = 0.0993,
  mu_contact = 0.543,
  sigma_contact = 0.115,
  prob_nas = 0.15 # made-up probability of nasalization
)

nas_oral &amp;lt;- rbeta2(N, mean = true$mu_oral, sd = true$sigma_oral)
nas_contact &amp;lt;- rbeta2(N, mean = true$mu_contact, sd = true$sigma_contact)

is_nas &amp;lt;- rbinom(N, size = 1, prob = true$prob_nas)
nas_nasalized &amp;lt;- ifelse(
  is_nas == 1,
  rbeta2(N, mean = true$mu_contact, sd = true$sigma_contact), # contact
  rbeta2(N, mean = true$mu_oral, sd = true$sigma_oral)         # oral
)

mix_data &amp;lt;- tidytable(
  etymology = rep(c(&amp;quot;oral&amp;quot;, &amp;quot;contact&amp;quot;, &amp;quot;nasalized&amp;quot;), each = N),
  nasalance = c(nas_oral, nas_contact, nas_nasalized)
) |&amp;gt;
  mutate(E = as.factor(etymology) |&amp;gt; as.numeric())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Check distribution and visualize:&lt;/p&gt;
&lt;pre class=&#34;r fold-hide&#34;&gt;&lt;code&gt;mix_data |&amp;gt;
  summarize(
    mean = mean(nasalance),
    sd = sd(nasalance),
    SE = sd(nasalance) / sqrt(n()),
    lq = quantile(nasalance, 0.025),
    hq = quantile(nasalance, 0.975),
    .by = c(&amp;quot;etymology&amp;quot;, &amp;quot;E&amp;quot;)
  )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tidytable: 3 × 7
  etymology     E  mean     sd      SE     lq    hq
  &amp;lt;chr&amp;gt;     &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
1 contact       1 0.551 0.119  0.0119  0.323  0.744
2 nasalized     2 0.215 0.169  0.0169  0.0203 0.687
3 oral          3 0.157 0.0895 0.00895 0.0330 0.374&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r fold-hide&#34;&gt;&lt;code&gt;ggplot(mix_data, aes(x = nasalance, fill = etymology, color = etymology)) +
  geom_density(alpha = 0.5, linewidth = 1.2) +
  labs(
    title = &amp;quot;SIMULATED distribution of nasalance by etymology (mixture)&amp;quot;,
    x = &amp;quot;Nasalance (N)&amp;quot;,
    y = &amp;quot;Density&amp;quot;
  ) +
  theme_minimal()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/case-study/mixture/mixture-nasal//case-study/mixture/mixture-nasal_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;div class=&#34;note&#34;&gt;
&lt;p&gt;Experiment with different values of &lt;code&gt;prob_nas&lt;/code&gt; and observe the effect on the distribution.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;simulate-data-from-a-non-mixture-process&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Simulate data from a &lt;em&gt;non-mixture&lt;/em&gt; process&lt;/h3&gt;
&lt;p&gt;Here every category gets its &lt;em&gt;own&lt;/em&gt; Beta distribution. The parameters for &lt;em&gt;oral&lt;/em&gt; and &lt;em&gt;contact&lt;/em&gt; are the same as above; the &lt;em&gt;nasalized&lt;/em&gt; mean/sd are set to match the empirical data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(99)

true2 &amp;lt;- list(
  mu_oral = true$mu_oral,
  sigma_oral = true$sigma_oral,
  mu_contact = true$mu_contact,
  sigma_contact = true$sigma_contact,
  mu_nasalized = 0.233,
  sigma_nasalized = 0.176
)
nas_oral &amp;lt;- rbeta2(N, mean = true2$mu_oral, sd = true2$sigma_oral)
nas_contact &amp;lt;- rbeta2(N, mean = true2$mu_contact, sd = true2$sigma_contact)
nas_nasalized2 &amp;lt;- rbeta2(N, mean = true2$mu_nasalized, sd = true2$sigma_nasalized)

nonmix_data &amp;lt;- tidytable(
  etymology = rep(c(&amp;quot;oral&amp;quot;, &amp;quot;contact&amp;quot;, &amp;quot;nasalized&amp;quot;), each = N),
  nasalance = c(nas_oral, nas_contact, nas_nasalized2)
) |&amp;gt;
  mutate(E = as.factor(etymology) |&amp;gt; as.numeric())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Check distribution and visualize:&lt;/p&gt;
&lt;pre class=&#34;r fold-hide&#34;&gt;&lt;code&gt;nonmix_data |&amp;gt; summarize(mean = mean(nasalance),
            sd = sd(nasalance),
            SE = mean(nasalance)/sqrt(n()),
            lq = quantile(nasalance, 0.025),
            hq = quantile(nasalance, 0.975),
            .by = c(&amp;quot;etymology&amp;quot;,&amp;quot;E&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tidytable: 3 × 7
  etymology     E  mean     sd     SE     lq    hq
  &amp;lt;chr&amp;gt;     &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
1 contact       1 0.551 0.119  0.0551 0.323  0.744
2 nasalized     2 0.231 0.183  0.0231 0.0110 0.641
3 oral          3 0.157 0.0895 0.0157 0.0330 0.374&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r fold-hide&#34;&gt;&lt;code&gt;ggplot(nonmix_data, aes(x = nasalance, fill = etymology, color = etymology))  +
  geom_density(alpha = 0.5, linewidth = 1.2) +
  labs(title = &amp;quot;SIMULATED distribution of nasalance by etymology&amp;quot;,
       x = &amp;quot;N&amp;quot;,
       y = &amp;quot;Density&amp;quot;) +
  theme_minimal()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/case-study/mixture/mixture-nasal//case-study/mixture/mixture-nasal_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Both distributions are visually similar, underscoring the difficulty of distinguishing the underlying generative processes by eye alone.&lt;/strong&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;fitting-the-simulated-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Fitting the simulated data&lt;/h2&gt;
&lt;p&gt;We next fit the simulated data sets with &lt;em&gt;both&lt;/em&gt; a mixture and a non-mixture model. We expect the following:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;the mixture model should outperform when the data are a true mixture; and&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;the non-mixture model should win when the data really come from three separate Beta distributions.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;hr /&gt;
&lt;div id=&#34;stan-implementation-two-component-mixture&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Stan implementation: two-component mixture&lt;/h3&gt;
&lt;p&gt;The Stan program below encodes the likelihood&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
  p(y_n\mid\Theta) = \theta\,p_1(y_n\mid\Theta_1) + (1-\theta)\,p_2(y_n\mid\Theta_2),
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(p_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(p_2\)&lt;/span&gt; are Beta distributions parameterised by their means and standard deviations. Because Stan does not allow discrete parameters in the &lt;code&gt;parameters&lt;/code&gt; block, we marginalise over the latent indicator $z_n$ via &lt;code&gt;log_sum_exp&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Stan model:&lt;/p&gt;
&lt;pre class=&#34;stan&#34;&gt;&lt;code&gt;#include beta2.stan
data {
  int&amp;lt;lower = 1&amp;gt; N;
  vector[N] nasalance;
  array[N] int&amp;lt;lower = 1&amp;gt; E;
}
parameters {
  real&amp;lt;lower = 0, upper = 1&amp;gt; mu_contact;
  real&amp;lt;lower = 0, upper = sqrt(mu_contact*(1-mu_contact))&amp;gt; sigma_contact;
  real&amp;lt;lower = 0, upper = 1 &amp;gt; mu_oral;
  real&amp;lt;lower= 0, upper = sqrt(mu_oral*(1-mu_oral))&amp;gt; sigma_oral;
  real&amp;lt;lower = 0, upper = 1&amp;gt; prob_nas;
}
model {
  // priors for the task component
  target += normal_lpdf(mu_contact | 0.6, 0.2);
  target += normal_lpdf(mu_oral | 0.2, 0.3);
  target += normal_lpdf(sigma_oral | 0, 0.3);
  target += normal_lpdf(sigma_contact | 0, 0.3);

  target += beta_lpdf(prob_nas | 3, 10);

  for(n in 1:N){
    if(E[n] == 1)  // contact
      target +=  beta2_lpdf(nasalance[n] | mu_contact, sigma_contact);
     else if(E[n] == 3)  // oral
       target += beta2_lpdf(nasalance[n] | mu_oral, sigma_oral);
    else if(E[n] == 2) //nas
       target += log_sum_exp(log(prob_nas) +
                  beta2_lpdf(nasalance[n] | mu_contact, sigma_contact),
                  log1m(prob_nas) +
                  beta2_lpdf(nasalance[n] | mu_oral, sigma_oral));
  }
}
generated quantities {
  vector[N] log_lik;
 for(n in 1:N){
    if(E[n] == 1)  // contact
      log_lik[n] =  beta2_lpdf(nasalance[n] | mu_contact, sigma_contact);
     else if(E[n] == 3)  // oral
       log_lik[n] = beta2_lpdf(nasalance[n] | mu_oral, sigma_oral);
     else if(E[n] == 2) //nas
       log_lik[n] = log_sum_exp(log(prob_nas) +
                                beta2_lpdf(nasalance[n] | mu_contact, sigma_contact),
                                log1m(prob_nas) +
                                beta2_lpdf(nasalance[n] | mu_oral, sigma_oral));
  }

  // Posterior predictions:
  vector[N] nasalance_rep;
  for (n in 1:N) {
    if (E[n] == 1)
      nasalance_rep[n] = beta2_rng(mu_contact, sigma_contact);
    else if (E[n] == 3)
      nasalance_rep[n] = beta2_rng(mu_oral, sigma_oral);
    else if (E[n] == 2) {
      if (bernoulli_rng(prob_nas) == 1)
        nasalance_rep[n] = beta2_rng(mu_contact, sigma_contact);
      else
        nasalance_rep[n] = beta2_rng(mu_oral, sigma_oral);
    }
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mix_m &amp;lt;- cmdstan_model(&amp;quot;./mix.stan&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;model-fits&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Model fits&lt;/h3&gt;
&lt;p&gt;Posterior recovery plots (&lt;code&gt;mcmc_recover_hist&lt;/code&gt;) show that all five parameters are recovered when the data truly come from a mixture. Notably, the same Stan model also &lt;em&gt;fits&lt;/em&gt; the non-mixture data well.&lt;/p&gt;
&lt;div id=&#34;fiting-data-generated-by-a-non-mixture-process-with-a-mixture-model&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Fiting data generated by a non-mixture process with a mixture model&lt;/h4&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mix_ls &amp;lt;- list(
  N = nrow(mix_data),
  nasalance = mix_data$nasalance,
  E = as.integer(mix_data$E)
)

mixm_mixd &amp;lt;- mix_m$sample(data = mix_ls, parallel_chains = 4)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Parameter recovery:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bayesplot::mcmc_recover_hist(
  x = mixm_mixd$draws(c(&amp;quot;mu_oral&amp;quot;, &amp;quot;sigma_oral&amp;quot;, &amp;quot;mu_contact&amp;quot;, &amp;quot;sigma_contact&amp;quot;, &amp;quot;prob_nas&amp;quot;)),
  true = unlist(true)
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/case-study/mixture/mixture-nasal//case-study/mixture/mixture-nasal_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;yrep &amp;lt;- mixm_mixd$draws(&amp;quot;nasalance_rep&amp;quot;) |&amp;gt; posterior::as_draws_matrix()
ppc_dens_overlay_grouped(y = mix_data$nasalance, yrep = yrep[1:100, ], group = mix_data$etymology)+ ggtitle(&amp;quot;Mixture model on mixture data&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/case-study/mixture/mixture-nasal//case-study/mixture/mixture-nasal_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;fitting-non-mixture-data-with-a-mixture-model&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Fitting non-mixture data with a mixture model&lt;/h4&gt;
&lt;p&gt;Data not generated by a mixture model can be perfectly fit by a mixture model!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nonmix_ls &amp;lt;- list(
  N = nrow(nonmix_data),
  nasalance = nonmix_data$nasalance,
  E = as.integer(nonmix_data$E)
)

mixm_nonmixd &amp;lt;- mix_m$sample(data = nonmix_ls, parallel_chains = 4)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;yrep &amp;lt;- mixm_nonmixd$draws(&amp;quot;nasalance_rep&amp;quot;) |&amp;gt; posterior::as_draws_matrix()
ppc_dens_overlay_grouped(y = nonmix_data$nasalance, yrep = yrep[1:100, ],group = nonmix_data$etymology)+ ggtitle(&amp;quot;Mixture model on non-mixture data&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/case-study/mixture/mixture-nasal//case-study/mixture/mixture-nasal_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;alternative-model-non-mixture-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Alternative model: non-mixture model&lt;/h2&gt;
&lt;p&gt;This alternative Stan program replaces the Bernoulli mixture with a dedicated Beta for the nasalised class. Everything else remains the same. This model therefore embodies the (maybe unrealistic) hypothesis that historically nasalised aspirates form a coherent third distribution rather than a mixture of two.&lt;/p&gt;
&lt;p&gt;Stan model:&lt;/p&gt;
&lt;pre class=&#34;stan&#34;&gt;&lt;code&gt;#include beta2.stan
data {
  int&amp;lt;lower = 1&amp;gt; N;
  vector[N] nasalance;
  array[N] int&amp;lt;lower = 1&amp;gt; E;
}
parameters {
  real&amp;lt;lower = 0, upper = 1&amp;gt; mu_contact;
  real&amp;lt;lower = 0, upper = sqrt(mu_contact*(1-mu_contact))&amp;gt; sigma_contact;
  real&amp;lt;lower = 0, upper = 1 &amp;gt; mu_oral;
  real&amp;lt;lower= 0, upper = sqrt(mu_oral*(1-mu_oral))&amp;gt; sigma_oral;
 real&amp;lt;lower = 0, upper = 1&amp;gt; mu_nasalized;
  real&amp;lt;lower = 0, upper = sqrt(mu_nasalized*(1-mu_nasalized))&amp;gt; sigma_nasalized;
}
model {
  // priors for the task component
  target += normal_lpdf(mu_contact | 0.6, 0.2);
  target += normal_lpdf(mu_oral | 0.2, 0.3);
  target += normal_lpdf(mu_nasalized | 0.2, 0.3);
  target += normal_lpdf(sigma_oral | 0, 0.3);
  target += normal_lpdf(sigma_contact | 0, 0.3);
  target += normal_lpdf(sigma_nasalized | 0, 0.3);

  for(n in 1:N){
    if(E[n] == 1)  // contact
      target +=  beta2_lpdf(nasalance[n] | mu_contact, sigma_contact);
     else if(E[n] == 3)  // oral
       target += beta2_lpdf(nasalance[n] | mu_oral, sigma_oral);
    else if(E[n] == 2) //nas
       target += beta2_lpdf(nasalance[n] | mu_nasalized, sigma_nasalized);
  }
}
generated quantities {
  vector[N] log_lik;
  vector[N] nasalance_rep;

  for (n in 1:N) {
    if (E[n] == 1) {  // contact
      log_lik[n] = beta2_lpdf(nasalance[n] | mu_contact, sigma_contact);
      nasalance_rep[n] = beta2_rng(mu_contact, sigma_contact);
    } else if (E[n] == 3) {  // oral
      log_lik[n] = beta2_lpdf(nasalance[n] | mu_oral, sigma_oral);
      nasalance_rep[n] = beta2_rng(mu_oral, sigma_oral);
    } else if (E[n] == 2) {  // nasalized
      log_lik[n] = beta2_lpdf(nasalance[n] | mu_nasalized, sigma_nasalized);
      nasalance_rep[n] = beta2_rng(mu_nasalized, sigma_nasalized);
    }
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;fitting-mixture-and-non-mixture-data-with-a-non-mixture-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Fitting mixture and non-mixture data with a non-mixture model&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nonmix_m &amp;lt;- cmdstan_model(&amp;quot;./nonmix.stan&amp;quot;)

nonmixm_mixd &amp;lt;- nonmix_m$sample(data = mix_ls, parallel_chains = 4)
nonmixm_nonmixd &amp;lt;- nonmix_m$sample(data = nonmix_ls, parallel_chains = 4)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nonmixm_mixd&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;        variable   mean median   sd  mad     q5    q95 rhat ess_bulk ess_tail
 lp__            223.15 223.47 1.75 1.56 219.71 225.34 1.00     1813     2718
 mu_contact        0.55   0.55 0.01 0.01   0.53   0.57 1.00     4616     2815
 sigma_contact     0.12   0.12 0.01 0.01   0.11   0.13 1.00     4713     2760
 mu_oral           0.16   0.16 0.01 0.01   0.14   0.17 1.00     4842     3373
 sigma_oral        0.10   0.10 0.01 0.01   0.08   0.11 1.00     4589     3023
 mu_nasalized      0.22   0.22 0.02 0.02   0.20   0.25 1.00     4331     2777
 sigma_nasalized   0.16   0.16 0.01 0.01   0.14   0.18 1.00     4025     2499
 log_lik[1]        1.12   1.12 0.07 0.07   1.00   1.24 1.00     5690     3494
 log_lik[2]        1.24   1.24 0.07 0.07   1.11   1.35 1.00     5416     3472
 log_lik[3]        0.84   0.84 0.08 0.08   0.70   0.97 1.00     5775     2994

 # showing 10 of 607 rows (change via &amp;#39;max_rows&amp;#39; argument or &amp;#39;cmdstanr_max_rows&amp;#39; option)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nonmixm_nonmixd&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;        variable   mean median   sd  mad     q5    q95 rhat ess_bulk ess_tail
 lp__            214.49 214.84 1.75 1.53 211.07 216.67 1.00     2090     2769
 mu_contact        0.55   0.55 0.01 0.01   0.53   0.57 1.00     4788     3191
 sigma_contact     0.12   0.12 0.01 0.01   0.11   0.13 1.00     4389     2846
 mu_oral           0.16   0.16 0.01 0.01   0.14   0.17 1.00     4649     3389
 sigma_oral        0.10   0.10 0.01 0.01   0.08   0.11 1.00     4673     2821
 mu_nasalized      0.23   0.23 0.02 0.02   0.21   0.26 1.00     4673     3239
 sigma_nasalized   0.18   0.18 0.01 0.01   0.16   0.20 1.00     3891     2795
 log_lik[1]        1.12   1.13 0.07 0.07   1.00   1.24 1.00     4995     3031
 log_lik[2]        1.24   1.24 0.07 0.07   1.12   1.35 1.00     4894     3185
 log_lik[3]        0.84   0.85 0.08 0.08   0.70   0.97 1.00     5106     3449

 # showing 10 of 607 rows (change via &amp;#39;max_rows&amp;#39; argument or &amp;#39;cmdstanr_max_rows&amp;#39; option)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Recovery of the parameters of the non-mixture with the non-mixture model:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bayesplot::mcmc_recover_hist(
  x = nonmixm_nonmixd$draws(c(&amp;quot;mu_oral&amp;quot;, &amp;quot;sigma_oral&amp;quot;, &amp;quot;mu_contact&amp;quot;, &amp;quot;sigma_contact&amp;quot;, &amp;quot;mu_nasalized&amp;quot;, &amp;quot;sigma_nasalized&amp;quot;)),
  true = unlist(true2)
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/case-study/mixture/mixture-nasal//case-study/mixture/mixture-nasal_files/figure-html/unnamed-chunk-20-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;yrep_mixfit &amp;lt;- nonmixm_mixd$draws(&amp;quot;nasalance_rep&amp;quot;) |&amp;gt; posterior::as_draws_matrix()
ppc_dens_overlay_grouped(
  y = mix_data$nasalance,
  yrep = yrep_mixfit[1:100, ],  
  group = mix_data$etymology
) + ggtitle(&amp;quot;Non-mixture model on mixture data&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/case-study/mixture/mixture-nasal//case-study/mixture/mixture-nasal_files/figure-html/unnamed-chunk-21-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;yrep_nonmixfit &amp;lt;- nonmixm_nonmixd$draws(&amp;quot;nasalance_rep&amp;quot;) |&amp;gt; posterior::as_draws_matrix()
ppc_dens_overlay_grouped(
  y = nonmix_data$nasalance,
  yrep = yrep_nonmixfit[1:100, ],
  group = nonmix_data$etymology
) + ggtitle(&amp;quot;Non-mixture model on non-mixture data&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/case-study/mixture/mixture-nasal//case-study/mixture/mixture-nasal_files/figure-html/unnamed-chunk-22-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;model-comparison&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Model comparison&lt;/h2&gt;
&lt;p&gt;Model comparison via approximate leave-one-out cross-validation (LOO-CV) is rather inconclusive (in this small-data setting):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;loo_mixm_mixd &amp;lt;- loo(mixm_mixd$draws(&amp;quot;log_lik&amp;quot;))
loo_mixm_nonmixd &amp;lt;- loo(mixm_nonmixd$draws(&amp;quot;log_lik&amp;quot;))
loo_nonmixm_mixd &amp;lt;- loo(nonmixm_mixd$draws(&amp;quot;log_lik&amp;quot;))
loo_nonmixm_nonmixd &amp;lt;- loo(nonmixm_nonmixd$draws(&amp;quot;log_lik&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;data-generated-by-a-mixture-process&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Data generated by a mixture process&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;loo_compare(list(mixture_model = loo_mixm_mixd,
                 non_mixture   = loo_nonmixm_mixd))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;              elpd_diff se_diff
mixture_model  0.0       0.0   
non_mixture   -8.1       4.1   &lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;data-generated-by-a-nonmixture-process&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Data generated by a non‑mixture process&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;loo_compare(list(mixture_model = loo_mixm_nonmixd,
                 non_mixture   = loo_nonmixm_nonmixd))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;              elpd_diff se_diff
non_mixture    0.0       0.0   
mixture_model -2.1       3.8   &lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;note&#34;&gt;
&lt;p&gt;Re-run everything changing both seeds to 123. What changes?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;hierarchical-extensions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Hierarchical extensions&lt;/h2&gt;
&lt;p&gt;Real speakers differ in how faithfully they produce nasalised aspirates. The next two Stan programs introduce &lt;strong&gt;speaker-level group effects&lt;/strong&gt; on the mixture weight and, in the final version, also on the contact/oral means via a Cholesky-factor LKJ prior. The high-level structure is unchanged.&lt;/p&gt;
&lt;div id=&#34;subject-level-adjustments-for-the-mixture-weight&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Subject level adjustments for the mixture weight&lt;/h3&gt;
&lt;pre class=&#34;stan&#34;&gt;&lt;code&gt;#include beta2.stan
data {
  int&amp;lt;lower = 1&amp;gt; N;
  vector[N] nasalance;
  array[N] int&amp;lt;lower = 1&amp;gt; E;
  array[N] int&amp;lt;lower = 1&amp;gt; speaker;
  int&amp;lt;lower = 1&amp;gt; N_speaker;
}
parameters {
  real&amp;lt;lower = 0, upper = 1&amp;gt; mu_contact;
  real&amp;lt;lower = 0, upper = sqrt(mu_contact*(1-mu_contact))&amp;gt; sigma_contact;
  real&amp;lt;lower = 0, upper = 1 &amp;gt; mu_oral;
  real&amp;lt;lower= 0, upper = sqrt(mu_oral*(1-mu_oral))&amp;gt; sigma_oral;

  // now it&amp;#39;s log odds
  real&amp;lt;lower = 0, upper = 1&amp;gt; av_prob_nas;

  real&amp;lt;lower = 0&amp;gt;  tau_u;
  vector[N_speaker] z_u;
}
transformed parameters {
  vector[N_speaker] u;
  u = tau_u * z_u;
}
model {
  // priors for the task component
  target += normal_lpdf(mu_contact | 0.6, 0.2);
  target += normal_lpdf(mu_oral | 0.2, 0.3);
  target += normal_lpdf(sigma_oral | 0, 0.3);
  target += normal_lpdf(sigma_contact | 0, 0.3);

  target += std_normal_lpdf(z_u);
  target += normal_lpdf(tau_u | 0, 2);
  
  target += beta_lpdf(av_prob_nas | 3, 10);

  for(n in 1:N){
    if(E[n] == 1)  // contact
      target +=  beta2_lpdf(nasalance[n] | mu_contact, sigma_contact);
     else if(E[n] == 3)  // oral
       target += beta2_lpdf(nasalance[n] | mu_oral, sigma_oral);
    else if(E[n] == 2) { //nas
       real logodds_nas = logit(av_prob_nas) + u[speaker[n]];
       target += log_sum_exp(log_inv_logit(logodds_nas) +
                  beta2_lpdf(nasalance[n] | mu_contact, sigma_contact),
                  log1m_inv_logit(logodds_nas)+
                  beta2_lpdf(nasalance[n] | mu_oral, sigma_oral));
       }
  }
}
generated quantities {
  vector[N] log_lik;
  vector[N] nasalance_rep;
  vector[N_speaker] prob_nas_speaker = inv_logit(logit(av_prob_nas) + u);
  
  for (n in 1:N) {
    if (E[n] == 1) {  // contact
      log_lik[n] = beta2_lpdf(nasalance[n] | mu_contact, sigma_contact);
      nasalance_rep[n] = beta2_rng(mu_contact, sigma_contact);
    } else if (E[n] == 3) {  // oral
      log_lik[n] = beta2_lpdf(nasalance[n] | mu_oral, sigma_oral);
      nasalance_rep[n] = beta2_rng(mu_oral, sigma_oral);
    } else if (E[n] == 2) { // nas
      real logodds_nas = logit(av_prob_nas) + u[speaker[n]];
      real theta = inv_logit(logodds_nas);
      log_lik[n] = log_sum_exp(
        log(theta) + beta2_lpdf(nasalance[n] | mu_contact, sigma_contact),
        log1m(theta) + beta2_lpdf(nasalance[n] | mu_oral, sigma_oral)
      );
      if (bernoulli_rng(theta) == 1)
        nasalance_rep[n] = beta2_rng(mu_contact, sigma_contact);
      else
        nasalance_rep[n] = beta2_rng(mu_oral, sigma_oral);
    }
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mix_s_m &amp;lt;- cmdstan_model(&amp;quot;./mix_s.stan&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We first verify that we can recover the generating parameters. &lt;span class=&#34;citation&#34;&gt;(A complete pipeline would also include SBC, which we omit for brevity: &lt;a href=&#34;/case-study/mixture/mixture-nasal/#ref-talts2018validating&#34;&gt;Talts et al. 2018&lt;/a&gt;; &lt;a href=&#34;/case-study/mixture/mixture-nasal/#ref-ModrakEtAl2023&#34;&gt;Modrák et al. 2023&lt;/a&gt;)&lt;/span&gt;. We skip this step here.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nas_ls &amp;lt;- list(N = nrow(nas_data),
               nasalance = nas_data$nasalance,
               E = nas_data$E,
               speaker = nas_data$speaker,
               N_speaker = max(nas_data$speaker))
               
nas_mix &amp;lt;- mix_s_m$sample(data = nas_ls, parallel_chains = 4)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;yrep &amp;lt;- nas_mix$draws(&amp;quot;nasalance_rep&amp;quot;) |&amp;gt; posterior::as_draws_matrix()
ppc_dens_overlay_grouped(
  y = nas_data$nasalance,
  yrep = yrep[1:100, ],
  group = nas_data$etymology
) + ggtitle(&amp;quot;Real data&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/case-study/mixture/mixture-nasal//case-study/mixture/mixture-nasal_files/figure-html/unnamed-chunk-28-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r fold-hide&#34;&gt;&lt;code&gt;yrep_mat &amp;lt;- yrep[1:100, ]  # assuming yrep is already created
n_draws &amp;lt;- nrow(yrep_mat)

# Build long dataframe from yrep and attach metadata
yrep_df &amp;lt;- as.data.frame(t(yrep_mat)) |&amp;gt;
  setNames(paste0(&amp;quot;draw&amp;quot;, 1:n_draws)) |&amp;gt;
  mutate(
    obs = nas_data$nasalance,
    speaker = nas_data$speaker,
    etymology = nas_data$etymology
  ) |&amp;gt;
  pivot_longer(
    cols = starts_with(&amp;quot;draw&amp;quot;),
    names_to = &amp;quot;draw&amp;quot;,
    values_to = &amp;quot;yrep&amp;quot;
  )

# Plot
ggplot(yrep_df, aes(x = etymology, y = yrep, fill = etymology)) +
  geom_violin(alpha = 0.5, scale = &amp;quot;width&amp;quot;) +
  geom_point(aes(y = obs), shape = 21, size = 1.5, position = position_jitter(width = 0.1)) +
  facet_wrap(~ speaker, scales = &amp;quot;free_y&amp;quot;) +
  labs(title = &amp;quot;Posterior predictive violins per speaker and etymology&amp;quot;,
       x = &amp;quot;Etymology&amp;quot;, y = &amp;quot;Nasalance&amp;quot;) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/case-study/mixture/mixture-nasal//case-study/mixture/mixture-nasal_files/figure-html/unnamed-chunk-29-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Are there subjects who nasalize more?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mcmc_intervals(nas_mix$draws(&amp;quot;prob_nas_speaker&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/case-study/mixture/mixture-nasal//case-study/mixture/mixture-nasal_files/figure-html/unnamed-chunk-30-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;div class=&#34;note&#34;&gt;
&lt;p&gt;Add an analogous parameter &lt;code&gt;w&lt;/code&gt; which modulates the probability of nasalization by &lt;code&gt;root&lt;/code&gt;. Which roots are most likely to be nasalized?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;subject-level-adjustments-for-the-mixture-weight-and-mean-nasalization.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Subject level adjustments for the mixture weight and mean nasalization.&lt;/h3&gt;
&lt;p&gt;A more comprehensive hierarchical model allows &lt;code&gt;mu_contact&lt;/code&gt; and &lt;code&gt;mu_oral&lt;/code&gt;&lt;a href=&#34;/case-study/mixture/mixture-nasal/#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; to vary by speaker and/or by root.&lt;/p&gt;
&lt;p&gt;Stan code:&lt;/p&gt;
&lt;pre class=&#34;stan&#34;&gt;&lt;code&gt;#include beta2.stan
data {
  int&amp;lt;lower = 1&amp;gt; N;
  vector[N] nasalance;
  array[N] int&amp;lt;lower = 1&amp;gt; E;
  array[N] int&amp;lt;lower = 1&amp;gt; speaker;
  int&amp;lt;lower = 1&amp;gt; N_speaker;
}
parameters {
  real&amp;lt;lower = 0, upper = 1&amp;gt; mu_contact;
  real&amp;lt;lower = 0, upper = sqrt(mu_contact*(1-mu_contact))&amp;gt; sigma_contact;
  real&amp;lt;lower = 0, upper = 1 &amp;gt; mu_oral;
  real&amp;lt;lower= 0, upper = sqrt(mu_oral*(1-mu_oral))&amp;gt; sigma_oral;

  // now it&amp;#39;s log odds
  real&amp;lt;lower = 0, upper = 1&amp;gt; av_prob_nas;

 vector&amp;lt;lower = 0&amp;gt;[3]  tau_u;
  matrix[3, N_speaker] z_u;
  cholesky_factor_corr[3] L_u;
}
transformed parameters {
  matrix[N_speaker, 3] u;
  u = (diag_pre_multiply(tau_u, L_u) * z_u)&amp;#39;;
}
model {
  // priors for the task component
  target += normal_lpdf(mu_contact | 0.6, 0.2);
  target += normal_lpdf(mu_oral | 0.2, 0.3);
  target += normal_lpdf(sigma_oral | 0, 0.3);
  target += normal_lpdf(sigma_contact | 0, 0.3);

  target += lkj_corr_cholesky_lpdf(L_u | 2);
  target += std_normal_lpdf(to_vector(z_u));
  target += normal_lpdf(tau_u | 0, 2);
  
  target += beta_lpdf(av_prob_nas | 3, 10);

  for(n in 1:N){
      real mu_c_adj = inv_logit(logit(mu_contact) + u[speaker[n], 1]);
      real mu_o_adj = inv_logit(logit(mu_oral) + u[speaker[n], 3]);
      if(E[n] == 1)  // contact
      target +=  beta2_lpdf(nasalance[n] | mu_c_adj, sigma_contact);
     else if(E[n] == 3)  // oral
       target += beta2_lpdf(nasalance[n] | mu_o_adj, sigma_oral);
    else if(E[n] == 2) { //nas
       real logodds_nas = logit(av_prob_nas) + u[speaker[n], 2];
       target += log_sum_exp(log_inv_logit(logodds_nas) +
                  beta2_lpdf(nasalance[n] | mu_c_adj, sigma_contact),
                  log1m_inv_logit(logodds_nas)+
                  beta2_lpdf(nasalance[n] | mu_o_adj, sigma_oral));
       }
  }
}
generated quantities {
  vector[N] log_lik;
  vector[N] nasalance_rep;
  vector[N_speaker] prob_nas_speaker = inv_logit(logit(av_prob_nas) + u[,3]);
  
  for (n in 1:N) {
    real mu_c_adj = inv_logit(logit(mu_contact) + u[speaker[n], 1]);
     real mu_o_adj = inv_logit(logit(mu_oral) + u[speaker[n], 3]);
        if (E[n] == 1) {  // contact
      log_lik[n] = beta2_lpdf(nasalance[n] | mu_c_adj, sigma_contact);
      nasalance_rep[n] = beta2_rng(mu_c_adj, sigma_contact);
    } else if (E[n] == 3) {  // oral
      log_lik[n] = beta2_lpdf(nasalance[n] | mu_o_adj, sigma_oral);
      nasalance_rep[n] = beta2_rng(mu_o_adj, sigma_oral);
    } else if (E[n] == 2) { // nas
      real logodds_nas = logit(av_prob_nas) + u[speaker[n], 3];
      real theta = inv_logit(logodds_nas);
      log_lik[n] = log_sum_exp(
        log(theta) + beta2_lpdf(nasalance[n] | mu_c_adj, sigma_contact),
        log1m(theta) + beta2_lpdf(nasalance[n] | mu_o_adj, sigma_oral)
      );
      if (bernoulli_rng(theta) == 1)
        nasalance_rep[n] = beta2_rng(mu_c_adj, sigma_contact);
      else
        nasalance_rep[n] = beta2_rng(mu_o_adj, sigma_oral);
    }
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mix_s2_m &amp;lt;- cmdstan_model(&amp;quot;./mix_s2.stan&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nas2_mix &amp;lt;- mix_s2_m$sample(data = nas_ls, parallel_chains = 4)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nas2_mix&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;      variable   mean median   sd  mad     q5    q95 rhat ess_bulk ess_tail
 lp__          546.59 546.95 6.15 6.29 536.10 556.04 1.00      955     1777
 mu_contact      0.54   0.54 0.02 0.02   0.51   0.57 1.00     2771     2464
 sigma_contact   0.13   0.13 0.01 0.01   0.11   0.14 1.00     3288     2848
 mu_oral         0.16   0.16 0.01 0.01   0.15   0.18 1.00     1856     1983
 sigma_oral      0.08   0.08 0.00 0.00   0.08   0.09 1.00     5449     2789
 av_prob_nas     0.15   0.14 0.06 0.05   0.07   0.26 1.00     2133     2315
 tau_u[1]        0.17   0.16 0.09 0.08   0.04   0.32 1.00     1266     1301
 tau_u[2]        1.68   1.57 0.63 0.54   0.89   2.87 1.00     1718     2197
 tau_u[3]        0.19   0.18 0.06 0.05   0.12   0.30 1.00     1879     2438
 z_u[1,1]       -0.52  -0.54 0.87 0.83  -1.93   0.91 1.00     2513     3060

 # showing 10 of 1493 rows (change via &amp;#39;max_rows&amp;#39; argument or &amp;#39;cmdstanr_max_rows&amp;#39; option)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;yrep &amp;lt;- nas2_mix$draws(&amp;quot;nasalance_rep&amp;quot;) |&amp;gt; posterior::as_draws_matrix()
ppc_dens_overlay_grouped(
  y = nas_data$nasalance,
  yrep = yrep[1:100, ],
  group = nas_data$etymology
) + ggtitle(&amp;quot;Real data with the more complex model&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/case-study/mixture/mixture-nasal//case-study/mixture/mixture-nasal_files/figure-html/unnamed-chunk-34-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;div class=&#34;note&#34;&gt;
&lt;p&gt;Consider additional posterior predictive checks.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;notes&#34;&gt;
&lt;p&gt;Consider simulating data.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;note&#34;&gt;
&lt;p&gt;Is the hierarchical mixture model preferable to a hierarchical non-mixture alternative?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusions&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Conclusions&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Visual similarity is deceptive.&lt;/strong&gt; Simulated mixture and non-mixture data look almost identical.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Model comparison alone can be inconclusive&lt;/strong&gt; at the current sample size: both mixture and non-mixture models achieve very similar LOO‑CV scores on either data-generating process.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Speaker-level effects matter.&lt;/strong&gt; Allowing the mixture weight to vary by speaker reveals substantial individual differences.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Domain knowledge remains crucial.&lt;/strong&gt; The structure of the model should be informed by theory.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;session-info&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;Session info&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sessionInfo()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;R version 4.5.0 (2025-04-11)
Platform: x86_64-pc-linux-gnu
Running under: Ubuntu 22.04.5 LTS

Matrix products: default
BLAS:   /usr/lib/x86_64-linux-gnu/blas/libblas.so.3.10.0 
LAPACK: /usr/lib/x86_64-linux-gnu/lapack/liblapack.so.3.10.0  LAPACK version 3.10.0

locale:
 [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C              
 [3] LC_TIME=nl_NL.UTF-8        LC_COLLATE=en_US.UTF-8    
 [5] LC_MONETARY=nl_NL.UTF-8    LC_MESSAGES=en_US.UTF-8   
 [7] LC_PAPER=nl_NL.UTF-8       LC_NAME=C                 
 [9] LC_ADDRESS=C               LC_TELEPHONE=C            
[11] LC_MEASUREMENT=nl_NL.UTF-8 LC_IDENTIFICATION=C       

time zone: Europe/Amsterdam
tzcode source: system (glibc)

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base     

other attached packages:
[1] loo_2.8.0        ggplot2_3.5.2    brms_2.22.0      Rcpp_1.0.14     
[5] bayesplot_1.12.0 posterior_1.6.1  cmdstanr_0.9.0   tidytable_0.11.2

loaded via a namespace (and not attached):
 [1] bridgesampling_1.1-5 tensorA_0.36.2.1     utf8_1.2.6          
 [4] sass_0.4.10          generics_0.1.4       stringi_1.8.7       
 [7] lattice_0.22-5       blogdown_1.21        digest_0.6.37       
[10] magrittr_2.0.3       evaluate_1.0.3       grid_4.5.0          
[13] RColorBrewer_1.1-3   bookdown_0.43        mvtnorm_1.3-3       
[16] fastmap_1.2.0        plyr_1.8.9           Matrix_1.7-3        
[19] jsonlite_2.0.0       processx_3.8.6       backports_1.5.0     
[22] ps_1.9.1             Brobdingnag_1.2-9    scales_1.4.0        
[25] codetools_0.2-19     jquerylib_0.1.4      abind_1.4-8         
[28] cli_3.6.5            rlang_1.1.6          withr_3.0.2         
[31] cachem_1.1.0         yaml_2.3.10          tools_4.5.0         
[34] parallel_4.5.0       reshape2_1.4.4       rstantools_2.4.0    
[37] coda_0.19-4.1        checkmate_2.3.2      dplyr_1.1.4         
[40] vctrs_0.6.5          R6_2.6.1             matrixStats_1.5.0   
[43] lifecycle_1.0.4      stringr_1.5.1        pkgconfig_2.0.3     
[46] RcppParallel_5.1.10  pillar_1.10.2        bslib_0.9.0         
[49] gtable_0.3.6         data.table_1.17.0    glue_1.8.0          
[52] xfun_0.52            tibble_3.3.0         tidyselect_1.2.1    
[55] rstudioapi_0.17.1    knitr_1.50           farver_2.1.2        
[58] nlme_3.1-168         htmltools_0.5.8.1    labeling_0.4.3      
[61] rmarkdown_2.29       compiler_4.5.0       distributional_0.5.0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I used R &lt;span class=&#34;citation&#34;&gt;(Version 4.5.0; &lt;a href=&#34;/case-study/mixture/mixture-nasal/#ref-R-base&#34;&gt;R Core Team 2025&lt;/a&gt;)&lt;/span&gt; and the R-packages &lt;em&gt;bayesplot&lt;/em&gt; &lt;span class=&#34;citation&#34;&gt;(Version 1.12.0; &lt;a href=&#34;/case-study/mixture/mixture-nasal/#ref-R-bayesplot&#34;&gt;Gabry and Mahr 2025&lt;/a&gt;)&lt;/span&gt;, &lt;em&gt;brms&lt;/em&gt; &lt;span class=&#34;citation&#34;&gt;(Version 2.22.0; &lt;a href=&#34;/case-study/mixture/mixture-nasal/#ref-R-brms&#34;&gt;Bürkner 2024&lt;/a&gt;)&lt;/span&gt;, &lt;em&gt;cmdstanr&lt;/em&gt; &lt;span class=&#34;citation&#34;&gt;(Version 0.9.0; &lt;a href=&#34;/case-study/mixture/mixture-nasal/#ref-R-cmdstanr&#34;&gt;Gabry et al. 2025&lt;/a&gt;)&lt;/span&gt;, &lt;em&gt;ggplot2&lt;/em&gt; &lt;span class=&#34;citation&#34;&gt;(Version 3.5.2; &lt;a href=&#34;/case-study/mixture/mixture-nasal/#ref-R-ggplot2&#34;&gt;Wickham et al. 2025&lt;/a&gt;)&lt;/span&gt;, &lt;em&gt;loo&lt;/em&gt; &lt;span class=&#34;citation&#34;&gt;(Version 2.8.0; &lt;a href=&#34;/case-study/mixture/mixture-nasal/#ref-R-loo&#34;&gt;Vehtari et al. 2024&lt;/a&gt;)&lt;/span&gt;, &lt;em&gt;posterior&lt;/em&gt; &lt;span class=&#34;citation&#34;&gt;(Version 1.6.1; &lt;a href=&#34;/case-study/mixture/mixture-nasal/#ref-R-posterior&#34;&gt;Bürkner et al. 2025&lt;/a&gt;)&lt;/span&gt;, &lt;em&gt;Rcpp&lt;/em&gt; &lt;span class=&#34;citation&#34;&gt;(Version 1.0.14; &lt;a href=&#34;/case-study/mixture/mixture-nasal/#ref-R-Rcpp&#34;&gt;Eddelbuettel et al. 2025&lt;/a&gt;)&lt;/span&gt; and &lt;em&gt;tidytable&lt;/em&gt; &lt;span class=&#34;citation&#34;&gt;(Version 0.11.2; &lt;a href=&#34;/case-study/mixture/mixture-nasal/#ref-R-tidytable&#34;&gt;Fairbanks 2024&lt;/a&gt;)&lt;/span&gt; for all our analyses.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34; entry-spacing=&#34;0&#34;&gt;
&lt;div id=&#34;ref-R-brms&#34; class=&#34;csl-entry&#34;&gt;
Bürkner, Paul-Christian. 2024. &lt;em&gt;Brms: Bayesian Regression Models Using Stan&lt;/em&gt;. &lt;a href=&#34;https://github.com/paul-buerkner/brms&#34;&gt;https://github.com/paul-buerkner/brms&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-R-posterior&#34; class=&#34;csl-entry&#34;&gt;
Bürkner, Paul-Christian, Jonah Gabry, Matthew Kay, and Aki Vehtari. 2025. &lt;em&gt;Posterior: Tools for Working with Posterior Distributions&lt;/em&gt;. &lt;a href=&#34;https://mc-stan.org/posterior/&#34;&gt;https://mc-stan.org/posterior/&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-R-Rcpp&#34; class=&#34;csl-entry&#34;&gt;
Eddelbuettel, Dirk, Romain Francois, JJ Allaire, Kevin Ushey, Qiang Kou, Nathan Russell, Iñaki Ucar, Doug Bates, and John Chambers. 2025. &lt;em&gt;Rcpp: Seamless r and c++ Integration&lt;/em&gt;. &lt;a href=&#34;https://www.rcpp.org&#34;&gt;https://www.rcpp.org&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-R-tidytable&#34; class=&#34;csl-entry&#34;&gt;
Fairbanks, Mark. 2024. &lt;em&gt;Tidytable: Tidy Interface to Data.table&lt;/em&gt;. &lt;a href=&#34;https://markfairbanks.github.io/tidytable/&#34;&gt;https://markfairbanks.github.io/tidytable/&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-R-cmdstanr&#34; class=&#34;csl-entry&#34;&gt;
Gabry, Jonah, Rok Češnovar, Andrew Johnson, and Steve Bronder. 2025. &lt;em&gt;Cmdstanr: R Interface to CmdStan&lt;/em&gt;. &lt;a href=&#34;https://mc-stan.org/cmdstanr/&#34;&gt;https://mc-stan.org/cmdstanr/&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-R-bayesplot&#34; class=&#34;csl-entry&#34;&gt;
Gabry, Jonah, and Tristan Mahr. 2025. &lt;em&gt;Bayesplot: Plotting for Bayesian Models&lt;/em&gt;. &lt;a href=&#34;https://mc-stan.org/bayesplot/&#34;&gt;https://mc-stan.org/bayesplot/&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-ModrakEtAl2023&#34; class=&#34;csl-entry&#34;&gt;
Modrák, Martin, Angie H. Moon, Shinyoung Kim, Paul-Christian Bürkner, Niko Huurre, Kateřina Faltejsková, Andrew Gelman, and Aki Vehtari. 2023. &lt;span&gt;“Simulation-Based Calibration Checking for &lt;span&gt;Bayesian&lt;/span&gt; Computation: &lt;span&gt;The&lt;/span&gt; Choice of Test Quantities Shapes Sensitivity.”&lt;/span&gt; &lt;em&gt;Bayesian Analysis&lt;/em&gt;, 1–28. &lt;a href=&#34;https://doi.org/10.1214/23-BA1404&#34;&gt;https://doi.org/10.1214/23-BA1404&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-NicenboimEtAl2025&#34; class=&#34;csl-entry&#34;&gt;
Nicenboim, Bruno, Daniel J. Schad, and Shravan Vasishth. 2025. &lt;em&gt;Introduction to Bayesian Data Analysis for Cognitive Science&lt;/em&gt;. 1st ed. Chapman; Hall/CRC. &lt;a href=&#34;https://doi.org/10.1201/9780429342646&#34;&gt;https://doi.org/10.1201/9780429342646&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-R-base&#34; class=&#34;csl-entry&#34;&gt;
R Core Team. 2025. &lt;em&gt;R: A Language and Environment for Statistical Computing&lt;/em&gt;. Vienna, Austria: R Foundation for Statistical Computing. &lt;a href=&#34;https://www.R-project.org/&#34;&gt;https://www.R-project.org/&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-talts2018validating&#34; class=&#34;csl-entry&#34;&gt;
Talts, Sean, Michael J. Betancourt, Daniel P. Simpson, Aki Vehtari, and Andrew Gelman. 2018. &lt;span&gt;“Validating &lt;span&gt;B&lt;/span&gt;ayesian Inference Algorithms with Simulation-Based Calibration.”&lt;/span&gt; &lt;em&gt;arXiv Preprint arXiv:1804.06788&lt;/em&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-R-loo&#34; class=&#34;csl-entry&#34;&gt;
Vehtari, Aki, Jonah Gabry, Måns Magnusson, Yuling Yao, Paul-Christian Bürkner, Topi Paananen, and Andrew Gelman. 2024. &lt;em&gt;Loo: Efficient Leave-One-Out Cross-Validation and WAIC for Bayesian Models&lt;/em&gt;. &lt;a href=&#34;https://mc-stan.org/loo/&#34;&gt;https://mc-stan.org/loo/&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-R-ggplot2&#34; class=&#34;csl-entry&#34;&gt;
Wickham, Hadley, Winston Chang, Lionel Henry, Thomas Lin Pedersen, Kohske Takahashi, Claus Wilke, Kara Woo, Hiroaki Yutani, Dewey Dunnington, and Teun van den Brand. 2025. &lt;em&gt;Ggplot2: Create Elegant Data Visualisations Using the Grammar of Graphics&lt;/em&gt;. &lt;a href=&#34;https://ggplot2.tidyverse.org&#34;&gt;https://ggplot2.tidyverse.org&lt;/a&gt;.
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes footnotes-end-of-document&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;with considerable extra work we could also deal with &lt;code&gt;sigma&lt;/code&gt;&lt;a href=&#34;/case-study/mixture/mixture-nasal/#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>/case-study/mispecification/test_files/libs/revealjs/plugin/notes/speaker-view/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/case-study/mispecification/test_files/libs/revealjs/plugin/notes/speaker-view/</guid><description>
&lt;html lang=&#34;en&#34;&gt;
	&lt;head&gt;
		&lt;meta charset=&#34;utf-8&#34;&gt;

		&lt;title&gt;reveal.js - Speaker View&lt;/title&gt;

		&lt;style&gt;
			body {
				font-family: Helvetica;
				font-size: 18px;
			}

			#current-slide,
			#upcoming-slide,
			#speaker-controls {
				padding: 6px;
				box-sizing: border-box;
				-moz-box-sizing: border-box;
			}

			#current-slide iframe,
			#upcoming-slide iframe {
				width: 100%;
				height: 100%;
				border: 1px solid #ddd;
			}

			#current-slide .label,
			#upcoming-slide .label {
				position: absolute;
				top: 10px;
				left: 10px;
				z-index: 2;
			}

			#connection-status {
				position: absolute;
				top: 0;
				left: 0;
				width: 100%;
				height: 100%;
				z-index: 20;
				padding: 30% 20% 20% 20%;
				font-size: 18px;
				color: #222;
				background: #fff;
				text-align: center;
				box-sizing: border-box;
				line-height: 1.4;
			}

			.overlay-element {
				height: 34px;
				line-height: 34px;
				padding: 0 10px;
				text-shadow: none;
				background: rgba( 220, 220, 220, 0.8 );
				color: #222;
				font-size: 14px;
			}

			.overlay-element.interactive:hover {
				background: rgba( 220, 220, 220, 1 );
			}

			#current-slide {
				position: absolute;
				width: 60%;
				height: 100%;
				top: 0;
				left: 0;
				padding-right: 0;
			}

			#upcoming-slide {
				position: absolute;
				width: 40%;
				height: 40%;
				right: 0;
				top: 0;
			}

			/* Speaker controls */
			#speaker-controls {
				position: absolute;
				top: 40%;
				right: 0;
				width: 40%;
				height: 60%;
				overflow: auto;
				font-size: 18px;
			}

				.speaker-controls-time.hidden,
				.speaker-controls-notes.hidden {
					display: none;
				}

				.speaker-controls-time .label,
				.speaker-controls-pace .label,
				.speaker-controls-notes .label {
					text-transform: uppercase;
					font-weight: normal;
					font-size: 0.66em;
					color: #666;
					margin: 0;
				}

				.speaker-controls-time, .speaker-controls-pace {
					border-bottom: 1px solid rgba( 200, 200, 200, 0.5 );
					margin-bottom: 10px;
					padding: 10px 16px;
					padding-bottom: 20px;
					cursor: pointer;
				}

				.speaker-controls-time .reset-button {
					opacity: 0;
					float: right;
					color: #666;
					text-decoration: none;
				}
				.speaker-controls-time:hover .reset-button {
					opacity: 1;
				}

				.speaker-controls-time .timer,
				.speaker-controls-time .clock {
					width: 50%;
				}

				.speaker-controls-time .timer,
				.speaker-controls-time .clock,
				.speaker-controls-time .pacing .hours-value,
				.speaker-controls-time .pacing .minutes-value,
				.speaker-controls-time .pacing .seconds-value {
					font-size: 1.9em;
				}

				.speaker-controls-time .timer {
					float: left;
				}

				.speaker-controls-time .clock {
					float: right;
					text-align: right;
				}

				.speaker-controls-time span.mute {
					opacity: 0.3;
				}

				.speaker-controls-time .pacing-title {
					margin-top: 5px;
				}

				.speaker-controls-time .pacing.ahead {
					color: blue;
				}

				.speaker-controls-time .pacing.on-track {
					color: green;
				}

				.speaker-controls-time .pacing.behind {
					color: red;
				}

				.speaker-controls-notes {
					padding: 10px 16px;
				}

				.speaker-controls-notes .value {
					margin-top: 5px;
					line-height: 1.4;
					font-size: 1.2em;
				}

			/* Layout selector */
			#speaker-layout {
				position: absolute;
				top: 10px;
				right: 10px;
				color: #222;
				z-index: 10;
			}
				#speaker-layout select {
					position: absolute;
					width: 100%;
					height: 100%;
					top: 0;
					left: 0;
					border: 0;
					box-shadow: 0;
					cursor: pointer;
					opacity: 0;

					font-size: 1em;
					background-color: transparent;

					-moz-appearance: none;
					-webkit-appearance: none;
					-webkit-tap-highlight-color: rgba(0, 0, 0, 0);
				}

				#speaker-layout select:focus {
					outline: none;
					box-shadow: none;
				}

			.clear {
				clear: both;
			}

			/* Speaker layout: Wide */
			body[data-speaker-layout=&#34;wide&#34;] #current-slide,
			body[data-speaker-layout=&#34;wide&#34;] #upcoming-slide {
				width: 50%;
				height: 45%;
				padding: 6px;
			}

			body[data-speaker-layout=&#34;wide&#34;] #current-slide {
				top: 0;
				left: 0;
			}

			body[data-speaker-layout=&#34;wide&#34;] #upcoming-slide {
				top: 0;
				left: 50%;
			}

			body[data-speaker-layout=&#34;wide&#34;] #speaker-controls {
				top: 45%;
				left: 0;
				width: 100%;
				height: 50%;
				font-size: 1.25em;
			}

			/* Speaker layout: Tall */
			body[data-speaker-layout=&#34;tall&#34;] #current-slide,
			body[data-speaker-layout=&#34;tall&#34;] #upcoming-slide {
				width: 45%;
				height: 50%;
				padding: 6px;
			}

			body[data-speaker-layout=&#34;tall&#34;] #current-slide {
				top: 0;
				left: 0;
			}

			body[data-speaker-layout=&#34;tall&#34;] #upcoming-slide {
				top: 50%;
				left: 0;
			}

			body[data-speaker-layout=&#34;tall&#34;] #speaker-controls {
				padding-top: 40px;
				top: 0;
				left: 45%;
				width: 55%;
				height: 100%;
				font-size: 1.25em;
			}

			/* Speaker layout: Notes only */
			body[data-speaker-layout=&#34;notes-only&#34;] #current-slide,
			body[data-speaker-layout=&#34;notes-only&#34;] #upcoming-slide {
				display: none;
			}

			body[data-speaker-layout=&#34;notes-only&#34;] #speaker-controls {
				padding-top: 40px;
				top: 0;
				left: 0;
				width: 100%;
				height: 100%;
				font-size: 1.25em;
			}

			@media screen and (max-width: 1080px) {
				body[data-speaker-layout=&#34;default&#34;] #speaker-controls {
					font-size: 16px;
				}
			}

			@media screen and (max-width: 900px) {
				body[data-speaker-layout=&#34;default&#34;] #speaker-controls {
					font-size: 14px;
				}
			}

			@media screen and (max-width: 800px) {
				body[data-speaker-layout=&#34;default&#34;] #speaker-controls {
					font-size: 12px;
				}
			}

		&lt;/style&gt;
	&lt;/head&gt;

	&lt;body&gt;

		&lt;div id=&#34;connection-status&#34;&gt;Loading speaker view...&lt;/div&gt;

		&lt;div id=&#34;current-slide&#34;&gt;&lt;/div&gt;
		&lt;div id=&#34;upcoming-slide&#34;&gt;&lt;span class=&#34;overlay-element label&#34;&gt;Upcoming&lt;/span&gt;&lt;/div&gt;
		&lt;div id=&#34;speaker-controls&#34;&gt;
			&lt;div class=&#34;speaker-controls-time&#34;&gt;
				&lt;h4 class=&#34;label&#34;&gt;Time &lt;span class=&#34;reset-button&#34;&gt;Click to Reset&lt;/span&gt;&lt;/h4&gt;
				&lt;div class=&#34;clock&#34;&gt;
					&lt;span class=&#34;clock-value&#34;&gt;0:00 AM&lt;/span&gt;
				&lt;/div&gt;
				&lt;div class=&#34;timer&#34;&gt;
					&lt;span class=&#34;hours-value&#34;&gt;00&lt;/span&gt;&lt;span class=&#34;minutes-value&#34;&gt;:00&lt;/span&gt;&lt;span class=&#34;seconds-value&#34;&gt;:00&lt;/span&gt;
				&lt;/div&gt;
				&lt;div class=&#34;clear&#34;&gt;&lt;/div&gt;

				&lt;h4 class=&#34;label pacing-title&#34; style=&#34;display: none&#34;&gt;Pacing – Time to finish current slide&lt;/h4&gt;
				&lt;div class=&#34;pacing&#34; style=&#34;display: none&#34;&gt;
					&lt;span class=&#34;hours-value&#34;&gt;00&lt;/span&gt;&lt;span class=&#34;minutes-value&#34;&gt;:00&lt;/span&gt;&lt;span class=&#34;seconds-value&#34;&gt;:00&lt;/span&gt;
				&lt;/div&gt;
			&lt;/div&gt;

			&lt;div class=&#34;speaker-controls-notes hidden&#34;&gt;
				&lt;h4 class=&#34;label&#34;&gt;Notes&lt;/h4&gt;
				&lt;div class=&#34;value&#34;&gt;&lt;/div&gt;
			&lt;/div&gt;
		&lt;/div&gt;
		&lt;div id=&#34;speaker-layout&#34; class=&#34;overlay-element interactive&#34;&gt;
			&lt;span class=&#34;speaker-layout-label&#34;&gt;&lt;/span&gt;
			&lt;select class=&#34;speaker-layout-dropdown&#34;&gt;&lt;/select&gt;
		&lt;/div&gt;

		&lt;script&gt;

			(function() {

				var notes,
					notesValue,
					currentState,
					currentSlide,
					upcomingSlide,
					layoutLabel,
					layoutDropdown,
					pendingCalls = {},
					lastRevealApiCallId = 0,
					connected = false

				var connectionStatus = document.querySelector( &#39;#connection-status&#39; );

				var SPEAKER_LAYOUTS = {
					&#39;default&#39;: &#39;Default&#39;,
					&#39;wide&#39;: &#39;Wide&#39;,
					&#39;tall&#39;: &#39;Tall&#39;,
					&#39;notes-only&#39;: &#39;Notes only&#39;
				};

				setupLayout();

				let openerOrigin;

				try {
					openerOrigin = window.opener.location.origin;
				}
				catch ( error ) { console.warn( error ) }

				// In order to prevent XSS, the speaker view will only run if its
				// opener has the same origin as itself
				if( window.location.origin !== openerOrigin ) {
					connectionStatus.innerHTML = &#39;Cross origin error.&lt;br&gt;The speaker window can only be opened from the same origin.&#39;;
					return;
				}

				var connectionTimeout = setTimeout( function() {
					connectionStatus.innerHTML = &#39;Error connecting to main window.&lt;br&gt;Please try closing and reopening the speaker view.&#39;;
				}, 5000 );

				window.addEventListener( &#39;message&#39;, function( event ) {

					// Validate the origin of all messages to avoid parsing messages
					// that aren&#39;t meant for us. Ignore when running off file:// so
					// that the speaker view continues to work without a web server.
					if( window.location.origin !== event.origin &amp;&amp; window.location.origin !== &#39;file://&#39; ) {
						return
					}

					clearTimeout( connectionTimeout );
					connectionStatus.style.display = &#39;none&#39;;

					var data = JSON.parse( event.data );

					// The overview mode is only useful to the reveal.js instance
					// where navigation occurs so we don&#39;t sync it
					if( data.state ) delete data.state.overview;

					// Messages sent by the notes plugin inside of the main window
					if( data &amp;&amp; data.namespace === &#39;reveal-notes&#39; ) {
						if( data.type === &#39;connect&#39; ) {
							handleConnectMessage( data );
						}
						else if( data.type === &#39;state&#39; ) {
							handleStateMessage( data );
						}
						else if( data.type === &#39;return&#39; ) {
							pendingCalls[data.callId](data.result);
							delete pendingCalls[data.callId];
						}
					}
					// Messages sent by the reveal.js inside of the current slide preview
					else if( data &amp;&amp; data.namespace === &#39;reveal&#39; ) {
						if( /ready/.test( data.eventName ) ) {
							// Send a message back to notify that the handshake is complete
							window.opener.postMessage( JSON.stringify({ namespace: &#39;reveal-notes&#39;, type: &#39;connected&#39;} ), &#39;*&#39; );
						}
						else if( /slidechanged|fragmentshown|fragmenthidden|paused|resumed/.test( data.eventName ) &amp;&amp; currentState !== JSON.stringify( data.state ) ) {

							dispatchStateToMainWindow( data.state );

						}
					}

				} );

				/**
				 * Updates the presentation in the main window to match the state
				 * of the presentation in the notes window.
				 */
				const dispatchStateToMainWindow = debounce(( state ) =&gt; {
					window.opener.postMessage( JSON.stringify({ method: &#39;setState&#39;, args: [ state ]} ), &#39;*&#39; );
				}, 500);

				/**
				 * Asynchronously calls the Reveal.js API of the main frame.
				 */
				function callRevealApi( methodName, methodArguments, callback ) {

					var callId = ++lastRevealApiCallId;
					pendingCalls[callId] = callback;
					window.opener.postMessage( JSON.stringify( {
						namespace: &#39;reveal-notes&#39;,
						type: &#39;call&#39;,
						callId: callId,
						methodName: methodName,
						arguments: methodArguments
					} ), &#39;*&#39; );

				}

				/**
				 * Called when the main window is trying to establish a
				 * connection.
				 */
				function handleConnectMessage( data ) {

					if( connected === false ) {
						connected = true;

						setupIframes( data );
						setupKeyboard();
						setupNotes();
						setupTimer();
						setupHeartbeat();
					}

				}

				/**
				 * Called when the main window sends an updated state.
				 */
				function handleStateMessage( data ) {

					// Store the most recently set state to avoid circular loops
					// applying the same state
					currentState = JSON.stringify( data.state );

					// No need for updating the notes in case of fragment changes
					if ( data.notes ) {
						notes.classList.remove( &#39;hidden&#39; );
						notesValue.style.whiteSpace = data.whitespace;
						if( data.markdown ) {
							notesValue.innerHTML = marked( data.notes );
						}
						else {
							notesValue.innerHTML = data.notes;
						}
					}
					else {
						notes.classList.add( &#39;hidden&#39; );
					}

					// Update the note slides
					currentSlide.contentWindow.postMessage( JSON.stringify({ method: &#39;setState&#39;, args: [ data.state ] }), &#39;*&#39; );
					upcomingSlide.contentWindow.postMessage( JSON.stringify({ method: &#39;setState&#39;, args: [ data.state ] }), &#39;*&#39; );
					upcomingSlide.contentWindow.postMessage( JSON.stringify({ method: &#39;next&#39; }), &#39;*&#39; );

				}

				// Limit to max one state update per X ms
				handleStateMessage = debounce( handleStateMessage, 200 );

				/**
				 * Forward keyboard events to the current slide window.
				 * This enables keyboard events to work even if focus
				 * isn&#39;t set on the current slide iframe.
				 *
				 * Block F5 default handling, it reloads and disconnects
				 * the speaker notes window.
				 */
				function setupKeyboard() {

					document.addEventListener( &#39;keydown&#39;, function( event ) {
						if( event.keyCode === 116 || ( event.metaKey &amp;&amp; event.keyCode === 82 ) ) {
							event.preventDefault();
							return false;
						}
						currentSlide.contentWindow.postMessage( JSON.stringify({ method: &#39;triggerKey&#39;, args: [ event.keyCode ] }), &#39;*&#39; );
					} );

				}

				/**
				 * Creates the preview iframes.
				 */
				function setupIframes( data ) {

					var params = [
						&#39;receiver&#39;,
						&#39;progress=false&#39;,
						&#39;history=false&#39;,
						&#39;transition=none&#39;,
						&#39;autoSlide=0&#39;,
						&#39;backgroundTransition=none&#39;
					].join( &#39;&amp;&#39; );

					var urlSeparator = /\?/.test(data.url) ? &#39;&amp;&#39; : &#39;?&#39;;
					var hash = &#39;#/&#39; + data.state.indexh + &#39;/&#39; + data.state.indexv;
					var currentURL = data.url + urlSeparator + params + &#39;&amp;scrollActivationWidth=false&amp;postMessageEvents=true&#39; + hash;
					var upcomingURL = data.url + urlSeparator + params + &#39;&amp;scrollActivationWidth=false&amp;controls=false&#39; + hash;

					currentSlide = document.createElement( &#39;iframe&#39; );
					currentSlide.setAttribute( &#39;width&#39;, 1280 );
					currentSlide.setAttribute( &#39;height&#39;, 1024 );
					currentSlide.setAttribute( &#39;src&#39;, currentURL );
					document.querySelector( &#39;#current-slide&#39; ).appendChild( currentSlide );

					upcomingSlide = document.createElement( &#39;iframe&#39; );
					upcomingSlide.setAttribute( &#39;width&#39;, 640 );
					upcomingSlide.setAttribute( &#39;height&#39;, 512 );
					upcomingSlide.setAttribute( &#39;src&#39;, upcomingURL );
					document.querySelector( &#39;#upcoming-slide&#39; ).appendChild( upcomingSlide );

				}

				/**
				 * Setup the notes UI.
				 */
				function setupNotes() {

					notes = document.querySelector( &#39;.speaker-controls-notes&#39; );
					notesValue = document.querySelector( &#39;.speaker-controls-notes .value&#39; );

				}

				/**
				 * We send out a heartbeat at all times to ensure we can
				 * reconnect with the main presentation window after reloads.
				 */
				function setupHeartbeat() {

					setInterval( () =&gt; {
						window.opener.postMessage( JSON.stringify({ namespace: &#39;reveal-notes&#39;, type: &#39;heartbeat&#39;} ), &#39;*&#39; );
					}, 1000 );

				}

				function getTimings( callback ) {

					callRevealApi( &#39;getSlidesAttributes&#39;, [], function ( slideAttributes ) {
						callRevealApi( &#39;getConfig&#39;, [], function ( config ) {
							var totalTime = config.totalTime;
							var minTimePerSlide = config.minimumTimePerSlide || 0;
							var defaultTiming = config.defaultTiming;
							if ((defaultTiming == null) &amp;&amp; (totalTime == null)) {
								callback(null);
								return;
							}
							// Setting totalTime overrides defaultTiming
							if (totalTime) {
								defaultTiming = 0;
							}
							var timings = [];
							for ( var i in slideAttributes ) {
								var slide = slideAttributes[ i ];
								var timing = defaultTiming;
								if( slide.hasOwnProperty( &#39;data-timing&#39; )) {
									var t = slide[ &#39;data-timing&#39; ];
									timing = parseInt(t);
									if( isNaN(timing) ) {
										console.warn(&#34;Could not parse timing &#39;&#34; + t + &#34;&#39; of slide &#34; + i + &#34;; using default of &#34; + defaultTiming);
										timing = defaultTiming;
									}
								}
								timings.push(timing);
							}
							if ( totalTime ) {
								// After we&#39;ve allocated time to individual slides, we summarize it and
								// subtract it from the total time
								var remainingTime = totalTime - timings.reduce( function(a, b) { return a + b; }, 0 );
								// The remaining time is divided by the number of slides that have 0 seconds
								// allocated at the moment, giving the average time-per-slide on the remaining slides
								var remainingSlides = (timings.filter( function(x) { return x == 0 }) ).length
								var timePerSlide = Math.round( remainingTime / remainingSlides, 0 )
								// And now we replace every zero-value timing with that average
								timings = timings.map( function(x) { return (x==0 ? timePerSlide : x) } );
							}
							var slidesUnderMinimum = timings.filter( function(x) { return (x &lt; minTimePerSlide) } ).length
							if ( slidesUnderMinimum ) {
								message = &#34;The pacing time for &#34; + slidesUnderMinimum + &#34; slide(s) is under the configured minimum of &#34; + minTimePerSlide + &#34; seconds. Check the data-timing attribute on individual slides, or consider increasing the totalTime or minimumTimePerSlide configuration options (or removing some slides).&#34;;
								alert(message);
							}
							callback( timings );
						} );
					} );

				}

				/**
				 * Return the number of seconds allocated for presenting
				 * all slides up to and including this one.
				 */
				function getTimeAllocated( timings, callback ) {

					callRevealApi( &#39;getSlidePastCount&#39;, [], function ( currentSlide ) {
						var allocated = 0;
						for (var i in timings.slice(0, currentSlide + 1)) {
							allocated += timings[i];
						}
						callback( allocated );
					} );

				}

				/**
				 * Create the timer and clock and start updating them
				 * at an interval.
				 */
				function setupTimer() {

					var start = new Date(),
					timeEl = document.querySelector( &#39;.speaker-controls-time&#39; ),
					clockEl = timeEl.querySelector( &#39;.clock-value&#39; ),
					hoursEl = timeEl.querySelector( &#39;.hours-value&#39; ),
					minutesEl = timeEl.querySelector( &#39;.minutes-value&#39; ),
					secondsEl = timeEl.querySelector( &#39;.seconds-value&#39; ),
					pacingTitleEl = timeEl.querySelector( &#39;.pacing-title&#39; ),
					pacingEl = timeEl.querySelector( &#39;.pacing&#39; ),
					pacingHoursEl = pacingEl.querySelector( &#39;.hours-value&#39; ),
					pacingMinutesEl = pacingEl.querySelector( &#39;.minutes-value&#39; ),
					pacingSecondsEl = pacingEl.querySelector( &#39;.seconds-value&#39; );

					var timings = null;
					getTimings( function ( _timings ) {

						timings = _timings;
						if (_timings !== null) {
							pacingTitleEl.style.removeProperty(&#39;display&#39;);
							pacingEl.style.removeProperty(&#39;display&#39;);
						}

						// Update once directly
						_updateTimer();

						// Then update every second
						setInterval( _updateTimer, 1000 );

					} );


					function _resetTimer() {

						if (timings == null) {
							start = new Date();
							_updateTimer();
						}
						else {
							// Reset timer to beginning of current slide
							getTimeAllocated( timings, function ( slideEndTimingSeconds ) {
								var slideEndTiming = slideEndTimingSeconds * 1000;
								callRevealApi( &#39;getSlidePastCount&#39;, [], function ( currentSlide ) {
									var currentSlideTiming = timings[currentSlide] * 1000;
									var previousSlidesTiming = slideEndTiming - currentSlideTiming;
									var now = new Date();
									start = new Date(now.getTime() - previousSlidesTiming);
									_updateTimer();
								} );
							} );
						}

					}

					timeEl.addEventListener( &#39;click&#39;, function() {
						_resetTimer();
						return false;
					} );

					function _displayTime( hrEl, minEl, secEl, time) {

						var sign = Math.sign(time) == -1 ? &#34;-&#34; : &#34;&#34;;
						time = Math.abs(Math.round(time / 1000));
						var seconds = time % 60;
						var minutes = Math.floor( time / 60 ) % 60 ;
						var hours = Math.floor( time / ( 60 * 60 )) ;
						hrEl.innerHTML = sign + zeroPadInteger( hours );
						if (hours == 0) {
							hrEl.classList.add( &#39;mute&#39; );
						}
						else {
							hrEl.classList.remove( &#39;mute&#39; );
						}
						minEl.innerHTML = &#39;:&#39; + zeroPadInteger( minutes );
						if (hours == 0 &amp;&amp; minutes == 0) {
							minEl.classList.add( &#39;mute&#39; );
						}
						else {
							minEl.classList.remove( &#39;mute&#39; );
						}
						secEl.innerHTML = &#39;:&#39; + zeroPadInteger( seconds );
					}

					function _updateTimer() {

						var diff, hours, minutes, seconds,
						now = new Date();

						diff = now.getTime() - start.getTime();

						clockEl.innerHTML = now.toLocaleTimeString( &#39;en-US&#39;, { hour12: true, hour: &#39;2-digit&#39;, minute:&#39;2-digit&#39; } );
						_displayTime( hoursEl, minutesEl, secondsEl, diff );
						if (timings !== null) {
							_updatePacing(diff);
						}

					}

					function _updatePacing(diff) {

						getTimeAllocated( timings, function ( slideEndTimingSeconds ) {
							var slideEndTiming = slideEndTimingSeconds * 1000;

							callRevealApi( &#39;getSlidePastCount&#39;, [], function ( currentSlide ) {
								var currentSlideTiming = timings[currentSlide] * 1000;
								var timeLeftCurrentSlide = slideEndTiming - diff;
								if (timeLeftCurrentSlide &lt; 0) {
									pacingEl.className = &#39;pacing behind&#39;;
								}
								else if (timeLeftCurrentSlide &lt; currentSlideTiming) {
									pacingEl.className = &#39;pacing on-track&#39;;
								}
								else {
									pacingEl.className = &#39;pacing ahead&#39;;
								}
								_displayTime( pacingHoursEl, pacingMinutesEl, pacingSecondsEl, timeLeftCurrentSlide );
							} );
						} );
					}

				}

				/**
				 * Sets up the speaker view layout and layout selector.
				 */
				function setupLayout() {

					layoutDropdown = document.querySelector( &#39;.speaker-layout-dropdown&#39; );
					layoutLabel = document.querySelector( &#39;.speaker-layout-label&#39; );

					// Render the list of available layouts
					for( var id in SPEAKER_LAYOUTS ) {
						var option = document.createElement( &#39;option&#39; );
						option.setAttribute( &#39;value&#39;, id );
						option.textContent = SPEAKER_LAYOUTS[ id ];
						layoutDropdown.appendChild( option );
					}

					// Monitor the dropdown for changes
					layoutDropdown.addEventListener( &#39;change&#39;, function( event ) {

						setLayout( layoutDropdown.value );

					}, false );

					// Restore any currently persisted layout
					setLayout( getLayout() );

				}

				/**
				 * Sets a new speaker view layout. The layout is persisted
				 * in local storage.
				 */
				function setLayout( value ) {

					var title = SPEAKER_LAYOUTS[ value ];

					layoutLabel.innerHTML = &#39;Layout&#39; + ( title ? ( &#39;: &#39; + title ) : &#39;&#39; );
					layoutDropdown.value = value;

					document.body.setAttribute( &#39;data-speaker-layout&#39;, value );

					// Persist locally
					if( supportsLocalStorage() ) {
						window.localStorage.setItem( &#39;reveal-speaker-layout&#39;, value );
					}

				}

				/**
				 * Returns the ID of the most recently set speaker layout
				 * or our default layout if none has been set.
				 */
				function getLayout() {

					if( supportsLocalStorage() ) {
						var layout = window.localStorage.getItem( &#39;reveal-speaker-layout&#39; );
						if( layout ) {
							return layout;
						}
					}

					// Default to the first record in the layouts hash
					for( var id in SPEAKER_LAYOUTS ) {
						return id;
					}

				}

				function supportsLocalStorage() {

					try {
						localStorage.setItem(&#39;test&#39;, &#39;test&#39;);
						localStorage.removeItem(&#39;test&#39;);
						return true;
					}
					catch( e ) {
						return false;
					}

				}

				function zeroPadInteger( num ) {

					var str = &#39;00&#39; + parseInt( num );
					return str.substring( str.length - 2 );

				}

				/**
				 * Limits the frequency at which a function can be called.
				 */
				function debounce( fn, ms ) {

					var lastTime = 0,
						timeout;

					return function() {

						var args = arguments;
						var context = this;

						clearTimeout( timeout );

						var timeSinceLastCall = Date.now() - lastTime;
						if( timeSinceLastCall &gt; ms ) {
							fn.apply( context, args );
							lastTime = Date.now();
						}
						else {
							timeout = setTimeout( function() {
								fn.apply( context, args );
								lastTime = Date.now();
							}, ms - timeSinceLastCall );
						}

					}

				}

			})();

		&lt;/script&gt;
	&lt;/body&gt;
&lt;/html&gt;</description>
    </item>
    
    <item>
      <title></title>
      <link>/case-study/mispecification/wrongmodels_files/libs/revealjs/plugin/notes/speaker-view/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/case-study/mispecification/wrongmodels_files/libs/revealjs/plugin/notes/speaker-view/</guid><description>
&lt;html lang=&#34;en&#34;&gt;
	&lt;head&gt;
		&lt;meta charset=&#34;utf-8&#34;&gt;

		&lt;title&gt;reveal.js - Speaker View&lt;/title&gt;

		&lt;style&gt;
			body {
				font-family: Helvetica;
				font-size: 18px;
			}

			#current-slide,
			#upcoming-slide,
			#speaker-controls {
				padding: 6px;
				box-sizing: border-box;
				-moz-box-sizing: border-box;
			}

			#current-slide iframe,
			#upcoming-slide iframe {
				width: 100%;
				height: 100%;
				border: 1px solid #ddd;
			}

			#current-slide .label,
			#upcoming-slide .label {
				position: absolute;
				top: 10px;
				left: 10px;
				z-index: 2;
			}

			#connection-status {
				position: absolute;
				top: 0;
				left: 0;
				width: 100%;
				height: 100%;
				z-index: 20;
				padding: 30% 20% 20% 20%;
				font-size: 18px;
				color: #222;
				background: #fff;
				text-align: center;
				box-sizing: border-box;
				line-height: 1.4;
			}

			.overlay-element {
				height: 34px;
				line-height: 34px;
				padding: 0 10px;
				text-shadow: none;
				background: rgba( 220, 220, 220, 0.8 );
				color: #222;
				font-size: 14px;
			}

			.overlay-element.interactive:hover {
				background: rgba( 220, 220, 220, 1 );
			}

			#current-slide {
				position: absolute;
				width: 60%;
				height: 100%;
				top: 0;
				left: 0;
				padding-right: 0;
			}

			#upcoming-slide {
				position: absolute;
				width: 40%;
				height: 40%;
				right: 0;
				top: 0;
			}

			/* Speaker controls */
			#speaker-controls {
				position: absolute;
				top: 40%;
				right: 0;
				width: 40%;
				height: 60%;
				overflow: auto;
				font-size: 18px;
			}

				.speaker-controls-time.hidden,
				.speaker-controls-notes.hidden {
					display: none;
				}

				.speaker-controls-time .label,
				.speaker-controls-pace .label,
				.speaker-controls-notes .label {
					text-transform: uppercase;
					font-weight: normal;
					font-size: 0.66em;
					color: #666;
					margin: 0;
				}

				.speaker-controls-time, .speaker-controls-pace {
					border-bottom: 1px solid rgba( 200, 200, 200, 0.5 );
					margin-bottom: 10px;
					padding: 10px 16px;
					padding-bottom: 20px;
					cursor: pointer;
				}

				.speaker-controls-time .reset-button {
					opacity: 0;
					float: right;
					color: #666;
					text-decoration: none;
				}
				.speaker-controls-time:hover .reset-button {
					opacity: 1;
				}

				.speaker-controls-time .timer,
				.speaker-controls-time .clock {
					width: 50%;
				}

				.speaker-controls-time .timer,
				.speaker-controls-time .clock,
				.speaker-controls-time .pacing .hours-value,
				.speaker-controls-time .pacing .minutes-value,
				.speaker-controls-time .pacing .seconds-value {
					font-size: 1.9em;
				}

				.speaker-controls-time .timer {
					float: left;
				}

				.speaker-controls-time .clock {
					float: right;
					text-align: right;
				}

				.speaker-controls-time span.mute {
					opacity: 0.3;
				}

				.speaker-controls-time .pacing-title {
					margin-top: 5px;
				}

				.speaker-controls-time .pacing.ahead {
					color: blue;
				}

				.speaker-controls-time .pacing.on-track {
					color: green;
				}

				.speaker-controls-time .pacing.behind {
					color: red;
				}

				.speaker-controls-notes {
					padding: 10px 16px;
				}

				.speaker-controls-notes .value {
					margin-top: 5px;
					line-height: 1.4;
					font-size: 1.2em;
				}

			/* Layout selector */
			#speaker-layout {
				position: absolute;
				top: 10px;
				right: 10px;
				color: #222;
				z-index: 10;
			}
				#speaker-layout select {
					position: absolute;
					width: 100%;
					height: 100%;
					top: 0;
					left: 0;
					border: 0;
					box-shadow: 0;
					cursor: pointer;
					opacity: 0;

					font-size: 1em;
					background-color: transparent;

					-moz-appearance: none;
					-webkit-appearance: none;
					-webkit-tap-highlight-color: rgba(0, 0, 0, 0);
				}

				#speaker-layout select:focus {
					outline: none;
					box-shadow: none;
				}

			.clear {
				clear: both;
			}

			/* Speaker layout: Wide */
			body[data-speaker-layout=&#34;wide&#34;] #current-slide,
			body[data-speaker-layout=&#34;wide&#34;] #upcoming-slide {
				width: 50%;
				height: 45%;
				padding: 6px;
			}

			body[data-speaker-layout=&#34;wide&#34;] #current-slide {
				top: 0;
				left: 0;
			}

			body[data-speaker-layout=&#34;wide&#34;] #upcoming-slide {
				top: 0;
				left: 50%;
			}

			body[data-speaker-layout=&#34;wide&#34;] #speaker-controls {
				top: 45%;
				left: 0;
				width: 100%;
				height: 50%;
				font-size: 1.25em;
			}

			/* Speaker layout: Tall */
			body[data-speaker-layout=&#34;tall&#34;] #current-slide,
			body[data-speaker-layout=&#34;tall&#34;] #upcoming-slide {
				width: 45%;
				height: 50%;
				padding: 6px;
			}

			body[data-speaker-layout=&#34;tall&#34;] #current-slide {
				top: 0;
				left: 0;
			}

			body[data-speaker-layout=&#34;tall&#34;] #upcoming-slide {
				top: 50%;
				left: 0;
			}

			body[data-speaker-layout=&#34;tall&#34;] #speaker-controls {
				padding-top: 40px;
				top: 0;
				left: 45%;
				width: 55%;
				height: 100%;
				font-size: 1.25em;
			}

			/* Speaker layout: Notes only */
			body[data-speaker-layout=&#34;notes-only&#34;] #current-slide,
			body[data-speaker-layout=&#34;notes-only&#34;] #upcoming-slide {
				display: none;
			}

			body[data-speaker-layout=&#34;notes-only&#34;] #speaker-controls {
				padding-top: 40px;
				top: 0;
				left: 0;
				width: 100%;
				height: 100%;
				font-size: 1.25em;
			}

			@media screen and (max-width: 1080px) {
				body[data-speaker-layout=&#34;default&#34;] #speaker-controls {
					font-size: 16px;
				}
			}

			@media screen and (max-width: 900px) {
				body[data-speaker-layout=&#34;default&#34;] #speaker-controls {
					font-size: 14px;
				}
			}

			@media screen and (max-width: 800px) {
				body[data-speaker-layout=&#34;default&#34;] #speaker-controls {
					font-size: 12px;
				}
			}

		&lt;/style&gt;
	&lt;/head&gt;

	&lt;body&gt;

		&lt;div id=&#34;connection-status&#34;&gt;Loading speaker view...&lt;/div&gt;

		&lt;div id=&#34;current-slide&#34;&gt;&lt;/div&gt;
		&lt;div id=&#34;upcoming-slide&#34;&gt;&lt;span class=&#34;overlay-element label&#34;&gt;Upcoming&lt;/span&gt;&lt;/div&gt;
		&lt;div id=&#34;speaker-controls&#34;&gt;
			&lt;div class=&#34;speaker-controls-time&#34;&gt;
				&lt;h4 class=&#34;label&#34;&gt;Time &lt;span class=&#34;reset-button&#34;&gt;Click to Reset&lt;/span&gt;&lt;/h4&gt;
				&lt;div class=&#34;clock&#34;&gt;
					&lt;span class=&#34;clock-value&#34;&gt;0:00 AM&lt;/span&gt;
				&lt;/div&gt;
				&lt;div class=&#34;timer&#34;&gt;
					&lt;span class=&#34;hours-value&#34;&gt;00&lt;/span&gt;&lt;span class=&#34;minutes-value&#34;&gt;:00&lt;/span&gt;&lt;span class=&#34;seconds-value&#34;&gt;:00&lt;/span&gt;
				&lt;/div&gt;
				&lt;div class=&#34;clear&#34;&gt;&lt;/div&gt;

				&lt;h4 class=&#34;label pacing-title&#34; style=&#34;display: none&#34;&gt;Pacing – Time to finish current slide&lt;/h4&gt;
				&lt;div class=&#34;pacing&#34; style=&#34;display: none&#34;&gt;
					&lt;span class=&#34;hours-value&#34;&gt;00&lt;/span&gt;&lt;span class=&#34;minutes-value&#34;&gt;:00&lt;/span&gt;&lt;span class=&#34;seconds-value&#34;&gt;:00&lt;/span&gt;
				&lt;/div&gt;
			&lt;/div&gt;

			&lt;div class=&#34;speaker-controls-notes hidden&#34;&gt;
				&lt;h4 class=&#34;label&#34;&gt;Notes&lt;/h4&gt;
				&lt;div class=&#34;value&#34;&gt;&lt;/div&gt;
			&lt;/div&gt;
		&lt;/div&gt;
		&lt;div id=&#34;speaker-layout&#34; class=&#34;overlay-element interactive&#34;&gt;
			&lt;span class=&#34;speaker-layout-label&#34;&gt;&lt;/span&gt;
			&lt;select class=&#34;speaker-layout-dropdown&#34;&gt;&lt;/select&gt;
		&lt;/div&gt;

		&lt;script&gt;

			(function() {

				var notes,
					notesValue,
					currentState,
					currentSlide,
					upcomingSlide,
					layoutLabel,
					layoutDropdown,
					pendingCalls = {},
					lastRevealApiCallId = 0,
					connected = false

				var connectionStatus = document.querySelector( &#39;#connection-status&#39; );

				var SPEAKER_LAYOUTS = {
					&#39;default&#39;: &#39;Default&#39;,
					&#39;wide&#39;: &#39;Wide&#39;,
					&#39;tall&#39;: &#39;Tall&#39;,
					&#39;notes-only&#39;: &#39;Notes only&#39;
				};

				setupLayout();

				let openerOrigin;

				try {
					openerOrigin = window.opener.location.origin;
				}
				catch ( error ) { console.warn( error ) }

				// In order to prevent XSS, the speaker view will only run if its
				// opener has the same origin as itself
				if( window.location.origin !== openerOrigin ) {
					connectionStatus.innerHTML = &#39;Cross origin error.&lt;br&gt;The speaker window can only be opened from the same origin.&#39;;
					return;
				}

				var connectionTimeout = setTimeout( function() {
					connectionStatus.innerHTML = &#39;Error connecting to main window.&lt;br&gt;Please try closing and reopening the speaker view.&#39;;
				}, 5000 );

				window.addEventListener( &#39;message&#39;, function( event ) {

					// Validate the origin of all messages to avoid parsing messages
					// that aren&#39;t meant for us. Ignore when running off file:// so
					// that the speaker view continues to work without a web server.
					if( window.location.origin !== event.origin &amp;&amp; window.location.origin !== &#39;file://&#39; ) {
						return
					}

					clearTimeout( connectionTimeout );
					connectionStatus.style.display = &#39;none&#39;;

					var data = JSON.parse( event.data );

					// The overview mode is only useful to the reveal.js instance
					// where navigation occurs so we don&#39;t sync it
					if( data.state ) delete data.state.overview;

					// Messages sent by the notes plugin inside of the main window
					if( data &amp;&amp; data.namespace === &#39;reveal-notes&#39; ) {
						if( data.type === &#39;connect&#39; ) {
							handleConnectMessage( data );
						}
						else if( data.type === &#39;state&#39; ) {
							handleStateMessage( data );
						}
						else if( data.type === &#39;return&#39; ) {
							pendingCalls[data.callId](data.result);
							delete pendingCalls[data.callId];
						}
					}
					// Messages sent by the reveal.js inside of the current slide preview
					else if( data &amp;&amp; data.namespace === &#39;reveal&#39; ) {
						if( /ready/.test( data.eventName ) ) {
							// Send a message back to notify that the handshake is complete
							window.opener.postMessage( JSON.stringify({ namespace: &#39;reveal-notes&#39;, type: &#39;connected&#39;} ), &#39;*&#39; );
						}
						else if( /slidechanged|fragmentshown|fragmenthidden|paused|resumed/.test( data.eventName ) &amp;&amp; currentState !== JSON.stringify( data.state ) ) {

							dispatchStateToMainWindow( data.state );

						}
					}

				} );

				/**
				 * Updates the presentation in the main window to match the state
				 * of the presentation in the notes window.
				 */
				const dispatchStateToMainWindow = debounce(( state ) =&gt; {
					window.opener.postMessage( JSON.stringify({ method: &#39;setState&#39;, args: [ state ]} ), &#39;*&#39; );
				}, 500);

				/**
				 * Asynchronously calls the Reveal.js API of the main frame.
				 */
				function callRevealApi( methodName, methodArguments, callback ) {

					var callId = ++lastRevealApiCallId;
					pendingCalls[callId] = callback;
					window.opener.postMessage( JSON.stringify( {
						namespace: &#39;reveal-notes&#39;,
						type: &#39;call&#39;,
						callId: callId,
						methodName: methodName,
						arguments: methodArguments
					} ), &#39;*&#39; );

				}

				/**
				 * Called when the main window is trying to establish a
				 * connection.
				 */
				function handleConnectMessage( data ) {

					if( connected === false ) {
						connected = true;

						setupIframes( data );
						setupKeyboard();
						setupNotes();
						setupTimer();
						setupHeartbeat();
					}

				}

				/**
				 * Called when the main window sends an updated state.
				 */
				function handleStateMessage( data ) {

					// Store the most recently set state to avoid circular loops
					// applying the same state
					currentState = JSON.stringify( data.state );

					// No need for updating the notes in case of fragment changes
					if ( data.notes ) {
						notes.classList.remove( &#39;hidden&#39; );
						notesValue.style.whiteSpace = data.whitespace;
						if( data.markdown ) {
							notesValue.innerHTML = marked( data.notes );
						}
						else {
							notesValue.innerHTML = data.notes;
						}
					}
					else {
						notes.classList.add( &#39;hidden&#39; );
					}

					// Update the note slides
					currentSlide.contentWindow.postMessage( JSON.stringify({ method: &#39;setState&#39;, args: [ data.state ] }), &#39;*&#39; );
					upcomingSlide.contentWindow.postMessage( JSON.stringify({ method: &#39;setState&#39;, args: [ data.state ] }), &#39;*&#39; );
					upcomingSlide.contentWindow.postMessage( JSON.stringify({ method: &#39;next&#39; }), &#39;*&#39; );

				}

				// Limit to max one state update per X ms
				handleStateMessage = debounce( handleStateMessage, 200 );

				/**
				 * Forward keyboard events to the current slide window.
				 * This enables keyboard events to work even if focus
				 * isn&#39;t set on the current slide iframe.
				 *
				 * Block F5 default handling, it reloads and disconnects
				 * the speaker notes window.
				 */
				function setupKeyboard() {

					document.addEventListener( &#39;keydown&#39;, function( event ) {
						if( event.keyCode === 116 || ( event.metaKey &amp;&amp; event.keyCode === 82 ) ) {
							event.preventDefault();
							return false;
						}
						currentSlide.contentWindow.postMessage( JSON.stringify({ method: &#39;triggerKey&#39;, args: [ event.keyCode ] }), &#39;*&#39; );
					} );

				}

				/**
				 * Creates the preview iframes.
				 */
				function setupIframes( data ) {

					var params = [
						&#39;receiver&#39;,
						&#39;progress=false&#39;,
						&#39;history=false&#39;,
						&#39;transition=none&#39;,
						&#39;autoSlide=0&#39;,
						&#39;backgroundTransition=none&#39;
					].join( &#39;&amp;&#39; );

					var urlSeparator = /\?/.test(data.url) ? &#39;&amp;&#39; : &#39;?&#39;;
					var hash = &#39;#/&#39; + data.state.indexh + &#39;/&#39; + data.state.indexv;
					var currentURL = data.url + urlSeparator + params + &#39;&amp;scrollActivationWidth=false&amp;postMessageEvents=true&#39; + hash;
					var upcomingURL = data.url + urlSeparator + params + &#39;&amp;scrollActivationWidth=false&amp;controls=false&#39; + hash;

					currentSlide = document.createElement( &#39;iframe&#39; );
					currentSlide.setAttribute( &#39;width&#39;, 1280 );
					currentSlide.setAttribute( &#39;height&#39;, 1024 );
					currentSlide.setAttribute( &#39;src&#39;, currentURL );
					document.querySelector( &#39;#current-slide&#39; ).appendChild( currentSlide );

					upcomingSlide = document.createElement( &#39;iframe&#39; );
					upcomingSlide.setAttribute( &#39;width&#39;, 640 );
					upcomingSlide.setAttribute( &#39;height&#39;, 512 );
					upcomingSlide.setAttribute( &#39;src&#39;, upcomingURL );
					document.querySelector( &#39;#upcoming-slide&#39; ).appendChild( upcomingSlide );

				}

				/**
				 * Setup the notes UI.
				 */
				function setupNotes() {

					notes = document.querySelector( &#39;.speaker-controls-notes&#39; );
					notesValue = document.querySelector( &#39;.speaker-controls-notes .value&#39; );

				}

				/**
				 * We send out a heartbeat at all times to ensure we can
				 * reconnect with the main presentation window after reloads.
				 */
				function setupHeartbeat() {

					setInterval( () =&gt; {
						window.opener.postMessage( JSON.stringify({ namespace: &#39;reveal-notes&#39;, type: &#39;heartbeat&#39;} ), &#39;*&#39; );
					}, 1000 );

				}

				function getTimings( callback ) {

					callRevealApi( &#39;getSlidesAttributes&#39;, [], function ( slideAttributes ) {
						callRevealApi( &#39;getConfig&#39;, [], function ( config ) {
							var totalTime = config.totalTime;
							var minTimePerSlide = config.minimumTimePerSlide || 0;
							var defaultTiming = config.defaultTiming;
							if ((defaultTiming == null) &amp;&amp; (totalTime == null)) {
								callback(null);
								return;
							}
							// Setting totalTime overrides defaultTiming
							if (totalTime) {
								defaultTiming = 0;
							}
							var timings = [];
							for ( var i in slideAttributes ) {
								var slide = slideAttributes[ i ];
								var timing = defaultTiming;
								if( slide.hasOwnProperty( &#39;data-timing&#39; )) {
									var t = slide[ &#39;data-timing&#39; ];
									timing = parseInt(t);
									if( isNaN(timing) ) {
										console.warn(&#34;Could not parse timing &#39;&#34; + t + &#34;&#39; of slide &#34; + i + &#34;; using default of &#34; + defaultTiming);
										timing = defaultTiming;
									}
								}
								timings.push(timing);
							}
							if ( totalTime ) {
								// After we&#39;ve allocated time to individual slides, we summarize it and
								// subtract it from the total time
								var remainingTime = totalTime - timings.reduce( function(a, b) { return a + b; }, 0 );
								// The remaining time is divided by the number of slides that have 0 seconds
								// allocated at the moment, giving the average time-per-slide on the remaining slides
								var remainingSlides = (timings.filter( function(x) { return x == 0 }) ).length
								var timePerSlide = Math.round( remainingTime / remainingSlides, 0 )
								// And now we replace every zero-value timing with that average
								timings = timings.map( function(x) { return (x==0 ? timePerSlide : x) } );
							}
							var slidesUnderMinimum = timings.filter( function(x) { return (x &lt; minTimePerSlide) } ).length
							if ( slidesUnderMinimum ) {
								message = &#34;The pacing time for &#34; + slidesUnderMinimum + &#34; slide(s) is under the configured minimum of &#34; + minTimePerSlide + &#34; seconds. Check the data-timing attribute on individual slides, or consider increasing the totalTime or minimumTimePerSlide configuration options (or removing some slides).&#34;;
								alert(message);
							}
							callback( timings );
						} );
					} );

				}

				/**
				 * Return the number of seconds allocated for presenting
				 * all slides up to and including this one.
				 */
				function getTimeAllocated( timings, callback ) {

					callRevealApi( &#39;getSlidePastCount&#39;, [], function ( currentSlide ) {
						var allocated = 0;
						for (var i in timings.slice(0, currentSlide + 1)) {
							allocated += timings[i];
						}
						callback( allocated );
					} );

				}

				/**
				 * Create the timer and clock and start updating them
				 * at an interval.
				 */
				function setupTimer() {

					var start = new Date(),
					timeEl = document.querySelector( &#39;.speaker-controls-time&#39; ),
					clockEl = timeEl.querySelector( &#39;.clock-value&#39; ),
					hoursEl = timeEl.querySelector( &#39;.hours-value&#39; ),
					minutesEl = timeEl.querySelector( &#39;.minutes-value&#39; ),
					secondsEl = timeEl.querySelector( &#39;.seconds-value&#39; ),
					pacingTitleEl = timeEl.querySelector( &#39;.pacing-title&#39; ),
					pacingEl = timeEl.querySelector( &#39;.pacing&#39; ),
					pacingHoursEl = pacingEl.querySelector( &#39;.hours-value&#39; ),
					pacingMinutesEl = pacingEl.querySelector( &#39;.minutes-value&#39; ),
					pacingSecondsEl = pacingEl.querySelector( &#39;.seconds-value&#39; );

					var timings = null;
					getTimings( function ( _timings ) {

						timings = _timings;
						if (_timings !== null) {
							pacingTitleEl.style.removeProperty(&#39;display&#39;);
							pacingEl.style.removeProperty(&#39;display&#39;);
						}

						// Update once directly
						_updateTimer();

						// Then update every second
						setInterval( _updateTimer, 1000 );

					} );


					function _resetTimer() {

						if (timings == null) {
							start = new Date();
							_updateTimer();
						}
						else {
							// Reset timer to beginning of current slide
							getTimeAllocated( timings, function ( slideEndTimingSeconds ) {
								var slideEndTiming = slideEndTimingSeconds * 1000;
								callRevealApi( &#39;getSlidePastCount&#39;, [], function ( currentSlide ) {
									var currentSlideTiming = timings[currentSlide] * 1000;
									var previousSlidesTiming = slideEndTiming - currentSlideTiming;
									var now = new Date();
									start = new Date(now.getTime() - previousSlidesTiming);
									_updateTimer();
								} );
							} );
						}

					}

					timeEl.addEventListener( &#39;click&#39;, function() {
						_resetTimer();
						return false;
					} );

					function _displayTime( hrEl, minEl, secEl, time) {

						var sign = Math.sign(time) == -1 ? &#34;-&#34; : &#34;&#34;;
						time = Math.abs(Math.round(time / 1000));
						var seconds = time % 60;
						var minutes = Math.floor( time / 60 ) % 60 ;
						var hours = Math.floor( time / ( 60 * 60 )) ;
						hrEl.innerHTML = sign + zeroPadInteger( hours );
						if (hours == 0) {
							hrEl.classList.add( &#39;mute&#39; );
						}
						else {
							hrEl.classList.remove( &#39;mute&#39; );
						}
						minEl.innerHTML = &#39;:&#39; + zeroPadInteger( minutes );
						if (hours == 0 &amp;&amp; minutes == 0) {
							minEl.classList.add( &#39;mute&#39; );
						}
						else {
							minEl.classList.remove( &#39;mute&#39; );
						}
						secEl.innerHTML = &#39;:&#39; + zeroPadInteger( seconds );
					}

					function _updateTimer() {

						var diff, hours, minutes, seconds,
						now = new Date();

						diff = now.getTime() - start.getTime();

						clockEl.innerHTML = now.toLocaleTimeString( &#39;en-US&#39;, { hour12: true, hour: &#39;2-digit&#39;, minute:&#39;2-digit&#39; } );
						_displayTime( hoursEl, minutesEl, secondsEl, diff );
						if (timings !== null) {
							_updatePacing(diff);
						}

					}

					function _updatePacing(diff) {

						getTimeAllocated( timings, function ( slideEndTimingSeconds ) {
							var slideEndTiming = slideEndTimingSeconds * 1000;

							callRevealApi( &#39;getSlidePastCount&#39;, [], function ( currentSlide ) {
								var currentSlideTiming = timings[currentSlide] * 1000;
								var timeLeftCurrentSlide = slideEndTiming - diff;
								if (timeLeftCurrentSlide &lt; 0) {
									pacingEl.className = &#39;pacing behind&#39;;
								}
								else if (timeLeftCurrentSlide &lt; currentSlideTiming) {
									pacingEl.className = &#39;pacing on-track&#39;;
								}
								else {
									pacingEl.className = &#39;pacing ahead&#39;;
								}
								_displayTime( pacingHoursEl, pacingMinutesEl, pacingSecondsEl, timeLeftCurrentSlide );
							} );
						} );
					}

				}

				/**
				 * Sets up the speaker view layout and layout selector.
				 */
				function setupLayout() {

					layoutDropdown = document.querySelector( &#39;.speaker-layout-dropdown&#39; );
					layoutLabel = document.querySelector( &#39;.speaker-layout-label&#39; );

					// Render the list of available layouts
					for( var id in SPEAKER_LAYOUTS ) {
						var option = document.createElement( &#39;option&#39; );
						option.setAttribute( &#39;value&#39;, id );
						option.textContent = SPEAKER_LAYOUTS[ id ];
						layoutDropdown.appendChild( option );
					}

					// Monitor the dropdown for changes
					layoutDropdown.addEventListener( &#39;change&#39;, function( event ) {

						setLayout( layoutDropdown.value );

					}, false );

					// Restore any currently persisted layout
					setLayout( getLayout() );

				}

				/**
				 * Sets a new speaker view layout. The layout is persisted
				 * in local storage.
				 */
				function setLayout( value ) {

					var title = SPEAKER_LAYOUTS[ value ];

					layoutLabel.innerHTML = &#39;Layout&#39; + ( title ? ( &#39;: &#39; + title ) : &#39;&#39; );
					layoutDropdown.value = value;

					document.body.setAttribute( &#39;data-speaker-layout&#39;, value );

					// Persist locally
					if( supportsLocalStorage() ) {
						window.localStorage.setItem( &#39;reveal-speaker-layout&#39;, value );
					}

				}

				/**
				 * Returns the ID of the most recently set speaker layout
				 * or our default layout if none has been set.
				 */
				function getLayout() {

					if( supportsLocalStorage() ) {
						var layout = window.localStorage.getItem( &#39;reveal-speaker-layout&#39; );
						if( layout ) {
							return layout;
						}
					}

					// Default to the first record in the layouts hash
					for( var id in SPEAKER_LAYOUTS ) {
						return id;
					}

				}

				function supportsLocalStorage() {

					try {
						localStorage.setItem(&#39;test&#39;, &#39;test&#39;);
						localStorage.removeItem(&#39;test&#39;);
						return true;
					}
					catch( e ) {
						return false;
					}

				}

				function zeroPadInteger( num ) {

					var str = &#39;00&#39; + parseInt( num );
					return str.substring( str.length - 2 );

				}

				/**
				 * Limits the frequency at which a function can be called.
				 */
				function debounce( fn, ms ) {

					var lastTime = 0,
						timeout;

					return function() {

						var args = arguments;
						var context = this;

						clearTimeout( timeout );

						var timeSinceLastCall = Date.now() - lastTime;
						if( timeSinceLastCall &gt; ms ) {
							fn.apply( context, args );
							lastTime = Date.now();
						}
						else {
							timeout = setTimeout( function() {
								fn.apply( context, args );
								lastTime = Date.now();
							}, ms - timeSinceLastCall );
						}

					}

				}

			})();

		&lt;/script&gt;
	&lt;/body&gt;
&lt;/html&gt;</description>
    </item>
    
  </channel>
</rss>
