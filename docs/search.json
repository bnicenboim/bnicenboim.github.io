[
  {
    "objectID": "software.html",
    "href": "software.html",
    "title": "Software",
    "section": "",
    "text": "An R package for tidy-ish manipulation of EEG data. It is fully functional, but it’s only able to do basic preprocessing.\n\n\n\nAn R package for estimating the log-probabilities of words in a given context using transformer models. The package provides an interface for utilizing pre-trained transformer models (such as GPT-2 or BERT) to obtain word probabilities. These log-probabilities are often utilized as predictors in psycholinguistic studies."
  },
  {
    "objectID": "software.html#eeguana",
    "href": "software.html#eeguana",
    "title": "Software",
    "section": "",
    "text": "An R package for tidy-ish manipulation of EEG data. It is fully functional, but it’s only able to do basic preprocessing."
  },
  {
    "objectID": "software.html#pangoling",
    "href": "software.html#pangoling",
    "title": "Software",
    "section": "",
    "text": "An R package for estimating the log-probabilities of words in a given context using transformer models. The package provides an interface for utilizing pre-trained transformer models (such as GPT-2 or BERT) to obtain word probabilities. These log-probabilities are often utilized as predictors in psycholinguistic studies."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bruno Nicenboim",
    "section": "",
    "text": "Email:\n[initial].[lastname] [at] tilburguniversity.edu\n\n\nOffice:\nD108, Dante Building, Tilburg University\n\n\nAddress:\nDepartment of Computational Cognitive Science,\n\n\n\nCognitive Science and Artificial Intelligence\n\n\n\nTilburg University\n\n\n\nPO Box 90153\n\n\n\n5000 LE Tilburg\n\n\n\nThe Netherlands"
  },
  {
    "objectID": "index.html#computational-cognitive-modeling",
    "href": "index.html#computational-cognitive-modeling",
    "title": "Bruno Nicenboim",
    "section": "Computational Cognitive Modeling",
    "text": "Computational Cognitive Modeling\nI study computational cognitive modeling of psycholinguistic phenomena, with some examples below:\n\nNicenboim, B. (2023). “The CoFI Reader: A Continuous Flow of Information approach to modeling reading.” In: MathPsych/ICCM/EMPG. University of Amsterdam, the Netherlands. [read]\nNicenboim, B. and S. Vasishth (2018). “Models of Retrieval in Sentence Comprehension: A computational evaluation using Bayesian hierarchical modeling.” In: Journal of Memory and Language, 99, pp. 1–34. ISSN: 0749-596X. DOI: 10.1016/j.jml.2017.08.004. [read]\n\nI am also interested in broader aspects of computational modeling:\n\nDubova, M., S. Chandramouli, G. Gigerenzer, P. Grünwald, W. Holmes, T. Lombrozo, M. Marelli, S. Musslick, B. Nicenboim, L. N. Ross, et al. (2025). “Is Ockham’s razor losing its edge? New perspectives on the principle of model parsimony.” In: Proceedings of the National Academy of Sciences, 122(5), p. e2401230121. DOI: 10.1073/pnas.2401230121. [read]\nI was one of the organizers of the first Computational Psycholinguistics Meeting (2025): a new recurring meeting dedicated exclusively to computational psycholinguistics.\nI organized the Lorentz Centre “Cognitive Modeling of Complex Behavior” workshop in January 2024, along with Riccardo Fusaroli and Marieke van Vugt. This hands-on event focused on collaborative modeling of cognitive phenomena. See here."
  },
  {
    "objectID": "index.html#eeg-in-psycholinguistics",
    "href": "index.html#eeg-in-psycholinguistics",
    "title": "Bruno Nicenboim",
    "section": "EEG in Psycholinguistics",
    "text": "EEG in Psycholinguistics\nI also work with EEG in psycholinguistics:\n\nK. Stone, B. Nicenboim, S. Vasishth, et al. “Understanding the effects of constraint and predictability in ERP.” In: Neurobiology of Language (Dec. 2022), pp. 1-71. DOI: 10.1162/nol_a_00094. [read]\nB. Nicenboim, S. Vasishth, and F. Rösler. “Are words pre-activated probabilistically during sentence comprehension? Evidence from new data and a Bayesian random-effects meta-analysis using publicly available data.” In: Neuropsychologia, 142 (2020), p. 107427. DOI: 10.1016/j.neuropsychologia.2020.107427. [read]\n\nI also developed an R package for EEG data manipulation: eeguana."
  },
  {
    "objectID": "index.html#bayesian-statistics",
    "href": "index.html#bayesian-statistics",
    "title": "Bruno Nicenboim",
    "section": "Bayesian Statistics",
    "text": "Bayesian Statistics\nBayesian statistics provides a powerful framework for cognitive science by allowing principled uncertainty quantification and hierarchical modeling. I mostly work with Stan (and brms):\n\nIntroduction to Bayesian Data Analysis for Cognitive Science , co-authored with Shravan Vasishth and Daniel Schad. You can [buy it] or [read it for free]."
  },
  {
    "objectID": "index.html#data-and-code",
    "href": "index.html#data-and-code",
    "title": "Bruno Nicenboim",
    "section": "Data and Code",
    "text": "Data and Code\nMost of the data and code from my published papers are available on the OSF website, with some exceptions in my GitHub repository."
  },
  {
    "objectID": "LICENSE.html",
    "href": "LICENSE.html",
    "title": "License",
    "section": "",
    "text": "MIT License\nCopyright (c) 2026 Bruno Nicenboim\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
  },
  {
    "objectID": "posts/posts/2026-01-06-a-simple-way-to-model-rankings-with-stan/index.html",
    "href": "posts/posts/2026-01-06-a-simple-way-to-model-rankings-with-stan/index.html",
    "title": "A simple way to model rankings with Stan",
    "section": "",
    "text": "Update Notice\n\n\n\nThis is an updated version of a post originally published on March 21, 2021.\nI’ve updated this post in January 2026 to work with current versions of Stan and R packages. The main changes include:\n\nUpdated Stan syntax to current standards\nUsing cmdstanr instead of rstan and some other R packages.\n\nThe core content and ideas remain the same."
  },
  {
    "objectID": "posts/posts/2026-01-06-a-simple-way-to-model-rankings-with-stan/index.html#the-initial-problem",
    "href": "posts/posts/2026-01-06-a-simple-way-to-model-rankings-with-stan/index.html#the-initial-problem",
    "title": "A simple way to model rankings with Stan",
    "section": "The initial problem",
    "text": "The initial problem\nI wrote what I thought was the generative process for some modeling work, and it looked too common to not have a name, so I started asking around on what it was a very popular website back in 2021, Twitter.\nOne useful clue was about the exploded logit distribution.1\n\nIn this post, I’ll show how this model can be fit in the probabilistic programming language Stan, and how it can be used to describe the underlying order of ranking data.\nI’m going to load some R packages that will be useful throughout this post.\n\nlibrary(tidytable) # Nicer alternative to dplyr and purrr\nlibrary(ggplot2) # Nice plots\nlibrary(extraDistr) # More distributions\nlibrary(rcorpora) # Get random words\nlibrary(cmdstanr) # Lightweight Stan interface\nlibrary(bayesplot) # Nice Bayesian plots\nset.seed(42)  # Keep everything R the same (not for Stan though)"
  },
  {
    "objectID": "posts/posts/2026-01-06-a-simple-way-to-model-rankings-with-stan/index.html#ranking-data",
    "href": "posts/posts/2026-01-06-a-simple-way-to-model-rankings-with-stan/index.html#ranking-data",
    "title": "A simple way to model rankings with Stan",
    "section": "Ranking data",
    "text": "Ranking data\nRanking data appear when we care about the underlying order that certain elements have. We might want to know which are the best horses after looking at several races (Gakis et al. 2018), which is the best candidate for a job after a series of interviewers talked to several candidates. More in line with cognitive science, we might want to know which are the best possible completions for a sentence or the best exemplars of a category.\nOne way to get a ranking of exemplars of a category, for example, is to present them to participants and ask them to order all (or a subset) of them (see Barsalou 1985).\n\nA ranking simulation using pizza toppings\n\n\ntoppings &lt;- corpora(\"foods/pizzaToppings\")$pizzaToppings\nN_toppings &lt;- length(toppings)\ntoppings\n\n [1] \"anchovies\"        \"artichoke\"       \n [3] \"bacon\"            \"breakfast bacon\" \n [5] \"Canadian bacon\"   \"cheese\"          \n [7] \"chicken\"          \"chili peppers\"   \n [9] \"feta\"             \"garlic\"          \n[11] \"green peppers\"    \"grilled onions\"  \n[13] \"ground beef\"      \"ham\"             \n[15] \"hot sauce\"        \"meatballs\"       \n[17] \"mushrooms\"        \"olives\"          \n[19] \"onions\"           \"pepperoni\"       \n[21] \"pineapple\"        \"sausage\"         \n[23] \"spinach\"          \"sun-dried tomato\"\n[25] \"tomatoes\"        \n\n\nLet’s say that we want to know the underlying order of pizza toppings. For the modeling, I’m going to assume that the toppings are ordered according to an underlying value, which also represents how likely it is for each topping to be the exemplar of their category.\nTo get a known ground truth for the ranking, I’m going to simulate an order of pizza toppings. I assign probabilities that sum up to one to the toppings by drawing a random sample from a Dirichlet distribution. The Dirichlet distribution is the generalization of the Beta distribution. It has a concentration parameter, usually \\(\\boldsymbol{\\alpha}\\), which is a vector as long as the probabilities we are sampling (25 here). When the vector is full of ones, the distribution is uniform: All probabilities are equally likely, so on average each one is \\(\\frac{1}{\\text{vector length}}\\) (\\(\\frac{1}{25}\\) here). By setting all the concentration parameters below one (namely \\(0.2\\)), I’m enforcing sparsity in the random values that I’m generating, that is, many probability values close to zero.\nThese is the true order that I’m assuming here:\n\n# all elements of the vector are 0.2\nalpha &lt;- rep(.2, N_toppings)\n# Generate one draw from a Dirichlet distribution\nP_toppings &lt;- c(rdirichlet(1, alpha)) %&gt;%\n  # Add names\n  setNames(toppings) %&gt;%\n  # Sort from the best exemplar\n  sort(decreasing = TRUE)\nP_toppings %&gt;%\n  round(3)\n\n breakfast bacon          chicken             feta \n           0.294            0.241            0.087 \n       anchovies sun-dried tomato           olives \n           0.087            0.077            0.057 \n       pepperoni        artichoke           cheese \n           0.056            0.049            0.010 \n  Canadian bacon            bacon              ham \n           0.008            0.008            0.006 \n       meatballs    chili peppers           garlic \n           0.004            0.004            0.004 \n     ground beef         tomatoes        hot sauce \n           0.003            0.003            0.002 \n          onions          sausage        pineapple \n           0.000            0.000            0.000 \n         spinach        mushrooms   grilled onions \n           0.000            0.000            0.000 \n   green peppers \n           0.000 \n\n\n\nGiven these values, if I were to ask a participant “What’s the most appropriate topping for a pizza?” I would assume that 29.37 percent of the time, I would get breakfast bacon.\n\nEssentially, we expect something like this to be happening:\n\\[\n\\text{response} \\sim \\text{Categorical}(\\Theta_{\\text{toppings}})\n\\]\nWith \\(\\Theta_{\\text{toppings}}\\) representing the different probabilities for each topping. The probability mass function of the categorical distribution is absurdly simple: It’s just the probability of the outcome.\n\\[\np(x = i) = \\Theta_i\n\\]\nwhere \\(i = \\{\\)breakfast bacon, chicken, feta, anchovies, sun-dried tomato, olives, pepperoni, artichoke, cheese, Canadian bacon, bacon, ham, meatballs, chili peppers, garlic, ground beef, tomatoes, hot sauce, onions, sausage, pineapple, spinach, mushrooms, grilled onions, green peppers\\(\\}\\).\nWe can simulate this with 100 participants as follows:\n\nresponse &lt;- rcat(100, P_toppings, names(P_toppings))\n\nAnd this should match approximately P_toppings.\n\ntable(response)/100\n\nresponse\n breakfast bacon          chicken             feta \n            0.26             0.19             0.16 \n       anchovies sun-dried tomato           olives \n            0.07             0.15             0.06 \n       pepperoni        artichoke           cheese \n            0.01             0.08             0.00 \n  Canadian bacon            bacon              ham \n            0.02             0.00             0.00 \n       meatballs    chili peppers           garlic \n            0.00             0.00             0.00 \n     ground beef         tomatoes        hot sauce \n            0.00             0.00             0.00 \n          onions          sausage        pineapple \n            0.00             0.00             0.00 \n         spinach        mushrooms   grilled onions \n            0.00             0.00             0.00 \n   green peppers \n            0.00 \n\n\nIt seems that by only asking participants to give the best topping we could already deduce the underlying order…\nTrue, but one motivation for considering ranking data is the amount of information that we gather with a list due to their combinatorial nature. If we ask participants to rank \\(n\\) items, an answer consists in making a single selection out of \\(n!\\) possibilities. Ordering 7 pizza toppings, for example, constitutes making a single selection out of 5040 possibilities!\nIf we don’t relay on lists and there is sparcity, it requires a large number of participants until we get answers of low probability. (For example, we’ll need a very large number of participants until we hear something else but hammer as an exemplar of tools).\n\nNow, what happens if we ask about the second most appropriate topping for a pizza?\n\nNow we need to exclude the first topping that was given, and draw another sample from a categorical distribution. (We don’t allow the participant to repeat toppings, that is, to say that the best topping is pineapple and the second best is also pineapple). This means that now the probability of the topping already given is zero, and that we need to normalize our original probability values by dividing them by the new total probability (which will be lower than 1).\nHere, the probability of getting the element \\(j\\) (where \\(j \\neq i\\)) is\n\\[\np(x = j) = \\frac{\\Theta_j}{\\sum \\Theta_{[-i]}}\n\\]\nwhere \\(\\Theta_{[-i]}\\) represents the probabilities of all the outcomes except of \\(i\\), which was the first one.\n\nWe can go on with the third best topping, where we need to normalize the remaining probabilities by dividing by the new sum of probabilities (e.g., we remove elements \\(i\\) and \\(j\\)).\n\n\\[\np(x = k) = \\frac{\\Theta_k}{\\sum \\Theta_{[-i,-j]}}\n\\]\n\nWe can do this until we get to the last element, which will be drawn with probability 1.\n\nAnd this is the exploded logit distribution.\nThis process can be simulated in R as follows:\n\nrexploded &lt;-  function(n, ranked = 3, prob, labels = NULL){\n  # run n times\n  lapply(1:n, function(nn){\n    res &lt;- rep(NA, ranked)\n    if(!is.null(labels)){\n      res &lt;- factor(res, labels)\n    } else {\n      # if there are no labels, just 1,2,3,...\n      labels &lt;- seq_along(prob)\n    }\n    for(i in 1:ranked){\n      # normalize the probability so that it sums to 1\n      prob &lt;- prob/sum(prob)\n      res[i] &lt;- rcat(1, prob = prob, labels = labels)\n      # remove the choice from the set:\n      prob[res[i]] &lt;- 0\n    }\n    res\n  })\n}\n\nIf we would like to simulate 50 subjects creating a ranking of the best 7 toppings, we would do the following:\n\nres &lt;- rexploded(n = 50,\n                 ranked = 7,\n                 prob = P_toppings,\n                 labels = names(P_toppings))\n# subject 1:\nres[[1]]\n\n[1] sun-dried tomato artichoke        olives          \n[4] breakfast bacon  chicken          pepperoni       \n[7] anchovies       \n25 Levels: breakfast bacon chicken feta ... green peppers\n\n\n\n\n\n\n\n\n\n\n\nWe have simulated ranking data of pizza toppings, can we recover the original probability values and “discover” the underlying order?"
  },
  {
    "objectID": "posts/posts/2026-01-06-a-simple-way-to-model-rankings-with-stan/index.html#fitting-the-exploded-logistic-distribution-in-stan",
    "href": "posts/posts/2026-01-06-a-simple-way-to-model-rankings-with-stan/index.html#fitting-the-exploded-logistic-distribution-in-stan",
    "title": "A simple way to model rankings with Stan",
    "section": "Fitting the exploded logistic distribution in Stan",
    "text": "Fitting the exploded logistic distribution in Stan\nTo fit the model in Stan, I’m going to create a custom probability mass function that takes an array of integers, x, which represents a set of rankings, and a vector of probability values, theta, that sums up to one.\nThe logic of this function is that the probability mass function of a ranking \\(\\{i,j,k, \\ldots, N \\}\\) can be written as a product of normalized categorical distributions (where the first one is just divided by 1).\n\\[\np(x = \\{i,j,k,\\ldots\\}) = \\frac{\\Theta_i}{\\sum \\Theta} \\cdot \\frac{\\Theta_j}{\\sum \\Theta_{[-i]}} \\cdot \\frac{\\Theta_k}{\\sum \\Theta_{[-i, -j]}} \\ldots\n\\]\nFor Stan, we need the log-PDF. In log-space, products become sums, and divisions differences, and the log of \\(\\sum \\Theta\\) will be zero:\n\\[\n\\begin{aligned}\n\\log(p(x = \\{i,j,k,\\ldots\\})) =& \\log(\\Theta_i) - \\log(\\sum \\Theta) \\\\\n& + \\log(\\Theta_j) - \\log(\\sum \\Theta_{[-i]}) \\\\\n&+ \\log(\\Theta_k) - \\log(\\sum \\Theta_{[-i, -j]}) \\\\\n& + \\ldots\n\\end{aligned}\n\\]\nThe following Stan code has a custom function that follows this logic but iterating over the rankings. In each iteration, it aggregates in the variable out the addends of the log probability mass function, and turns the probability of selecting again the already ranked element to zero. I save this code as \"exploded.stan\".\n\nfunctions {\n  real exploded_lpmf(array[] int x, vector Theta){\n    real out = 0;\n    vector[num_elements(Theta)] thetar = Theta;\n    for(pos in x){\n      out += log(thetar[pos]) - log(sum(thetar));\n      thetar[pos] = 0;\n    }\n    return(out);\n  }\n}\ndata{\n  int N_ranking; // total times the choices were ranked\n  int N_ranked; // total choices ranked\n  int N_options; // total options\n  array[N_ranking, N_ranked] int res;\n}\nparameters {\n  simplex[N_options] Theta;\n}\nmodel {\n  target += dirichlet_lpdf(Theta | rep_vector(1, N_options));\n  for(r in 1:N_ranking){\n    target += exploded_lpmf(res[r] | Theta);\n  }\n}\n\nThe whole model includes the usual data declaration, the parameter Theta declared as a simplex (i.e., it sums to one), and a uniform Dirichlet prior for Theta. (I’m assuming that I don’t know how sparse the probabilities are).\nLet’s see if I can recover the parameter values.\n\n# Make the list of lists into a matrix\nres_matrix &lt;- t(sapply(res, as.numeric))\nldata &lt;- list(\n  res = res_matrix, \n  N_ranked = length(res[[1]]), \n  N_options = length(P_toppings), \n  N_ranking = length(res)\n) \n\nm_expl &lt;- cmdstan_model(\"exploded.stan\")\n\nf_exploded &lt;- m_expl$sample(\n  data = ldata,\n  seed = 123,\n  parallel_chains = 4,\n  refresh = 0\n)\n\n\nf_exploded\n\n variable    mean  median   sd  mad      q5     q95 rhat\n lp__     -724.97 -724.61 3.65 3.57 -731.42 -719.65 1.00\n Theta[1]    0.29    0.29 0.04 0.04    0.23    0.35 1.00\n Theta[2]    0.27    0.27 0.04 0.03    0.21    0.33 1.00\n Theta[3]    0.08    0.08 0.01 0.01    0.06    0.10 1.00\n Theta[4]    0.09    0.08 0.01 0.01    0.06    0.11 1.00\n Theta[5]    0.07    0.07 0.01 0.01    0.05    0.09 1.00\n Theta[6]    0.05    0.04 0.01 0.01    0.03    0.06 1.00\n Theta[7]    0.04    0.04 0.01 0.01    0.03    0.06 1.00\n Theta[8]    0.05    0.05 0.01 0.01    0.03    0.06 1.00\n Theta[9]    0.01    0.01 0.00 0.00    0.00    0.02 1.00\n ess_bulk ess_tail\n     1637     2191\n     5941     3310\n     4884     3247\n     5410     2878\n     5771     2956\n     5578     3435\n     5973     3091\n     6068     2815\n     5606     3141\n     6586     3244\n\n # showing 10 of 26 rows (change via 'max_rows' argument or 'cmdstanr_max_rows' option)\n\n\nI plot the posterior distributions of the probability values and the true probability values below.\n\nmcmc_recover_hist(f_exploded$draws(\"Theta\"),\n                  P_toppings,\n                  facet_args =\n                    list(scales = \"fixed\", ncol = 3)) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\nIt looks reasonable. However, if we really want to be sure that this is working, we should probably use simulation based calibration (Talts et al. 2018)."
  },
  {
    "objectID": "posts/posts/2026-01-06-a-simple-way-to-model-rankings-with-stan/index.html#what-is-this-good-for",
    "href": "posts/posts/2026-01-06-a-simple-way-to-model-rankings-with-stan/index.html#what-is-this-good-for",
    "title": "A simple way to model rankings with Stan",
    "section": "What is this good for?",
    "text": "What is this good for?\nThis super simple example shows how to get an underlying ranking based on a set of responses from a number of subjects. It’s straightforward to adapt this model to data from participants ranking elements from different sets of the same size (e.g., 7 out of 25 toppings, 7 out of 25 tools). It’s a little less straightforward if the sets are of different sizes, e.g., rank 7 toppings out 25, but 7 tools out 50. This is just because Stan doesn’t allow ragged arrays. See this Stan Discourse thread for some tips on implementing the latter model."
  },
  {
    "objectID": "posts/posts/2026-01-06-a-simple-way-to-model-rankings-with-stan/index.html#could-this-be-used-as-a-cognitive-model-of-peoples-rankings",
    "href": "posts/posts/2026-01-06-a-simple-way-to-model-rankings-with-stan/index.html#could-this-be-used-as-a-cognitive-model-of-peoples-rankings",
    "title": "A simple way to model rankings with Stan",
    "section": "Could this be used as a cognitive model of people’s rankings?",
    "text": "Could this be used as a cognitive model of people’s rankings?\n\nMaybe. And I enter here in the realm of half baked research, ideal for a blog post.\nLee, Steyvers, and Miller (2014) show the implementation of a cognitive model for rank order data from the latent knowledge of participants, which is based on Thurstonian models (Thurstone 1927, 1931) fitted with Bayesian methods in JAGS (Johnson and Kuhn 2013).\nThe exploded logit model seems to be closely related to the Thurstonian model. The Thurstonian model assumes that each participant assigns an underlying score to each item of a set, which is drawn from a true score with normally distributed error. The score determines the order that the participant gives. We can think about the exploded logit similarly. While I modeled the underlying ranking based on probability values, one could assume that each participant \\(s\\) had their own score \\(\\mu_{is}\\) for each item (or pizza topping) \\(i\\), which is built as a common score \\(\\mu_i\\) together with some individual deviation \\(\\epsilon_{is}\\):\n\\[\n\\mu_{is} = \\mu_i + \\epsilon_{is}\n\\]\nIf we assume that \\(\\epsilon_{is}\\) has a Gumbel distribution, then the probability of \\(\\mu_{is}\\) being ranked first out of N options is determined by a softmax function:\n\\[\nP(i) = \\frac{\\exp(\\mu_i)}{\\sum \\exp(\\mu)}\n\\]\nwhere \\(\\mu\\) is the vector of scores for all elements of the set.\nAnd the probability of ordering \\(j\\) second is:\n\\[\nP(i,j,\\ldots) = \\frac{\\exp(\\mu_j)}{\\sum \\exp(\\mu_{[-i]})}\n\\]\nand so forth.\nThese last equations are essentially the same categorical distributions that I used before, but the softmax function converts the unbounded scores into probabilities first. However, with the exploded logit, the error term goes away leading to a more tractable model. This is not the case for the Thurstonian model. The Thurstonian model is more complex, but at the same time we gain more flexibility. With the error term, the Thurstonian model can incorporate the reliability of the participants’ judgments and even correlations, which, as far as I know, can’t be included in the exploded logit model."
  },
  {
    "objectID": "posts/posts/2026-01-06-a-simple-way-to-model-rankings-with-stan/index.html#how-to-cite-this-post",
    "href": "posts/posts/2026-01-06-a-simple-way-to-model-rankings-with-stan/index.html#how-to-cite-this-post",
    "title": "A simple way to model rankings with Stan",
    "section": "How to cite this post",
    "text": "How to cite this post\n\n\n\n\n\n\nCitation\n\n\n\n\n\nBibTeX:\n@misc{nicenboim2026asimplewaytomodelrankingswithstan,\n  author = {Nicenboim, Bruno},\n  title = {A simple way to model rankings with Stan},\n  year = {2026},\n  month = {januari},\n  url = {https://bruno.nicenboim.me/posts/posts/2026-01-07-a-simple-way-to-model-rankings-with-stan/},\n  doi = {10.5281/zenodo.18171805}\n}\nAPA:\nNicenboim, B. (2026, januari 07). A simple way to model rankings with Stan. https://doi.org/10.5281/zenodo.18171805"
  },
  {
    "objectID": "posts/posts/2026-01-06-a-simple-way-to-model-rankings-with-stan/index.html#session-info",
    "href": "posts/posts/2026-01-06-a-simple-way-to-model-rankings-with-stan/index.html#session-info",
    "title": "A simple way to model rankings with Stan",
    "section": "Session info",
    "text": "Session info\n\nsessionInfo()\n\nR version 4.5.0 (2025-04-11)\nPlatform: x86_64-pc-linux-gnu\nRunning under: Ubuntu 22.04.5 LTS\n\nMatrix products: default\nBLAS:   /usr/lib/x86_64-linux-gnu/blas/libblas.so.3.10.0 \nLAPACK: /usr/lib/x86_64-linux-gnu/lapack/liblapack.so.3.10.0  LAPACK version 3.10.0\n\nlocale:\n [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C              \n [3] LC_TIME=nl_NL.UTF-8        LC_COLLATE=en_US.UTF-8    \n [5] LC_MONETARY=nl_NL.UTF-8    LC_MESSAGES=en_US.UTF-8   \n [7] LC_PAPER=nl_NL.UTF-8       LC_NAME=C                 \n [9] LC_ADDRESS=C               LC_TELEPHONE=C            \n[11] LC_MEASUREMENT=nl_NL.UTF-8 LC_IDENTIFICATION=C       \n\ntime zone: Europe/Amsterdam\ntzcode source: system (glibc)\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets \n[6] methods   base     \n\nother attached packages:\n[1] bayesplot_1.12.0  cmdstanr_0.9.0    rcorpora_2.0.1   \n[4] extraDistr_1.10.0 ggplot2_3.5.2     tidytable_0.11.2 \n\nloaded via a namespace (and not attached):\n [1] gtable_0.3.6         jsonlite_2.0.0      \n [3] dplyr_1.1.4          compiler_4.5.0      \n [5] tidyselect_1.2.1     Rcpp_1.1.0          \n [7] stringr_1.5.1        scales_1.4.0        \n [9] yaml_2.3.10          fastmap_1.2.0       \n[11] plyr_1.8.9           R6_2.6.1            \n[13] labeling_0.4.3       generics_0.1.4      \n[15] distributional_0.5.0 knitr_1.50          \n[17] htmlwidgets_1.6.4    backports_1.5.0     \n[19] checkmate_2.3.2      tibble_3.3.0        \n[21] pillar_1.11.1        RColorBrewer_1.1-3  \n[23] posterior_1.6.1      rlang_1.1.6         \n[25] stringi_1.8.7        xfun_0.52           \n[27] cli_3.6.5            withr_3.0.2         \n[29] magrittr_2.0.4       ps_1.9.1            \n[31] processx_3.8.6       digest_0.6.37       \n[33] grid_4.5.0           lifecycle_1.0.4     \n[35] vctrs_0.6.5          tensorA_0.36.2.1    \n[37] evaluate_1.0.3       glue_1.8.0          \n[39] data.table_1.18.0    farver_2.1.2        \n[41] abind_1.4-8          reshape2_1.4.4      \n[43] rmarkdown_2.29       matrixStats_1.5.0   \n[45] tools_4.5.0          pkgconfig_2.0.3     \n[47] htmltools_0.5.8.1"
  },
  {
    "objectID": "posts/posts/2026-01-06-a-simple-way-to-model-rankings-with-stan/index.html#references",
    "href": "posts/posts/2026-01-06-a-simple-way-to-model-rankings-with-stan/index.html#references",
    "title": "A simple way to model rankings with Stan",
    "section": "References",
    "text": "References\n\n\nBarsalou, Lawrence W. 1985. “Ideals, Central Tendency, and Frequency of Instantiation as Determinants of Graded Structure in Categories.” Journal of Experimental Psychology: Learning, Memory, and Cognition 11 (4): 629.\n\n\nBeggs, S, S Cardell, and J Hausman. 1981. “Assessing the Potential Demand for Electric Cars.” Journal of Econometrics 17 (1): 1–19. https://doi.org/https://doi.org/10.1016/0304-4076(81)90056-7.\n\n\nGakis, Konstantinos, Panos Pardalos, Chang-Hwan Choi, Jae-Hyeon Park, and Jiwun Yoon. 2018. “Simulation of a Probabilistic Model for Multi-Contestant Races.” Athens Journal of Sports 5 (2): 95–114.\n\n\nJohnson, Timothy R., and Kristine M. Kuhn. 2013. “Bayesian Thurstonian Models for Ranking Data Using JAGS.” Behavior Research Methods 45 (3): 857–72. https://doi.org/10.3758/s13428-012-0300-3.\n\n\nLee, Michael D., Mark Steyvers, and Brent Miller. 2014. “A Cognitive Model for Aggregating People’s Rankings.” PLOS ONE 9 (5): e96431. https://doi.org/10.1371/journal.pone.0096431.\n\n\nLuce, R. Duncan. 1959. Individual Choice Behavior : A Theoretical Analysis. Book. Wiley N.Y.\n\n\nPlackett, R. L. 1975. “The Analysis of Permutations.” Journal of the Royal Statistical Society. Series C (Applied Statistics) 24 (2): 193–202. http://www.jstor.org/stable/2346567.\n\n\nTalts, Sean, Michael Betancourt, Daniel Simpson, Aki Vehtari, and Andrew Gelman. 2018. “Validating Bayesian Inference Algorithms with Simulation-Based Calibration,” April. http://arxiv.org/abs/1804.06788.\n\n\nThurstone, Louis L. 1927. “A Law of Comparative Judgement.” Psychological Reviews 34: 273–86.\n\n\n———. 1931. “Rank Order as a Psycho-Physical Method.” Journal of Experimental Psychology 14 (3): 187."
  },
  {
    "objectID": "posts/posts/2026-01-06-a-simple-way-to-model-rankings-with-stan/index.html#footnotes",
    "href": "posts/posts/2026-01-06-a-simple-way-to-model-rankings-with-stan/index.html#footnotes",
    "title": "A simple way to model rankings with Stan",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis model is also called the rank ordered logit model (Beggs, Cardell, and Hausman 1981) or Plackett–Luce model due to Plackett (1975) and Luce (1959), but I liked the explosion part more.↩︎"
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "CURRICULUM VITAE",
    "section": "",
    "text": "Email:\n[initial].[lastname] [at] tilburguniversity.edu\n\n\nOffice:\nD108, Dante Building, Tilburg University\n\n\nAddress:\nDepartment of Computational Cognitive Science,\n\n\n\nCognitive Science and Artificial Intelligence\n\n\n\nTilburg University\n\n\n\nPO Box 90153\n\n\n\n5000 LE Tilburg\n\n\n\nThe Netherlands"
  },
  {
    "objectID": "cv.html#book",
    "href": "cv.html#book",
    "title": "CURRICULUM VITAE",
    "section": "Book",
    "text": "Book\n\nNicenboim, B., D. J. Schad, and S. Vasishth (2025). Introduction to Bayesian Data Analysis for Cognitive Science. 1st ed. Chapman and Hall/CRC. DOI: 10.1201/9780429342646."
  },
  {
    "objectID": "cv.html#published-or-accepted-in-peer-reviewed-journals",
    "href": "cv.html#published-or-accepted-in-peer-reviewed-journals",
    "title": "CURRICULUM VITAE",
    "section": "Published or accepted in peer-reviewed journals",
    "text": "Published or accepted in peer-reviewed journals\n\nBoelders, S. M., B. Nicenboim, E. Butterbrod, W. De Baene, E. Postma, G. Rutten, L. Ong, and K. Gehring (2025). “Predicting cognitive function three months after surgery in patients with a glioma”. In: Neuro-Oncology Advances, p. vdaf081. ISSN: 2632-2498. DOI: 10.1093/noajnl/vdaf081.\n\n\nDubova, M., S. Chandramouli, G. Gigerenzer, P. Grünwald, W. Holmes, T. Lombrozo, M. Marelli, S. Musslick, B. Nicenboim, L. N. Ross, et al. (2025). “Is Ockham’s razor losing its edge? New perspectives on the principle of model parsimony”. In: Proceedings of the National Academy of Sciences 122.5, p. e2401230121. DOI: 10.1073/pnas.2401230121.\n\n\nBoelders, S. M., B. Nicenboim, E. O. Postma, L. S. Ong, and K. Gehring (2024). “Modeling uncertainty in individual predictions of cognitive functioning for untreated glioma patients using Bayesian regression and clinical variables”. In: Neuro-Oncology 26.Supplement_5, pp. v60-v60. ISSN: 1522-8517. DOI: 10.1093/neuonc/noae144.196.\n\n\nSchad, D. J., B. Nicenboim, and S. Vasishth (2024). “Data aggregation can lead to biased inferences in Bayesian linear mixed models and Bayesian analysis of variance.” In: Psychological Methods. ISSN: 1082-989X. DOI: 10.1037/met0000621.\n\n\nDoorn, J. van, J. M. Haaf, A. M. Stefan, E. Wagenmakers, G. E. Cox, C. P. Davis-Stober, A. Heathcote, D. W. Heck, M. Kalish, D. Kellen, et al. (2023). “Bayes Factors for Mixed Models: a Discussion”. In: Computational Brain & Behavior 6.1, pp. 140-158. ISSN: 2522-087X. DOI: 10.1007/s42113-022-00160-3.\n\n\nVasishth, S., H. Yadav, D. J. Schad, and B. Nicenboim (2023). “Sample Size Determination for Bayesian Hierarchical Models Commonly Used in Psycholinguistics”. In: Computational Brain & Behavior 6.1, pp. 102-126. ISSN: 2522-087X. DOI: 10.1007/s42113-021-00125-y.\n\n\nBen-Artzi, I., Y. Kessler, B. Nicenboim, and N. Shahar (2023). “Computational mechanisms underlying latent inverse value updating of unchosen actions”. In: Science Advances 9.42, p. eadi2704. DOI: 10.1126/sciadv.adi2704.\n\n\nStone, K., B. Nicenboim, S. Vasishth, and F. Rösler (2022). “Understanding the effects of constraint and predictability in ERP”. In: Neurobiology of Language, pp. 1-71. ISSN: 2641-4368. DOI: 10.1162/nol_a_00094.\n\n\nPatterson, C., P. B. Schumacher, B. Nicenboim, J. Hagen, and A. Kehler (2022). “A Bayesian approach to German personal and demonstrative pronouns”. In: Frontiers in Psychology, Language Sciences 12. ISSN: 1664-1078. DOI: 10.3389/fpsyg.2021.672927. eprint: https://www.frontiersin.org/articles/10.3389/fpsyg.2021.672927/abstract.\n\n\nSchad, D. J., B. Nicenboim, P. Bürkner, M. Betancourt, and S. Vasishth (2022). “Workflow Techniques for the Robust Use of Bayes Factors”. In: Psychological methods. ISSN: 1082-989X. DOI: 10.1037/met0000472.\n\n\nAlbert, A. and B. Nicenboim (2022). “Modeling Sonority in Terms of Pitch Intelligibility With the Nucleus Attraction Principle”. In: Cognitive Science 46.7, p. e13161. DOI: doi.or1/cogs.13161.\n\n\nLisson, P., D. Pregla, B. Nicenboim, D. Paape, M. L. van het Nederend, F. Burchert, N. Stadie, D. Caplan, and S. Vasishth (2021). “A computational evaluation of two models of retrieval processes in sentence processing in aphasia”. In: Cognitive Science 45.4. DOI: 10.1111/cogs.12956.\n\n\nNicenboim, B., S. Vasishth, and F. Rösler (2020). “Are words pre-activated probabilistically during sentence comprehension? Evidence from new data and a Bayesian random-effects meta-analysis using publicly available data”. In: Neuropsychologia 142, p. 107427. ISSN: 0028-3932. DOI: 10.1016/j.neuropsychologia.2020.107427.\n\n\nVasishth, S., B. Nicenboim, F. Engelmann, and F. Burchert (2019). “Computational models of retrieval processes in sentence processing”. In: Trends in Cognitive Sciences 23.11, pp. 968 - 982. ISSN: 1364-6613. DOI: 10.1016/j.tics.2019.09.003.\n\n\nNicenboim, B., S. Vasishth, F. Engelmann, and K. Suckow (2018). “Exploratory and confirmatory analyses in sentence processing: A case study of number interference in German”. In: Cognitive Science 42.S4, pp. 1075–1100. DOI: 10.1111/cogs.12589.\n\n\nNicenboim, B., T. B. Roettger, and S. Vasishth (2018). “Using meta-analysis for evidence synthesis: The case of incomplete neutralization in German”. In: Journal of Phonetics 70, pp. 39–55. DOI: 10.1016/j.wocn.2018.06.001.\n\n\nVasishth, S., B. Nicenboim, M. E. Beckman, F. Li, and E. Kong (2018). “Bayesian data analysis in the phonetic sciences: A tutorial introduction”. In: Journal of Phonetics 71, pp. 147–161. DOI: 10.1016/j.wocn.2018.07.008.\n\n\nNicenboim, B. and S. Vasishth (2018). “Models of Retrieval in Sentence Comprehension: A computational evaluation using Bayesian hierarchical modeling”. In: Journal of Memory and Language 99, pp. 1 –34. ISSN: 0749-596X. DOI: 10.1016/j.jml.2017.08.004.\n\n\nPaape, D., B. Nicenboim, and S. Vasishth (2017). “Does antecedent complexity affect ellipsis processing? An empirical investigation”. In: Glossa: A journal of general linguistics. 2.1, p. 71. DOI: 10.5334/gjgl.290.\n\n\nNicenboim, B., P. Logačev, C. Gattei, and S. Vasishth (2016). “When high-capacity readers slow down and low-capacity readers speed up: Working memory and locality effects”. In: Frontiers in Psychology 7.280. ISSN: 1664-1078. DOI: 10.3389/fpsyg.2016.00280.\n\n\nNicenboim, B. and S. Vasishth (2016). “Statistical methods for linguistic research: Foundational Ideas - Part II”. In: Language and Linguistics Compass 10.11, pp. 591–613. ISSN: 1749-818X. DOI: 10.1111/lnc3.12207.\n\n\nVasishth, S. and B. Nicenboim (2016). “Statistical Methods for Linguistic Research: Foundational Ideas - Part I”. In: Language and Linguistics Compass 10.8, pp. 349–369. ISSN: 1749-818X. DOI: 10.1111/lnc3.12201.\n\n\nNicenboim, B., S. Vasishth, C. Gattei, M. Sigman, and R. Kliegl (2015). “Working memory differences in long-distance dependency resolution”. In: Frontiers in Psychology 6.312. ISSN: 1664-1078. DOI: 10.3389/fpsyg.2015.00312."
  },
  {
    "objectID": "cv.html#unpublishedunder-review-manuscripts",
    "href": "cv.html#unpublishedunder-review-manuscripts",
    "title": "CURRICULUM VITAE",
    "section": "Unpublished/Under review manuscripts",
    "text": "Unpublished/Under review manuscripts\n\nNicenboim, B., M. K. van Vugt, R. G. Alhama, B. Anderson, F. Bontje, M. Chimento, S. Columbus, E. S. Dalmaijer, J. Dotlacil, S. M. Østergaard, et al. (2025). “It takes a village to model complex behaviour: A community-based approach”. DOI: 10.31234/osf.io/d2v54_v1.\n\n\nMoorman, S., M. van Elk, E. Vassena, A. Abelmann, F. de Andrade, A. Badura, J. Benichov, N. de Bode, B. Braams, I. Brazil, et al. (2025). “The importance of Brain, Behaviour and Cognition research in changing times and landscapes: Challenges, opportunities and future prospects”. En. DOI: 10.5281/ZENODO.17493166.\n\n\nVasishth, S., B. Nicenboim, N. Chopin, and R. Ryder (2017). “Bayesian hierarchical finite mixture models of reading times: A case study”. DOI: 10.17605/OSF.IO/FWX3S."
  },
  {
    "objectID": "cv.html#short-peer-reviewed-papers-in-conferences",
    "href": "cv.html#short-peer-reviewed-papers-in-conferences",
    "title": "CURRICULUM VITAE",
    "section": "Short peer-reviewed papers in conferences",
    "text": "Short peer-reviewed papers in conferences\n\nNicenboim, B. (2023). “The CoFI Reader: A Continuous Flow of Information approach to modeling reading”. In: MathPsych/ICCM/EMPG. University of Amsterdam, the Netherlands.\n\n\nNicenboim, B. (2018). “The implementation of a model of choice: The (truncated) linear ballistic accumulator”. In: StanCon. Aalto University, Helsinki, Finland. DOI: 10.5281/zenodo.1465990.\n\n\nVasishth, S., N. Chopin, R. Ryder, and B. Nicenboim (2017). “Modelling dependency completion in sentence comprehension as a Bayesian hierarchical mixture process: A case study involving Chinese relative clauses”. In: Proceedings of Cognitive Science Conference. London, UK.\n\n\nVasishth, S., L. Jaeger, and B. Nicenboim (2017). “Feature overwriting as a finite mixture process: Evidence from comprehension data”. In: Proceedings of MathPsych/ICCM Conference. Warwick, UK.\n\n\nNicenboim, B. and S. Vasishth (2017). “Models of Retrieval in Sentence Comprehension”. In: StanCon. (superseeded by 10.1016/j.jml.2017.08.004). Columbia University New York, NY."
  },
  {
    "objectID": "cv.html#contributions-to-open-software",
    "href": "cv.html#contributions-to-open-software",
    "title": "CURRICULUM VITAE",
    "section": "Contributions to open software",
    "text": "Contributions to open software\n\neeguana (https://bruno.nicenboim.me/eeguana/) An R package for tidy-ish manipulation of EEG data. Author\npangoling (https://bruno.nicenboim.me/pangoling/) An R package for estimating the log-probabilities of words in a given context using transformer models. Author\nloo (https://cran.r-project.org/web/packages/loo/). Efficient Leave-One-Out Cross-Validation and WAIC for Bayesian Models. Contributor\nMNE (https://mne.tools/). Open-source Python package for exploring, visualizing, and analyzing human neurophysiological data. Bug fixing\nBayesplot (https://github.com/stan-dev) R package providing an extensive library of plotting functions for use after fitting Bayesian models. Bug fixing"
  },
  {
    "objectID": "cv.html#workshops-and-summer-schools",
    "href": "cv.html#workshops-and-summer-schools",
    "title": "CURRICULUM VITAE",
    "section": "Workshops and summer schools",
    "text": "Workshops and summer schools\n\n\n\n\n\n\n\n2017-2025\n“Advanced Bayesian methods”,\n\n\n\nYearly Summer School on Statistical Methods for Linguistics and Psychology (SMLP),\n\n\n\nUniversity of Potsdam, Germany\n\n\n2020-2021\n“Introduction to computational Bayesian methods using Stan”, Physalia courses,\n\n\n\nBerlin, Germany\n\n\n2020\n“Methods in Advanced Statistics”, with Shravan Vasishth\n\n\n\n2020 Winter School organized by Netherlands Graduate School in Linguistics (LOT),\n\n\n\nTilburg, Netherlands\n\n\n2019\n“Introduction to Bayesian statistics using brms”,\n\n\n\nUniversity of Cologne, Germany\n\n\n2019\n“Introduction to Bayesian statistics using brms”,\n\n\n\nUniversity of Edinburgh, UK\n\n\n2018\nTalk:“Cognitive models of memory processes in sentence comprehension: A case study using Bayesian hierarchical modeling”\n\n\n\nMasterclass in Bayesian Statistics, Research school,\n\n\n\nCIRM (Marseille Luminy, France)\n\n\n2017\n“Introduction to Bayesian Modeling using Stan”,\n\n\n\n13. Tagung der Fachgruppe Methoden und Evaluation der Deutschen Gesellschaft für Psychologie,\n\n\n\nTübingen, Germany"
  },
  {
    "objectID": "cv.html#courses",
    "href": "cv.html#courses",
    "title": "CURRICULUM VITAE",
    "section": "Courses",
    "text": "Courses\n\n\n\n\n\n\n\n2025-2026\nLecturer in “Research Workshop”, BA in Cognitive Science & AI. Tilburg University\n\n\n2022-2025\nLecturer in “Bayesian Multilevel Models/Bayesian Modeling for Data Science”, Master in Data Science & Society. Tilburg University\n\n\n2021-2025\nLecturer in “Bayesian Models of Cognitive Processes”, Master in Cognitive Science & AI. Tilburg University\n\n\n2020-2021\nLecturer in “Research skill: Programing with R”, Data Science & Society. Tilburg University\n\n\n2020-2021\nLecturer in “Methodology for Premasters Data Science & Society”. Tilburg University\n\n\n2015–2017 (Winter)\nLecturer in “Advanced Data Analysis”. University of Potsdam\n\n\n2016 (Summer)\nLecturer in “Predictions in Language Processing”. University of Potsdam\n\n\n2015–2016 (Winter)\nLecturer in “Individual Differences in Sentence Processing”. University of Potsdam\n\n\n2015 (Summer)\nLecturer in “Predictions in Language Processing”. University of Potsdam\n\n\n2008–2010\nTeaching assistant in “Syntax Beginners” and “Foundations of Theoretical Linguistics” Tel Aviv University"
  },
  {
    "objectID": "posts/posts/2026-01-06-welcome-to-my-new-quarto-site/index.html",
    "href": "posts/posts/2026-01-06-welcome-to-my-new-quarto-site/index.html",
    "title": "Welcome to my new Quarto site",
    "section": "",
    "text": "After years of using Hugo, I’ve finally made the switch to Quarto.\nThe main reason? Hugo wasn’t allowing me to update my website anymore!\nThe old posts from my Hugo site are gone (for now), but I’m looking forward to building up new content here. Stay tuned!"
  },
  {
    "objectID": "posts/posts/2026-01-06-welcome-to-my-new-quarto-site/index.html#a-fresh-start",
    "href": "posts/posts/2026-01-06-welcome-to-my-new-quarto-site/index.html#a-fresh-start",
    "title": "Welcome to my new Quarto site",
    "section": "",
    "text": "After years of using Hugo, I’ve finally made the switch to Quarto.\nThe main reason? Hugo wasn’t allowing me to update my website anymore!\nThe old posts from my Hugo site are gone (for now), but I’m looking forward to building up new content here. Stay tuned!"
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "Posts",
    "section": "",
    "text": "A simple way to model rankings with Stan\n\n\n\nStan\n\nBayesian\n\nR\n\nCitable\n\n\n\nHow to fit the exploded logit distribution in Stan to model ranking data\n\n\n\n\n\nJan 7, 2026\n\n\nBruno Nicenboim\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome to my new Quarto site\n\n\n\nMeta\n\nQuarto\n\n\n\n\n\n\n\n\n\nJan 6, 2026\n\n\nBruno Nicenboim\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "publications.html#section",
    "href": "publications.html#section",
    "title": "Publications & Presentations",
    "section": "2025",
    "text": "2025\n\nBoelders, S. M., B. Nicenboim, E. Butterbrod, W. De Baene, E. Postma, G. Rutten, L. Ong, and K. Gehring (2025). “Predicting cognitive function three months after surgery in patients with a glioma”. In: Neuro-Oncology Advances, p. vdaf081. ISSN: 2632-2498. DOI: 10.1093/noajnl/vdaf081. [eprint] [read] [bib]\n\n\nDubova, M., S. Chandramouli, G. Gigerenzer, P. Grünwald, W. Holmes, T. Lombrozo, M. Marelli, S. Musslick, B. Nicenboim, L. N. Ross, et al. (2025). “Is Ockham’s razor losing its edge? New perspectives on the principle of model parsimony”. In: Proceedings of the National Academy of Sciences 122.5, p. e2401230121. DOI: 10.1073/pnas.2401230121. [eprint] [read] [bib]"
  },
  {
    "objectID": "publications.html#section-1",
    "href": "publications.html#section-1",
    "title": "Publications & Presentations",
    "section": "2024",
    "text": "2024\n\nBoelders, S. M., B. Nicenboim, E. O. Postma, L. S. Ong, and K. Gehring (2024). “Modeling uncertainty in individual predictions of cognitive functioning for untreated glioma patients using Bayesian regression and clinical variables”. In: Neuro-Oncology 26.Supplement_5, pp. v60-v60. ISSN: 1522-8517. DOI: 10.1093/neuonc/noae144.196. [eprint] [read] [bib]\n\n\nSchad, D. J., B. Nicenboim, and S. Vasishth (2024). “Data aggregation can lead to biased inferences in Bayesian linear mixed models and Bayesian analysis of variance.” In: Psychological Methods. ISSN: 1082-989X. DOI: 10.1037/met0000621. [bib]"
  },
  {
    "objectID": "publications.html#section-2",
    "href": "publications.html#section-2",
    "title": "Publications & Presentations",
    "section": "2023",
    "text": "2023\n\nDoorn, J. van, J. M. Haaf, A. M. Stefan, E. Wagenmakers, G. E. Cox, C. P. Davis-Stober, A. Heathcote, D. W. Heck, M. Kalish, D. Kellen, et al. (2023). “Bayes Factors for Mixed Models: a Discussion”. In: Computational Brain & Behavior 6.1, pp. 140-158. ISSN: 2522-087X. DOI: 10.1007/s42113-022-00160-3. [eprint] [bib]\n\n\nVasishth, S., H. Yadav, D. J. Schad, and B. Nicenboim (2023). “Sample Size Determination for Bayesian Hierarchical Models Commonly Used in Psycholinguistics”. In: Computational Brain & Behavior 6.1, pp. 102-126. ISSN: 2522-087X. DOI: 10.1007/s42113-021-00125-y. [eprint] [bib]\n\n\nBen-Artzi, I., Y. Kessler, B. Nicenboim, and N. Shahar (2023). “Computational mechanisms underlying latent inverse value updating of unchosen actions”. In: Science Advances 9.42, p. eadi2704. DOI: 10.1126/sciadv.adi2704. [eprint] [bib]"
  },
  {
    "objectID": "publications.html#section-3",
    "href": "publications.html#section-3",
    "title": "Publications & Presentations",
    "section": "2022",
    "text": "2022\n\nStone, K., B. Nicenboim, S. Vasishth, and F. Rösler (2022). “Understanding the effects of constraint and predictability in ERP”. In: Neurobiology of Language, pp. 1-71. ISSN: 2641-4368. DOI: 10.1162/nol_a_00094. [eprint] [code/data] [bib]\n\n\nPatterson, C., P. B. Schumacher, B. Nicenboim, J. Hagen, and A. Kehler (2022). “A Bayesian approach to German personal and demonstrative pronouns”. In: Frontiers in Psychology, Language Sciences 12. ISSN: 1664-1078. DOI: 10.3389/fpsyg.2021.672927. eprint: https://www.frontiersin.org/articles/10.3389/fpsyg.2021.672927/abstract. [eprint] [bib]\n\n\nSchad, D. J., B. Nicenboim, P. Bürkner, M. Betancourt, and S. Vasishth (2022). “Workflow Techniques for the Robust Use of Bayes Factors”. In: Psychological methods. ISSN: 1082-989X. DOI: 10.1037/met0000472. [eprint] [code/data] [bib]\n\n\nAlbert, A. and B. Nicenboim (2022). “Modeling Sonority in Terms of Pitch Intelligibility With the Nucleus Attraction Principle”. In: Cognitive Science 46.7, p. e13161. DOI: doi.or1/cogs.13161. [eprint] [code/data] [bib]"
  },
  {
    "objectID": "publications.html#section-4",
    "href": "publications.html#section-4",
    "title": "Publications & Presentations",
    "section": "2021",
    "text": "2021\n\nLisson, P., D. Pregla, B. Nicenboim, D. Paape, M. L. van het Nederend, F. Burchert, N. Stadie, D. Caplan, and S. Vasishth (2021). “A computational evaluation of two models of retrieval processes in sentence processing in aphasia”. In: Cognitive Science 45.4. DOI: 10.1111/cogs.12956. [eprint] [bib]"
  },
  {
    "objectID": "publications.html#section-5",
    "href": "publications.html#section-5",
    "title": "Publications & Presentations",
    "section": "2020",
    "text": "2020\n\nNicenboim, B., S. Vasishth, and F. Rösler (2020). “Are words pre-activated probabilistically during sentence comprehension? Evidence from new data and a Bayesian random-effects meta-analysis using publicly available data”. In: Neuropsychologia 142, p. 107427. ISSN: 0028-3932. DOI: 10.1016/j.neuropsychologia.2020.107427. [eprint] [bib]"
  },
  {
    "objectID": "publications.html#section-6",
    "href": "publications.html#section-6",
    "title": "Publications & Presentations",
    "section": "2019",
    "text": "2019\n\nVasishth, S., B. Nicenboim, F. Engelmann, and F. Burchert (2019). “Computational models of retrieval processes in sentence processing”. In: Trends in Cognitive Sciences 23.11, pp. 968 - 982. ISSN: 1364-6613. DOI: 10.1016/j.tics.2019.09.003. [eprint] [bib]"
  },
  {
    "objectID": "publications.html#section-7",
    "href": "publications.html#section-7",
    "title": "Publications & Presentations",
    "section": "2018",
    "text": "2018\n\nNicenboim, B., S. Vasishth, F. Engelmann, and K. Suckow (2018). “Exploratory and confirmatory analyses in sentence processing: A case study of number interference in German”. In: Cognitive Science 42.S4, pp. 1075–1100. DOI: 10.1111/cogs.12589. [eprint] [code/data] [bib]\n\n\nNicenboim, B., T. B. Roettger, and S. Vasishth (2018). “Using meta-analysis for evidence synthesis: The case of incomplete neutralization in German”. In: Journal of Phonetics 70, pp. 39–55. DOI: 10.1016/j.wocn.2018.06.001. [eprint] [code/data] [bib]\n\n\nVasishth, S., B. Nicenboim, M. E. Beckman, F. Li, and E. Kong (2018). “Bayesian data analysis in the phonetic sciences: A tutorial introduction”. In: Journal of Phonetics 71, pp. 147–161. DOI: 10.1016/j.wocn.2018.07.008. [eprint] [code/data] [bib]\n\n\nNicenboim, B. and S. Vasishth (2018). “Models of Retrieval in Sentence Comprehension: A computational evaluation using Bayesian hierarchical modeling”. In: Journal of Memory and Language 99, pp. 1 –34. ISSN: 0749-596X. DOI: 10.1016/j.jml.2017.08.004. [eprint] [code/data] [bib]"
  },
  {
    "objectID": "publications.html#section-8",
    "href": "publications.html#section-8",
    "title": "Publications & Presentations",
    "section": "2017",
    "text": "2017\n\nPaape, D., B. Nicenboim, and S. Vasishth (2017). “Does antecedent complexity affect ellipsis processing? An empirical investigation”. In: Glossa: A journal of general linguistics. 2.1, p. 71. DOI: 10.5334/gjgl.290. [eprint] [bib]"
  },
  {
    "objectID": "publications.html#section-9",
    "href": "publications.html#section-9",
    "title": "Publications & Presentations",
    "section": "2016",
    "text": "2016\n\nNicenboim, B., P. Logačev, C. Gattei, and S. Vasishth (2016). “When high-capacity readers slow down and low-capacity readers speed up: Working memory and locality effects”. In: Frontiers in Psychology 7.280. ISSN: 1664-1078. DOI: 10.3389/fpsyg.2016.00280. [eprint] [code/data] [bib]\n\n\nNicenboim, B. and S. Vasishth (2016). “Statistical methods for linguistic research: Foundational Ideas - Part II”. In: Language and Linguistics Compass 10.11, pp. 591–613. ISSN: 1749-818X. DOI: 10.1111/lnc3.12207. [eprint] [bib]\n\n\nVasishth, S. and B. Nicenboim (2016). “Statistical Methods for Linguistic Research: Foundational Ideas - Part I”. In: Language and Linguistics Compass 10.8, pp. 349–369. ISSN: 1749-818X. DOI: 10.1111/lnc3.12201. [eprint] [bib]"
  },
  {
    "objectID": "publications.html#section-10",
    "href": "publications.html#section-10",
    "title": "Publications & Presentations",
    "section": "2015",
    "text": "2015\n\nNicenboim, B., S. Vasishth, C. Gattei, M. Sigman, and R. Kliegl (2015). “Working memory differences in long-distance dependency resolution”. In: Frontiers in Psychology 6.312. ISSN: 1664-1078. DOI: 10.3389/fpsyg.2015.00312. [eprint] [code/data] [bib]"
  },
  {
    "objectID": "team.html",
    "href": "team.html",
    "title": "Team",
    "section": "",
    "text": "Together with Giovanni Cassani, I am the PI of the Computational Psycholinguistics Lab.\nMy current team is the following:"
  },
  {
    "objectID": "team.html#phd-student",
    "href": "team.html#phd-student",
    "title": "Team",
    "section": "Phd Student",
    "text": "Phd Student\n\n\n\n\n\nSara Møller Østergaard"
  }
]