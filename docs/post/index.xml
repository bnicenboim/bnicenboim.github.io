<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Bruno Nicenboim</title>
    <link>/post/</link>
    <description>Recent content in Posts on Bruno Nicenboim</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 29 Jan 2025 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Save the Date: Computational Psycholinguistics Meeting 2025</title>
      <link>/2025/01/29/save-the-date-computational-psycholinguistics-meeting-2025/</link>
      <pubDate>Wed, 29 Jan 2025 00:00:00 +0000</pubDate>
      
      <guid>/2025/01/29/save-the-date-computational-psycholinguistics-meeting-2025/</guid><description>&lt;p&gt;We are excited to announce the launch of a new recurring meeting dedicated exclusively to computational psycholinguistics. The first Computational Psycholinguistics Meeting 2025 will take place in Utrecht, the Netherlands, on December 18–19, 2025.&lt;/p&gt;
&lt;p&gt;This meeting provides a dedicated space for researchers working with (neuro-)symbolic, Bayesian, deep-learning, connectionist, and mechanistic models like ACT-R to discuss how these approaches explain and predict human language production, perception, and processing.&lt;/p&gt;
&lt;h3 id=&#34;keynote-speakers&#34;&gt;Keynote Speakers&lt;/h3&gt;
&lt;p&gt;Stefan Frank (Radboud University) – &lt;a href=&#34;http://stefanfrank.info/&#34;&gt;Website&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Vera Demberg (Saarland University) – &lt;a href=&#34;https://www.uni-saarland.de/lehrstuhl/demberg/members/verademberg.html&#34;&gt;Website&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;topics-not-exhaustive&#34;&gt;Topics (not exhaustive)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;How (neuro-)symbolic, Bayesian, deep-learning, connectionist, and  mechanistic models (e.g., ACT-R) explain and predict human language processing.&lt;/li&gt;
&lt;li&gt;Strengths and limitations of different modeling approaches.&lt;/li&gt;
&lt;li&gt;Integrating linguistic information across words, sentences, and discourse for comprehension and production.&lt;/li&gt;
&lt;li&gt;Computational, algorithmic, and implementational analyses of psycholinguistic phenomena.&lt;/li&gt;
&lt;li&gt;Advances in modeling semantics, syntax, sentence processing, speech perception, and production.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;More details, including the call for abstracts, will be available soon. For updates, visit: &lt;a href=&#34;https://cpl2025.sites.uu.nl/&#34;&gt;https://cpl2025.sites.uu.nl/&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;organizers&#34;&gt;Organizers&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://www.jakubdotlacil.com/&#34;&gt;Jakub Dotlačil&lt;/a&gt;, &lt;a href=&#34;https://www.cl.uzh.ch/en/research-groups/digital-linguistics/people/group-leader/jaeger.html&#34;&gt;Lena Jäger&lt;/a&gt;, and &lt;a href=&#34;https://www.bruno.nicenboim.me/&#34;&gt;Bruno Nicenboim&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Doe mee aan ons experiment waarbij je teksten moet aanvullen! Draag bij aan taalkundig onderzoek en verdien €10!</title>
      <link>/2024/02/01/participants/</link>
      <pubDate>Thu, 01 Feb 2024 00:00:00 +0000</pubDate>
      
      <guid>/2024/02/01/participants/</guid><description>


&lt;p&gt;Is Nederlands jouw moedertaal en ben je boven de 18 jaar? Houd je van taalpuzzels en snelle denkopdrachten? Zo ja, dan hebben we een interessante kans voor jou!&lt;/p&gt;
&lt;p&gt;We voeren een taalkundig experiment uit en nodigen je uit om deel te nemen. Het doel van dit experiment is om de verwerking van taal te onderzoeken door te bestuderen hoe Nederlandstalige sprekers teksten met ontbrekende woorden aanvullen. Zo werkt het:&lt;/p&gt;
&lt;p&gt;Je krijgt korte stukjes tekst te zien met ontbrekende woorden. Het is jouw taak om deze teksten aan te vullen met het eerste woord dat in je opkomt. Dit is een leuke manier om te onderzoeken hoe onze hersenen taal verwerken en weggelaten informatie aanvullen.&lt;/p&gt;
&lt;p&gt;Het experiment zal plaatsvinden in MindLabs Tilburg en duurt ongeveer één uur. Om je te bedanken voor je tijd en bijdrage, ontvang je een vergoeding van €10!&lt;/p&gt;
&lt;p&gt;Vereisten om te kunnen deelnemen aan dit experiment:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Nederlands is je enige moedertaal (je bent niet meertalig opgevoed)&lt;/li&gt;
&lt;li&gt;Je bent 18 jaar of ouder&lt;/li&gt;
&lt;li&gt;Je bent in staat om Nederlandse teksten te lezen en te begrijpen&lt;/li&gt;
&lt;li&gt;Je hebt geen leer- of leesstoornis (bijv. ADHD, ADD, autisme of dyslexie)&lt;/li&gt;
&lt;li&gt;Je hebt een normaal of naar normaal gecorrigeerd (bril/contactlenzen/operatie) gezichtsvermogen&lt;/li&gt;
&lt;li&gt;Je bent bereid om deel te nemen aan een experiment waarbij je ontbrekende woorden in teksten moet aanvullen&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Interesse? Neem contact met ons op via &lt;a href=&#34;mailto:tekstonderzoek_aanvullen@tilburguniversity.edu&#34; class=&#34;email&#34;&gt;tekstonderzoek_aanvullen@tilburguniversity.edu&lt;/a&gt; om jouw sessie in te plannen. We kijken uit naar jouw deelname aan dit interessante onderzoek!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Not a tutorial of a Bayesian implementation of a reinforcement learning model</title>
      <link>/2021/11/29/bayesian-h-reinforcement-learning/</link>
      <pubDate>Mon, 29 Nov 2021 00:00:00 +0000</pubDate>
      
      <guid>/2021/11/29/bayesian-h-reinforcement-learning/</guid><description>


&lt;p&gt;&lt;strong&gt;Update Feb 1st, 2024: I updated the Stan syntax.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color: red;&#34;&gt;
&lt;strong&gt;⚠️ This post is NOT about “Masheen Lurning”: I’m spelling it like this to avoid luring visitors into this post for the wrong reasons. In the context of “Masheen Lurning”, one teaches a “masheen” to make the most rewarding decisions. In contrast, this post is about fitting human (or possibly also animal) data with decisions made along a number of trials assuming that some type of reinforcement learning occurred.⚠️&lt;/strong&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;I’ll focus on a Bayesian implementation of Q-learning using &lt;a href=&#34;https://mc-stan.org/&#34;&gt;Stan&lt;/a&gt;. Q-learning is a specific type of reinforcement learning (RL) model that can explain how humans (and other organisms) learn behavioral policies (to make decisions) from rewards. &lt;span class=&#34;citation&#34;&gt;Niv (&lt;a href=&#34;/2021/11/29/bayesian-h-reinforcement-learning/#ref-niv2009reinforcement&#34;&gt;2009&lt;/a&gt;)&lt;/span&gt; presents a very extensive review of RL in the brain. Readers interested in The Book about RL should see &lt;span class=&#34;citation&#34;&gt;Sutton and Barto (&lt;a href=&#34;/2021/11/29/bayesian-h-reinforcement-learning/#ref-sutton2018reinforcement&#34;&gt;2018&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;I will &lt;em&gt;mostly&lt;/em&gt; follow the implementation of &lt;span class=&#34;citation&#34;&gt;van Geen and Gerraty (&lt;a href=&#34;/2021/11/29/bayesian-h-reinforcement-learning/#ref-vanGeenGerraty2021&#34;&gt;2021&lt;/a&gt;)&lt;/span&gt;, who present a tutorial on a hierarchical Bayesian implementation of Q-learning. However, I think there are places where a different parametrization would be more appropriate. I will also model my own simulated data, especially because I wanted to check if I’m doing things right. Another useful tutorial about Bayesian modeling of RL is &lt;span class=&#34;citation&#34;&gt;Zhang et al. (&lt;a href=&#34;/2021/11/29/bayesian-h-reinforcement-learning/#ref-ZhangEtAl2020&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt;, however, that tutorial uses a Rescorla–Wagner model, rather than Q-learning. Before you continue reading, notice that if you just want to fit data using a RL model and don’t care about the nitty gritty details, you can just use the R package &lt;a href=&#34;https://ccs-lab.github.io/hBayesDM/articles/getting_started.html&#34;&gt;hBayesDM&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The rest of the post is about simulating one subject doing an experiment and fitting it with Stan, and then simulating several subjects doing the same experiment and fitting that data with Stan.&lt;/p&gt;
&lt;p&gt;First, I’m going to load some R packages that will be useful throughout this post and set a seed, so that the simulated data will be always the same.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr) # Data manipulation
library(tidyr) # To pivot the data
library(purrr) # List manipulation
library(ggplot2) # Nice plots
library(extraDistr) # More distributions
library(MASS) # Tu use the multinormal distribution
library(cmdstanr) # Lightweight Stan interface
library(bayesplot) # Nice Bayesian plots
set.seed(123)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;restless-bandits-or-slot-machines-with-non-fixed-probabilities-of-a-reward&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Restless bandits or slot machines with non-fixed probabilities of a reward&lt;/h1&gt;
&lt;p&gt;I simulate data from a two-armed restless bandit, where the arms are independent. This is basically as if there were two slot machines that offer rewards with probabilities that vary with time following a random walk. I always wondered why so many researchers are modeling slot machines, but it turns out they can actually represent realistic options in life. There is a &lt;a href=&#34;https://speekenbrink-lab.github.io/modelling/2019/02/28/fit_kf_rl_1.html&#34;&gt;post from Speekenbrink lab&lt;/a&gt; that gives context to the restless bandits (and they also simulate data from a different RL algorithm).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/2021/11/29/bayesian-h-reinforcement-learning/https://media.giphy.com/media/xT1XGOzcLKDYN2kxLW/giphy.gif&#34; /&gt;&lt;/p&gt;
&lt;p&gt;I set up here a restless bandit with 2 arms that runs over 1000 trials. The reward, &lt;span class=&#34;math inline&#34;&gt;\(R_{t,a}\)&lt;/span&gt;, for the time step or trial &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; for the arm &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt; follows a random walk and it’s defined as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;For &lt;span class=&#34;math inline&#34;&gt;\(t =1\)&lt;/span&gt;,
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
R_{t,a} \sim \mathit{Normal}(0, \sigma)
\end{equation}\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;For &lt;span class=&#34;math inline&#34;&gt;\(t &amp;gt;1\)&lt;/span&gt;,&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
R_{t,a} \sim \mathit{Normal}(R_{t-1,a}, \sigma)
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;with &lt;span class=&#34;math inline&#34;&gt;\(\sigma = .1\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In R, this looks as follows.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# set up
N_arms &amp;lt;- 2
N_trials &amp;lt;- 1000
sigma &amp;lt;- .1
## Reward matrix:
# First define a matrix with values sampled from Normal(0, sigma)
R &amp;lt;- matrix(rnorm(N_trials * N_arms, mean = 0, sd = sigma), 
            ncol = N_arms)
# Then I do a cumulative sum over the rows (MARGIN = 2)
R &amp;lt;- apply(R, MARGIN = 2, FUN = cumsum)
colnames(R) &amp;lt;- paste0(&amp;quot;arm_&amp;quot;, 1:N_arms)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The first rows of the reward matrix will look like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(R)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;      arm_1 arm_2
[1,] -0.056 -0.10
[2,] -0.079 -0.20
[3,]  0.077 -0.21
[4,]  0.084 -0.22
[5,]  0.097 -0.47
[6,]  0.268 -0.37&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this simulated experiment, a subject is presented at each trial with two choices, to either select arm one or arm two. Each time they’ll get a reward (or a negative reward, that is, a punisment) determined by a random walk simulated before in the matrix &lt;span class=&#34;math inline&#34;&gt;\(R\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Some &lt;code&gt;tidyr&lt;/code&gt; to plot the rewards.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;d_exp &amp;lt;- R %&amp;gt;%
  as_tibble() %&amp;gt;%
  pivot_longer(cols = everything(), 
               names_to = &amp;quot;arm&amp;quot;, 
               values_to = &amp;quot;reward&amp;quot;) %&amp;gt;%
  group_by(arm) %&amp;gt;%
  mutate(trial = 1:n())
ggplot(d_exp, aes(x = trial, y = reward, color = arm)) +
  geom_line()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/2021/11/29/bayesian-h-reinforcement-learning//2021/11/29/bayesian-h-reinforcement-learning/index_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;I see that the arm that will give better rewards depends on the trial, and that a subject should keep &lt;em&gt;exploring&lt;/em&gt; at least until trial ~650 to verify that they are not &lt;em&gt;exploiting&lt;/em&gt; the wrong arm. There are many, many variations of the bandits, some of them give a fixed reward with some probability based on the random walk, sometimes there are more arms, and all of them are presented as choices, and in some other tasks only a random subset of the arms are presented to the subject.&lt;/p&gt;
&lt;div id=&#34;simulating-actions-using-q-learning&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Simulating actions using Q-learning&lt;/h2&gt;
&lt;p&gt;There are also many, many RL models. Q-learning is a type of RL model where an agent (e.g., a human subject) learns the predictive value (in terms of future expected rewards) of taking a specific action (e.g., choosing arm one or two of the bandit) at a certain state (here, at a given trial), &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;. This predictive value is denoted as &lt;span class=&#34;math inline&#34;&gt;\(Q\)&lt;/span&gt;. In a simplified model of Q-learning, the actions of a subject will depend (to some extent) on &lt;span class=&#34;math inline&#34;&gt;\(Q\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Let’s focus on the case of two arms:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;For &lt;span class=&#34;math inline&#34;&gt;\(t = 1\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In the first trial, &lt;span class=&#34;math inline&#34;&gt;\(Q\)&lt;/span&gt; will be initialized with the same value for each action (e.g., &lt;span class=&#34;math inline&#34;&gt;\(0.5\)&lt;/span&gt;), and the subject will take an action with probability &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; based on their bias to one of the arms. I use &lt;span class=&#34;math inline&#34;&gt;\(\beta_{0}\)&lt;/span&gt; as the bias to arm 2. This parameter can vary freely, &lt;span class=&#34;math inline&#34;&gt;\([-\infty,\infty]\)&lt;/span&gt;, and it’s converted to the &lt;span class=&#34;math inline&#34;&gt;\([0,1]\)&lt;/span&gt; range using &lt;span class=&#34;math inline&#34;&gt;\(logit^{-1}\)&lt;/span&gt;. If there are more arms presented one should use a &lt;span class=&#34;math inline&#34;&gt;\(softmax\)&lt;/span&gt; function instead.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
\begin{aligned}
Q_{t=1,a = 1} &amp;amp;= .5 \\
Q_{t=1,a = 2} &amp;amp;= .5 \\
\theta &amp;amp;= logit^{-1}(\beta_0)
\end{aligned}
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Whether the subject decides arm 1 or arm 2 depends on &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
action \sim
\begin{cases}
1 \text{ with probability } 1-\theta_t\\
2 \text{ with probability } \theta_t
\end{cases}
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Now I update the &lt;span class=&#34;math inline&#34;&gt;\(Q\)&lt;/span&gt; value that corresponds to the action taken for the next trial. Here, &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;, the learning rate, determines the extent to which the prediction error will play a role in updating an action’s value. This prediction error is quantified as the difference between the expected value of an action and the actual reward
on a given trial, &lt;span class=&#34;math inline&#34;&gt;\((R_t - Q_{t, a})\)&lt;/span&gt;.
The parameter &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; is bounded between &lt;span class=&#34;math inline&#34;&gt;\([0,1]\)&lt;/span&gt; to identify the model.&lt;a href=&#34;/2021/11/29/bayesian-h-reinforcement-learning/#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; Higher values of &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; imply greater sensitivity to the most recent choice outcome, that is the most recent reward.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
Q_{t + 1,action} = Q_{t, action} + \alpha \cdot (R_{t,action} - Q_{t, action})
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;For the action non taken, &lt;span class=&#34;math inline&#34;&gt;\(-action\)&lt;/span&gt;, the value of &lt;span class=&#34;math inline&#34;&gt;\(Q\)&lt;/span&gt; remains the same (in the first trial, that would be 0.5):&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
Q_{t + 1, -action} = Q_{t, -action}
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;For &lt;span class=&#34;math inline&#34;&gt;\(t &amp;gt;1\)&lt;/span&gt; things get more interesting: the influence of &lt;span class=&#34;math inline&#34;&gt;\(Q\)&lt;/span&gt; in the agent behavior is governed by an inverse temperature parameter
(&lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt;). Smaller values of &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; lead to a more exploratory behavior.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The idea will be that we obtain the probability, &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;, of a specific action in a given trial based on two factors: that action’s value, &lt;span class=&#34;math inline&#34;&gt;\(Q\)&lt;/span&gt; in that trial (learned through its reward history), and how influential this value will be in determining the action taken. Again, because there are only two choices we can use &lt;span class=&#34;math inline&#34;&gt;\(logit^-1\)&lt;/span&gt; function, if we had more choices we would use a &lt;span class=&#34;math inline&#34;&gt;\(softmax\)&lt;/span&gt; function.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
\theta_t = logit^{-1}(\beta_0 + \beta_1 \cdot (Q_{t,2} - Q_{t,1}))
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Again, and for every trial, whether the subject decides arm 1 or arm 2 depends on &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;:
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
action \sim
\begin{cases}
1 \text{ with probability } 1-\theta_t\\
2 \text{ with probability } \theta_t
\end{cases}
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;And &lt;span class=&#34;math inline&#34;&gt;\(Q\)&lt;/span&gt; gets updated for the next trial:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
\begin{aligned}
Q_{t + 1,a = action} &amp;amp;= Q_{t, action} + \alpha \cdot (R_{t,action} - Q_{t, action})\\
Q_{t + 1,a = -action} &amp;amp;= Q_{t, -action}
\end{aligned}
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The following R code implements these equations:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# True values
alpha &amp;lt;- .3 # learning rate
beta_0 &amp;lt;- 0.5 # bias to arm &amp;quot;b&amp;quot;
beta_1 &amp;lt;- 3 # inverse temperature
# Q vector with the first value set at 0.5
# True Q matrix, it has an extra row,
# since it keeps updating also for trial N_trial + 1
Q &amp;lt;- matrix(nrow = N_trials + 1, ncol = N_arms)
Q[1, ] &amp;lt;- rep(.5, N_arms) # initial values for all Q
action &amp;lt;- rep(NA, times = N_trials)
theta &amp;lt;- rep(NA, times = N_trials)
for (t in 1:N_trials) {
  # probability of choosing arm_2
  theta[t] &amp;lt;- plogis(beta_0 + beta_1 * (Q[t, 2] - Q[t, 1]))
  # What the synthethic subject would respond, 
  # with 1 indicating arm 1, and 2 indicating arm 2
  action[t] &amp;lt;- rbern(1, theta[t]) + 1
  Q[t + 1, action[t]] &amp;lt;- Q[t, action[t]] +
    alpha * (R[t, action[t]] - Q[t, action[t]])
  nonactions_t &amp;lt;- (1:N_arms)[-action[t]]
  Q[t + 1, nonactions_t] &amp;lt;- Q[t, nonactions_t]
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is what I’m generating trial by trial:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tibble(Q[1:N_trials, ], R, action)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 1,000 × 3
   `Q[1:N_trials, ]`[,1]    [,2] R[,&amp;quot;arm_1&amp;quot;] [,&amp;quot;arm_2&amp;quot;] action
                   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt;      &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;
 1                 0.5    0.5        -0.0560    -0.0996      2
 2                 0.5    0.320      -0.0791    -0.204       1
 3                 0.326  0.320       0.0768    -0.205       2
 4                 0.326  0.162       0.0839    -0.219       1
 5                 0.254  0.162       0.0968    -0.474       2
 6                 0.254 -0.0283      0.268     -0.369       2
 7                 0.254 -0.131       0.314     -0.344       1
 8                 0.272 -0.131       0.188     -0.103       2
 9                 0.272 -0.122       0.119     -0.0344      1
10                 0.226 -0.122       0.0746    -0.0791      1
# ℹ 990 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s see what would this subject choose in each trial.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;d_res &amp;lt;- tibble(
  arm = ifelse(action == 1, &amp;quot;arm_1&amp;quot;, &amp;quot;arm_2&amp;quot;),
  trial = 1:N_trials
) %&amp;gt;%
  left_join(d_exp)
ggplot(d_exp, aes(x = trial, y = reward, color = arm)) +
  geom_line() +
  geom_point(
    data = d_res,
    aes(x = trial, y = reward),
    color = &amp;quot;black&amp;quot;,
    shape = 1
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/2021/11/29/bayesian-h-reinforcement-learning//2021/11/29/bayesian-h-reinforcement-learning/index_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It’s interesting that there is a point (around the trial 650), where the second arm is so clearly better than the first arm, that it makes almost no sense to keep exploring.&lt;/p&gt;
&lt;p&gt;What will be the reward of this subject?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum(d_res$reward)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 1788&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What will be the maximum possible reward?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum(apply(R, MARGIN = 1, FUN = max))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 1846&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now let’s fit the model in Stan. A major difference with the R code is that I don’t keep track of all the Q values. I just keep track of one Q value for each arm, and I update them at the end of every trial. Also I store the difference between them in the vector &lt;code&gt;Q_diff&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  vector[N_arms] Q = [.5, .5]&amp;#39;;
  vector[N_trials] Q_diff;
  for(t in 1:N_trials){
    Q_diff[t] = Q[2] - Q[1];
    Q[action[t]] += alpha * (R[t, action[t]] - Q[action[t]]);
  }&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The complete code is shown below.&lt;/p&gt;
&lt;pre class=&#34;stan&#34;&gt;&lt;code&gt;data {
  int&amp;lt;lower=0&amp;gt; N_trials;
  array[N_trials] int action;
  int N_arms;
  matrix[N_trials,N_arms] R;
}
transformed data {
  array[N_trials] int response;
  for(n in 1:N_trials)
    response[n] = action[n] - 1;
}
parameters {
  real&amp;lt;lower = 0, upper = 1&amp;gt; alpha;
  real beta_0;
  real beta_1;
}
model {
  vector[N_arms] Q = [.5, .5]&amp;#39;;
  vector[N_trials] Q_diff;
  for(t in 1:N_trials){
    Q_diff[t] = Q[2] - Q[1];
    Q[action[t]] += alpha * (R[t, action[t]] - Q[action[t]]);
  }
  target += beta_lpdf(alpha | 1, 1);
  target += normal_lpdf(beta_0 | 2, 5);
  target += normal_lpdf(beta_1 | 0, 1);
  target += bernoulli_logit_lpmf(response | beta_0 + beta_1 * Q_diff);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I compile the model and fit it.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;m_ql &amp;lt;- cmdstan_model(&amp;quot;qlearning.stan&amp;quot;)
fit_ql &amp;lt;- m_ql$sample(
  data = list(
    action = action,
    N_trials = N_trials,
    N_arms = N_arms,
    R = R
  ),
  parallel_chains = 4
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The sampling is super fast, just a couple of seconds. In this case, compiling the model was slower than sampling.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit_ql$summary(c(&amp;quot;alpha&amp;quot;, &amp;quot;beta_0&amp;quot;, &amp;quot;beta_1&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 3 × 10
  variable  mean median     sd    mad    q5   q95  rhat ess_bulk ess_tail
  &amp;lt;chr&amp;gt;    &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;
1 alpha    0.266  0.261 0.0497 0.0469 0.196 0.355  1.00    2258.    1790.
2 beta_0   0.586  0.586 0.119  0.121  0.390 0.782  1.00    2906.    3191.
3 beta_1   2.98   2.97  0.246  0.243  2.59  3.39   1.00    2704.    2881.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The recovery of the parameters seems fine.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mcmc_recover_hist(fit_ql$draws(c(&amp;quot;alpha&amp;quot;, &amp;quot;beta_0&amp;quot;, &amp;quot;beta_1&amp;quot;)),
  true = c(alpha, beta_0, beta_1)
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/2021/11/29/bayesian-h-reinforcement-learning//2021/11/29/bayesian-h-reinforcement-learning/index_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Either my Stan model was correct or I made the same mistakes in the R code and in the Stan model.&lt;a href=&#34;/2021/11/29/bayesian-h-reinforcement-learning/#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;a-hierarchical-q-learning-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;A hierarchical Q-learning model&lt;/h2&gt;
&lt;p&gt;Now let’s try to implement this hierarchically. I have several subjects (&lt;code&gt;N_subj&lt;/code&gt;), they all have their own bias, their own learning rate, and their own inverse temperature, but these values are not that different from subject to subject (there is shrinkage). I also assume that somehow these three parameters are mildly correlated, people with more bias also have a stronger learning rate and inverse temperature.&lt;/p&gt;
&lt;p&gt;Here, I diverge a bit from the model of &lt;span class=&#34;citation&#34;&gt;van Geen and Gerraty (&lt;a href=&#34;/2021/11/29/bayesian-h-reinforcement-learning/#ref-vanGeenGerraty2021&#34;&gt;2021&lt;/a&gt;)&lt;/span&gt;. I just assume that all the individual differences are generated from the same multinormal distribution. This allows me to model the variance (or SD) of the parameters and to assume that they could be correlated (or not).&lt;/p&gt;
&lt;p&gt;This means that in the hierarchical model, rather than &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; I use &lt;span class=&#34;math inline&#34;&gt;\(\alpha + u_{i,1}\)&lt;/span&gt;, rather than &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\beta_0 + u_{i,2}\)&lt;/span&gt;, and rather than &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(beta_1 + u_{i,3}\)&lt;/span&gt;. Where &lt;span class=&#34;math inline&#34;&gt;\(u\)&lt;/span&gt; is a matrix generated as follows.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
    {\begin{pmatrix}
    u_{i,1} \\
    u_{i,2} \\
    u_{i,3}
    \end{pmatrix}}
   \sim {\mathcal {N}}
    \left(
   {\begin{pmatrix}
    0\\
    0\\
    0
   \end{pmatrix}}
,\boldsymbol{\Sigma_u} \right)
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;with &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; indexing subjects.&lt;/p&gt;
&lt;p&gt;I simulate all this below with the following parameters. More details about how this works can be found in &lt;a href=&#34;https://vasishth.github.io/bayescogsci/book/ch-complexstan.html&#34;&gt;chapter 11 of the book about Bayesian modeling&lt;/a&gt; that I’m writting with Daniel Schad and Shravan Vasishth.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;alpha &amp;lt;- .3 # learning rate
tau_u_alpha &amp;lt;- .1 # by-subj SD in the learning rate
beta_0 &amp;lt;- 0.5 # bias to arm &amp;quot;b&amp;quot;
tau_u_beta_0 &amp;lt;- .15  # by-subj SD in the bias
beta_1 &amp;lt;- 3 # inverse temperature
tau_u_beta_1 &amp;lt;- .3 # by-subj SD in the inverse temperature
N_adj &amp;lt;- 3 # number of &amp;quot;random effects&amp;quot; or by-subj adjustments
N_subj &amp;lt;- 20
tau_u &amp;lt;- c(tau_u_alpha, tau_u_beta_0, tau_u_beta_1)
rho_u &amp;lt;- .3 # corr between adjustments
Cor_u &amp;lt;- matrix(rep(rho_u, N_adj^2), nrow = N_adj)
diag(Cor_u) &amp;lt;- 1
Sigma_u &amp;lt;- diag(tau_u, N_adj, N_adj) %*%
  Cor_u %*%
  diag(tau_u, N_adj, N_adj)
u &amp;lt;- mvrnorm(n = N_subj, rep(0, N_adj), Sigma_u)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now I have that each subject has it’s own &lt;span class=&#34;math inline&#34;&gt;\(Q\)&lt;/span&gt;, call it &lt;code&gt;Q_i&lt;/code&gt;. Rather than using vectors as I did when there was only one subject, in the next piece of code I use lists of everything: each element of a list corresponds to one subject. The logic is analogous to the simulation of one subject, but with an extra loop.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Q_i &amp;lt;- matrix(.5, nrow = (N_trials + 1), ncol = N_arms)
Q &amp;lt;- rep(list(Q_i), N_subj)
theta &amp;lt;- rep(list(rep(NA, N_trials)), N_subj)
action &amp;lt;- rep(list(rep(NA, N_trials)), N_subj)
for (i in 1:N_subj) {
  for (t in 1:N_trials) {
    theta[[i]][t] &amp;lt;- plogis(beta_0 + u[i, 2] + 
                     (beta_1 + u[i, 3]) * (Q[[i]][t, 2] - Q[[i]][t, 1]))
    action[[i]][t] &amp;lt;- rbern(1, theta[[i]][t]) + 1
    alpha_i &amp;lt;- plogis(qlogis(alpha) + u[i, 1])
    Q[[i]][t + 1, action[[i]][t]] &amp;lt;- 
              Q[[i]][t, action[[i]][t]] +
              alpha_i * (R[t, action[[i]][t]] - Q[[i]][t, action[[i]][t]])
    nonactions_t &amp;lt;- (1:N_arms)[-action[[i]][t]]
    Q[[i]][t + 1, nonactions_t] &amp;lt;- Q[[i]][t, nonactions_t]
  }
}
# Convert the actions taken by subjects into a matrix
action_matrix &amp;lt;- matrix(as.integer(unlist(action)), ncol = N_subj)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I store everything in a list as Stan likes it:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data_h &amp;lt;- list(
  N_subj = N_subj,
  action = action_matrix,
  N_trials = N_trials,
  N_arms = N_arms,
  R = R
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The Stan code for the hierarchical model is a straightforward extension of the non-hierarchical version with one extra loop. A small difference here is that I calculate the log-likelihood inside the loop for each subject and then I add all those values together outside the loop for extra efficiency. Rather than using a multinormal distribution, I generate the &lt;code&gt;u&lt;/code&gt; values using a Cholesky factorization, details about this can also be found in &lt;a href=&#34;https://vasishth.github.io/bayescogsci/book/ch-complexstan.html&#34;&gt;the same chapter 11&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;stan&#34;&gt;&lt;code&gt;data {
  int&amp;lt;lower = 0&amp;gt; N_trials;
  int&amp;lt;lower = 0&amp;gt; N_subj;
  int&amp;lt;lower = 0&amp;gt; N_arms;
  matrix[N_trials, N_arms] R;
  array[N_trials, N_subj] int action;
}
transformed data{
  int N_adj = 3;
  array[N_trials, N_subj] int response;
    for(i in 1:N_subj)
      for(n in 1:N_trials)
        response[n,i] = action[n,i] - 1;
}
parameters {
  real&amp;lt;lower = 0, upper = 1&amp;gt; alpha;
  real beta_0;
  real beta_1;
  vector&amp;lt;lower = 0&amp;gt;[N_adj] tau_u;
  matrix[N_adj, N_subj] z_u;
  cholesky_factor_corr[N_adj] L_u;
}
transformed parameters{
  matrix[N_subj, N_adj] u = (diag_pre_multiply(tau_u, L_u) * z_u)&amp;#39;;
}
model {
  matrix[N_arms, N_subj] Q = rep_matrix(.5,N_arms, N_subj);
  matrix[N_trials, N_subj] Q_diff;
  vector[N_subj] log_lik;
  for(i in 1:N_subj){
    real alpha_i = inv_logit(logit(alpha) + u[i, 1]);
    for(t in 1:N_trials){
      Q_diff[t, i] = Q[2, i] - Q[1, i];
      Q[action[t, i], i] += alpha_i * 
                        (R[t, action[t, i]] - Q[action[t, i], i]);
    }
    log_lik[i] = bernoulli_logit_lpmf(response[ ,i] | 
              beta_0 + u[i, 2] + (beta_1 + u[i, 3]) * Q_diff[,i]);
  }
  target += beta_lpdf(alpha | 1, 1);
  target += normal_lpdf(beta_0 | 2, 5);
  target += normal_lpdf(beta_1 | 0, 1);
  target += normal_lpdf(tau_u | .1, .5)
            - N_adj * normal_lccdf(0 | .1, .5);
  target += lkj_corr_cholesky_lpdf(L_u | 2);
  target += std_normal_lpdf(to_vector(z_u));
  target += sum(log_lik);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I fit the model below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;m_ql_h &amp;lt;- cmdstan_model(&amp;quot;qlearning_h.stan&amp;quot;)
fit_ql_h &amp;lt;- m_ql_h$sample(
  data = data_h,
  parallel_chains = 4
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It took 7 minutes in my computer.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit_ql_h&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt; variable     mean   median   sd  mad       q5      q95 rhat ess_bulk ess_tail
 lp__     -4568.72 -4568.26 7.88 7.83 -4582.50 -4556.30 1.00      864     1691
 alpha        0.31     0.31 0.02 0.01     0.29     0.34 1.00     2777     2099
 beta_0       0.60     0.60 0.06 0.06     0.51     0.70 1.00     2325     2887
 beta_1       3.02     3.02 0.10 0.09     2.87     3.18 1.00     2635     2415
 tau_u[1]     0.13     0.12 0.09 0.10     0.01     0.30 1.00     1548     2187
 tau_u[2]     0.23     0.22 0.05 0.05     0.15     0.33 1.00     1713     2583
 tau_u[3]     0.36     0.35 0.09 0.09     0.22     0.52 1.00     1811     2552
 z_u[1,1]     0.44     0.47 0.91 0.88    -1.12     1.89 1.00     1517     2751
 z_u[2,1]     1.06     1.06 0.66 0.65    -0.01     2.13 1.00     3463     3017
 z_u[3,1]     0.58     0.58 0.75 0.74    -0.66     1.80 1.00     4226     2838

 # showing 10 of 136 rows (change via &amp;#39;max_rows&amp;#39; argument or &amp;#39;cmdstanr_max_rows&amp;#39; option)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s check if I could recover the parameters.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mcmc_recover_hist(fit_ql_h$draws(
  c(&amp;quot;alpha&amp;quot;, &amp;quot;beta_0&amp;quot;, &amp;quot;beta_1&amp;quot;, &amp;quot;tau_u&amp;quot;)
),
true = c(alpha, beta_0, beta_1, tau_u)
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/2021/11/29/bayesian-h-reinforcement-learning//2021/11/29/bayesian-h-reinforcement-learning/index_files/figure-html/unnamed-chunk-22-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It seems relatively good, but I should probably check it with simulation based calibration &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;/2021/11/29/bayesian-h-reinforcement-learning/#ref-taltsValidatingBayesianInference2018&#34;&gt;Talts et al. 2018&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;where-do-we-go-from-here&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Where do we go from here?&lt;/h2&gt;
&lt;p&gt;One could compare the fit and the predictions of this flavor of Q-learning with another flavor, or with another RL model. The idea would be that each model reflects different theoretical assumptions. It’s also possible to use posterior predictive checks to investigate where the a specific RL model fails and how it should be modified. Finally, RL could be part of a larger model, as in the case of &lt;span class=&#34;citation&#34;&gt;Pedersen, Frank, and Biele (&lt;a href=&#34;/2021/11/29/bayesian-h-reinforcement-learning/#ref-pedersen2017drift&#34;&gt;2017&lt;/a&gt;)&lt;/span&gt;, who combined drift diffusion with RL.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;session-info&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Session info &lt;a href=&#34;/2021/11/29/bayesian-h-reinforcement-learning/#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sessionInfo()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;R version 4.3.2 (2023-10-31)
Platform: x86_64-pc-linux-gnu (64-bit)
Running under: Ubuntu 22.04.3 LTS

Matrix products: default
BLAS:   /usr/lib/x86_64-linux-gnu/blas/libblas.so.3.10.0 
LAPACK: /usr/lib/x86_64-linux-gnu/lapack/liblapack.so.3.10.0

locale:
 [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C               LC_TIME=nl_NL.UTF-8        LC_COLLATE=en_US.UTF-8     LC_MONETARY=nl_NL.UTF-8   
 [6] LC_MESSAGES=en_US.UTF-8    LC_PAPER=nl_NL.UTF-8       LC_NAME=C                  LC_ADDRESS=C               LC_TELEPHONE=C            
[11] LC_MEASUREMENT=nl_NL.UTF-8 LC_IDENTIFICATION=C       

time zone: Europe/Amsterdam
tzcode source: system (glibc)

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base     

other attached packages:
[1] bayesplot_1.10.0   cmdstanr_0.7.1     MASS_7.3-60        extraDistr_1.9.1   ggplot2_3.4.4.9000 purrr_1.0.2        tidyr_1.3.0        dplyr_1.1.3       

loaded via a namespace (and not attached):
 [1] gtable_0.3.4         tensorA_0.36.2.1     xfun_0.41            bslib_0.5.1          processx_3.8.3       papaja_0.1.2         vctrs_0.6.5         
 [8] tools_4.3.2          ps_1.7.6             generics_0.1.3       tibble_3.2.1         fansi_1.0.6          highr_0.10           RefManageR_1.4.0    
[15] pkgconfig_2.0.3      data.table_1.14.10   checkmate_2.3.1      distributional_0.3.2 assertthat_0.2.1     lifecycle_1.0.4      rcorpora_2.0.0      
[22] compiler_4.3.2       farver_2.1.1         stringr_1.5.0        munsell_0.5.0        tinylabels_0.2.4     htmltools_0.5.7      sass_0.4.7          
[29] yaml_2.3.7           pillar_1.9.0         crayon_1.5.2         jquerylib_0.1.4      cachem_1.0.8         abind_1.4-5          posterior_1.5.0     
[36] tidyselect_1.2.0     digest_0.6.34        stringi_1.8.1        reshape2_1.4.4       bookdown_0.36        labeling_0.4.3       bibtex_0.5.1        
[43] fastmap_1.1.1        grid_4.3.2           colorspace_2.1-0     cli_3.6.2            magrittr_2.0.3       emo_0.0.0.9000       utf8_1.2.4          
[50] withr_3.0.0          scales_1.3.0         backports_1.4.1      lubridate_1.9.3      timechange_0.2.0     httr_1.4.7           rmarkdown_2.25      
[57] matrixStats_1.2.0    blogdown_1.18        evaluate_0.23        knitr_1.45           rlang_1.1.3          Rcpp_1.0.12          glue_1.7.0          
[64] xml2_1.3.5           rstudioapi_0.15.0    jsonlite_1.8.8       R6_2.5.1             plyr_1.8.9          &lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2 unnumbered&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34;&gt;
&lt;div id=&#34;ref-R-htmltools&#34; class=&#34;csl-entry&#34;&gt;
Cheng, Joe, Carson Sievert, Barret Schloerke, Winston Chang, Yihui Xie, and Jeff Allen. 2023. &lt;em&gt;Htmltools: Tools for HTML&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=htmltools&#34;&gt;https://CRAN.R-project.org/package=htmltools&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-R-cmdstanr&#34; class=&#34;csl-entry&#34;&gt;
Gabry, Jonah, and Rok Češnovar. 2021. &lt;em&gt;Cmdstanr: R Interface to ’CmdStan’&lt;/em&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-R-bayesplot&#34; class=&#34;csl-entry&#34;&gt;
Gabry, Jonah, Daniel Simpson, Aki Vehtari, Michael Betancourt, and Andrew Gelman. 2019. &lt;span&gt;“Visualization in Bayesian Workflow.”&lt;/span&gt; &lt;em&gt;J. R. Stat. Soc. A&lt;/em&gt; 182: 389–402. &lt;a href=&#34;https://doi.org/10.1111/rssa.12378&#34;&gt;https://doi.org/10.1111/rssa.12378&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-R-purrr&#34; class=&#34;csl-entry&#34;&gt;
Henry, Lionel, and Hadley Wickham. 2020. &lt;em&gt;Purrr: Functional Programming Tools&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=purrr&#34;&gt;https://CRAN.R-project.org/package=purrr&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-R-rcorpora&#34; class=&#34;csl-entry&#34;&gt;
Kazemi, Darius, Cole Willsea, Serin Delaunay, Karl Swedberg, Matthew Rothenberg, Greg Kennedy, Nathaniel Mitchell, et al. 2018. &lt;em&gt;Rcorpora: A Collection of Small Text Corpora of Interesting Data&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=rcorpora&#34;&gt;https://CRAN.R-project.org/package=rcorpora&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-R-RefManageR&#34; class=&#34;csl-entry&#34;&gt;
McLean, Mathew William. 2017. &lt;span&gt;“RefManageR: Import and Manage BibTeX and BibLaTeX References in r.”&lt;/span&gt; &lt;em&gt;The Journal of Open Source Software&lt;/em&gt;. &lt;a href=&#34;https://doi.org/10.21105/joss.00338&#34;&gt;https://doi.org/10.21105/joss.00338&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-niv2009reinforcement&#34; class=&#34;csl-entry&#34;&gt;
Niv, Yael. 2009. &lt;span&gt;“Reinforcement Learning in the Brain.”&lt;/span&gt; &lt;em&gt;Journal of Mathematical Psychology&lt;/em&gt; 53 (3): 139–54.
&lt;/div&gt;
&lt;div id=&#34;ref-pedersen2017drift&#34; class=&#34;csl-entry&#34;&gt;
Pedersen, Mads Lund, Michael J Frank, and Guido Biele. 2017. &lt;span&gt;“The Drift Diffusion Model as the Choice Rule in Reinforcement Learning.”&lt;/span&gt; &lt;em&gt;Psychonomic Bulletin &amp;amp; Review&lt;/em&gt; 24 (4): 1234–51.
&lt;/div&gt;
&lt;div id=&#34;ref-R-base&#34; class=&#34;csl-entry&#34;&gt;
R Core Team. 2021. &lt;em&gt;R: A Language and Environment for Statistical Computing&lt;/em&gt;. Vienna, Austria: R Foundation for Statistical Computing. &lt;a href=&#34;https://www.R-project.org/&#34;&gt;https://www.R-project.org/&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-sutton2018reinforcement&#34; class=&#34;csl-entry&#34;&gt;
Sutton, Richard S, and Andrew G Barto. 2018. &lt;em&gt;Reinforcement Learning: An Introduction&lt;/em&gt;. MIT press.
&lt;/div&gt;
&lt;div id=&#34;ref-taltsValidatingBayesianInference2018&#34; class=&#34;csl-entry&#34;&gt;
Talts, Sean, Michael Betancourt, Daniel Simpson, Aki Vehtari, and Andrew Gelman. 2018. &lt;span&gt;“Validating &lt;span&gt;Bayesian Inference Algorithms&lt;/span&gt; with &lt;span&gt;Simulation&lt;/span&gt;-&lt;span&gt;Based Calibration&lt;/span&gt;,”&lt;/span&gt; April. &lt;a href=&#34;http://arxiv.org/abs/1804.06788&#34;&gt;http://arxiv.org/abs/1804.06788&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-vanGeenGerraty2021&#34; class=&#34;csl-entry&#34;&gt;
van Geen, Camilla, and Raphael T. Gerraty. 2021. &lt;span&gt;“Hierarchical &lt;span&gt;Bayesian&lt;/span&gt; Models of Reinforcement Learning: &lt;span&gt;Introduction&lt;/span&gt; and Comparison to Alternative Methods.”&lt;/span&gt; &lt;em&gt;Journal of Mathematical Psychology&lt;/em&gt; 105: 102602. https://doi.org/&lt;a href=&#34;https://doi.org/10.1016/j.jmp.2021.102602&#34;&gt;https://doi.org/10.1016/j.jmp.2021.102602&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-R-ggplot2&#34; class=&#34;csl-entry&#34;&gt;
Wickham, Hadley. 2016. &lt;em&gt;Ggplot2: Elegant Graphics for Data Analysis&lt;/em&gt;. Springer-Verlag New York. &lt;a href=&#34;https://ggplot2.tidyverse.org&#34;&gt;https://ggplot2.tidyverse.org&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-R-stringr&#34; class=&#34;csl-entry&#34;&gt;
———. 2022. &lt;em&gt;Stringr: Simple, Consistent Wrappers for Common String Operations&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=stringr&#34;&gt;https://CRAN.R-project.org/package=stringr&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-R-dplyr&#34; class=&#34;csl-entry&#34;&gt;
Wickham, Hadley, Romain François, Lionel Henry, and Kirill Müller. 2021. &lt;em&gt;Dplyr: A Grammar of Data Manipulation&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=dplyr&#34;&gt;https://CRAN.R-project.org/package=dplyr&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-R-extraDistr&#34; class=&#34;csl-entry&#34;&gt;
Wolodzko, Tymoteusz. 2020. &lt;em&gt;extraDistr: Additional Univariate and Multivariate Distributions&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=extraDistr&#34;&gt;https://CRAN.R-project.org/package=extraDistr&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-R-knitr&#34; class=&#34;csl-entry&#34;&gt;
Xie, Yihui. 2015. &lt;em&gt;Dynamic Documents with &lt;span&gt;R&lt;/span&gt; and Knitr&lt;/em&gt;. 2nd ed. Boca Raton, Florida: Chapman; Hall/CRC. &lt;a href=&#34;https://yihui.org/knitr/&#34;&gt;https://yihui.org/knitr/&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-R-rmarkdown_a&#34; class=&#34;csl-entry&#34;&gt;
Xie, Yihui, J. J. Allaire, and Garrett Grolemund. 2018. &lt;em&gt;R Markdown: The Definitive Guide&lt;/em&gt;. Boca Raton, Florida: Chapman; Hall/CRC. &lt;a href=&#34;https://bookdown.org/yihui/rmarkdown&#34;&gt;https://bookdown.org/yihui/rmarkdown&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-R-rmarkdown_b&#34; class=&#34;csl-entry&#34;&gt;
Xie, Yihui, Christophe Dervieux, and Emily Riederer. 2020. &lt;em&gt;R Markdown Cookbook&lt;/em&gt;. Boca Raton, Florida: Chapman; Hall/CRC. &lt;a href=&#34;https://bookdown.org/yihui/rmarkdown-cookbook&#34;&gt;https://bookdown.org/yihui/rmarkdown-cookbook&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-ZhangEtAl2020&#34; class=&#34;csl-entry&#34;&gt;
Zhang, Lei, Lukas Lengersdorff, Nace Mikus, Jan Gläscher, and Claus Lamm. 2020. &lt;span&gt;“&lt;span class=&#34;nocase&#34;&gt;Using reinforcement learning models in social neuroscience: frameworks, pitfalls and suggestions of best practices&lt;/span&gt;.”&lt;/span&gt; &lt;em&gt;Social Cognitive and Affective Neuroscience&lt;/em&gt; 15 (6): 695–707. &lt;a href=&#34;https://doi.org/10.1093/scan/nsaa089&#34;&gt;https://doi.org/10.1093/scan/nsaa089&lt;/a&gt;.
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes footnotes-end-of-document&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;I actually forgot to bound the parameter at one point, and I could verify that the chains of the MCMC sampler got stuck at different modes.&lt;a href=&#34;/2021/11/29/bayesian-h-reinforcement-learning/#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;A &lt;a href=&#34;https://xkcd.com/2303/&#34;&gt;Type-V Error&lt;/a&gt;.&lt;a href=&#34;/2021/11/29/bayesian-h-reinforcement-learning/#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;I used R &lt;span class=&#34;citation&#34;&gt;(Version 4.3.2; &lt;a href=&#34;/2021/11/29/bayesian-h-reinforcement-learning/#ref-R-base&#34;&gt;R Core Team 2021&lt;/a&gt;)&lt;/span&gt; and the R-packages &lt;em&gt;bayesplot&lt;/em&gt; &lt;span class=&#34;citation&#34;&gt;(Version 1.10.0; &lt;a href=&#34;/2021/11/29/bayesian-h-reinforcement-learning/#ref-R-bayesplot&#34;&gt;Gabry et al. 2019&lt;/a&gt;)&lt;/span&gt;, &lt;em&gt;cmdstanr&lt;/em&gt; &lt;span class=&#34;citation&#34;&gt;(Version 0.7.1; &lt;a href=&#34;/2021/11/29/bayesian-h-reinforcement-learning/#ref-R-cmdstanr&#34;&gt;Gabry and Češnovar 2021&lt;/a&gt;)&lt;/span&gt;, &lt;em&gt;dplyr&lt;/em&gt; &lt;span class=&#34;citation&#34;&gt;(Version 1.1.3; &lt;a href=&#34;/2021/11/29/bayesian-h-reinforcement-learning/#ref-R-dplyr&#34;&gt;Wickham et al. 2021&lt;/a&gt;)&lt;/span&gt;, &lt;em&gt;extraDistr&lt;/em&gt; &lt;span class=&#34;citation&#34;&gt;(Version 1.9.1; &lt;a href=&#34;/2021/11/29/bayesian-h-reinforcement-learning/#ref-R-extraDistr&#34;&gt;Wolodzko 2020&lt;/a&gt;)&lt;/span&gt;, &lt;em&gt;ggplot2&lt;/em&gt; &lt;span class=&#34;citation&#34;&gt;(Version 3.4.4.9000; &lt;a href=&#34;/2021/11/29/bayesian-h-reinforcement-learning/#ref-R-ggplot2&#34;&gt;Wickham 2016&lt;/a&gt;)&lt;/span&gt;, &lt;em&gt;htmltools&lt;/em&gt; &lt;span class=&#34;citation&#34;&gt;(Version 0.5.7; &lt;a href=&#34;/2021/11/29/bayesian-h-reinforcement-learning/#ref-R-htmltools&#34;&gt;Cheng et al. 2023&lt;/a&gt;)&lt;/span&gt;, &lt;em&gt;knitr&lt;/em&gt; &lt;span class=&#34;citation&#34;&gt;(Version 1.45; &lt;a href=&#34;/2021/11/29/bayesian-h-reinforcement-learning/#ref-R-knitr&#34;&gt;Xie 2015&lt;/a&gt;)&lt;/span&gt;, &lt;em&gt;purrr&lt;/em&gt; &lt;span class=&#34;citation&#34;&gt;(Version 1.0.2; &lt;a href=&#34;/2021/11/29/bayesian-h-reinforcement-learning/#ref-R-purrr&#34;&gt;Henry and Wickham 2020&lt;/a&gt;)&lt;/span&gt;, &lt;em&gt;rcorpora&lt;/em&gt; &lt;span class=&#34;citation&#34;&gt;(Version 2.0.0; &lt;a href=&#34;/2021/11/29/bayesian-h-reinforcement-learning/#ref-R-rcorpora&#34;&gt;Kazemi et al. 2018&lt;/a&gt;)&lt;/span&gt;, &lt;em&gt;RefManageR&lt;/em&gt; &lt;span class=&#34;citation&#34;&gt;(Version 1.4.0; &lt;a href=&#34;/2021/11/29/bayesian-h-reinforcement-learning/#ref-R-RefManageR&#34;&gt;McLean 2017&lt;/a&gt;)&lt;/span&gt;, &lt;em&gt;rmarkdown&lt;/em&gt; &lt;span class=&#34;citation&#34;&gt;(Version 2.25; &lt;a href=&#34;/2021/11/29/bayesian-h-reinforcement-learning/#ref-R-rmarkdown_a&#34;&gt;Xie, Allaire, and Grolemund 2018&lt;/a&gt;; &lt;a href=&#34;/2021/11/29/bayesian-h-reinforcement-learning/#ref-R-rmarkdown_b&#34;&gt;Xie, Dervieux, and Riederer 2020&lt;/a&gt;)&lt;/span&gt;, and &lt;em&gt;stringr&lt;/em&gt; &lt;span class=&#34;citation&#34;&gt;(Version 1.5.0; &lt;a href=&#34;/2021/11/29/bayesian-h-reinforcement-learning/#ref-R-stringr&#34;&gt;Wickham 2022&lt;/a&gt;)&lt;/span&gt; to generate this document.&lt;a href=&#34;/2021/11/29/bayesian-h-reinforcement-learning/#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>A simple way to model rankings with Stan</title>
      <link>/2021/03/21/a-simple-way-to-model-rankings-with-stan/</link>
      <pubDate>Sun, 21 Mar 2021 00:00:00 +0000</pubDate>
      
      <guid>/2021/03/21/a-simple-way-to-model-rankings-with-stan/</guid><description>


&lt;p&gt;&lt;strong&gt;Update Feb 1st, 2024: I updated the Stan syntax.&lt;/strong&gt;&lt;/p&gt;
&lt;div id=&#34;the-initial-problem&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The initial problem&lt;/h2&gt;
&lt;p&gt;I wrote what I thought it was the generative process for some modeling work, and it looked too common to not have a name, so I started to ask around in Twitter:&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Prob q: Is there a name for a categorical distribution that whenever you get an outcome you remove it from the set? &lt;br&gt;Ex. 1st draw: y ~ cat(p1,p2,p3,p4) ; y = 2&lt;br&gt;2nd draw y ~ cat(p1/k,p3/k,p4/k) (k = p1+p3+p4); y =1&lt;br&gt;3rd, y ~ categorical(p3/(p3+p4),p4/(p3+p4)); y =3&lt;br&gt;4th, it&amp;#39;s y =4&lt;/p&gt;&amp;mdash; @bruno_nicenboim@fediscience.org (@bruno_nicenboim) &lt;a href=&#34;https://twitter.com/bruno_nicenboim/status/1369217174699732995?ref_src=twsrc%5Etfw&#34;&gt;March 9, 2021&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;


&lt;p&gt;One useful clue was this one:&lt;/p&gt;

&lt;p&gt;It turns out that the distribution that I was describing is in general used for rankings or ordered data and it is called an &lt;a href=&#34;https://en.wikipedia.org/wiki/Discrete_choice#exploded_logit&#34;&gt;&lt;em&gt;exploded logit distribution&lt;/em&gt;&lt;/a&gt;.&lt;a href=&#34;/2021/03/21/a-simple-way-to-model-rankings-with-stan/#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/2021/03/21/a-simple-way-to-model-rankings-with-stan/https://media.giphy.com/media/3osxYCsLd9qgsgqpwI/giphy.gif&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In this post, I’ll show how this model can be fit in the probabilistic programming language &lt;a href=&#34;https://mc-stan.org/&#34;&gt;Stan&lt;/a&gt;, and how it can be used to describe the underlying order of ranking data.&lt;/p&gt;
&lt;p&gt;I’m going to load some R packages that will be useful throughout this post.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr) # Data manipulation
library(purrr) # List manipulation
library(ggplot2) # Nice plots
library(extraDistr) # More distributions
library(rcorpora) # Get random words
library(cmdstanr) # Lightweight Stan interface
library(bayesplot) # Nice Bayesian plots&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;ranking-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Ranking data&lt;/h2&gt;
&lt;p&gt;Ranking data appear when we care about the &lt;em&gt;underlying&lt;/em&gt; order that certain elements have. We might want to know which are the best horses after looking at several races &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;/2021/03/21/a-simple-way-to-model-rankings-with-stan/#ref-gakisetal2018&#34;&gt;Gakis et al. 2018&lt;/a&gt;)&lt;/span&gt;, which is the best candidate for a job after a series of interviewers talked to several candidates. More in line with cognitive science, we might want to know which are the best possible completions for a sentence or the best exemplars of a category.&lt;/p&gt;
&lt;p&gt;One way to get a ranking of exemplars of a category, for example, is to present them to participants and ask them to order all (or a subset) of them &lt;span class=&#34;citation&#34;&gt;(see &lt;a href=&#34;/2021/03/21/a-simple-way-to-model-rankings-with-stan/#ref-Barsalou1985&#34;&gt;Barsalou 1985&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;div id=&#34;a-ranking-simulation-using-pizza-toppings&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;A ranking simulation using pizza toppings&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;/2021/03/21/a-simple-way-to-model-rankings-with-stan/https://media.giphy.com/media/3oEjHZhG9COPG6XjzO/giphy.gif&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Let’s consider the following 25 pizza toppings:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;toppings &amp;lt;- corpora(&amp;quot;foods/pizzaToppings&amp;quot;)$pizzaToppings
N_toppings &amp;lt;- length(toppings)
toppings&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt; [1] &amp;quot;anchovies&amp;quot;        &amp;quot;artichoke&amp;quot;       
 [3] &amp;quot;bacon&amp;quot;            &amp;quot;breakfast bacon&amp;quot; 
 [5] &amp;quot;Canadian bacon&amp;quot;   &amp;quot;cheese&amp;quot;          
 [7] &amp;quot;chicken&amp;quot;          &amp;quot;chili peppers&amp;quot;   
 [9] &amp;quot;feta&amp;quot;             &amp;quot;garlic&amp;quot;          
[11] &amp;quot;green peppers&amp;quot;    &amp;quot;grilled onions&amp;quot;  
[13] &amp;quot;ground beef&amp;quot;      &amp;quot;ham&amp;quot;             
[15] &amp;quot;hot sauce&amp;quot;        &amp;quot;meatballs&amp;quot;       
[17] &amp;quot;mushrooms&amp;quot;        &amp;quot;olives&amp;quot;          
[19] &amp;quot;onions&amp;quot;           &amp;quot;pepperoni&amp;quot;       
[21] &amp;quot;pineapple&amp;quot;        &amp;quot;sausage&amp;quot;         
[23] &amp;quot;spinach&amp;quot;          &amp;quot;sun-dried tomato&amp;quot;
[25] &amp;quot;tomatoes&amp;quot;        &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s say that we want to know the underlying order of pizza toppings. For the modeling, I’m going to assume that the toppings are ordered according to an underlying value, which also represents how likely it is for each topping to be &lt;em&gt;the&lt;/em&gt; exemplar of their category.&lt;/p&gt;
&lt;p&gt;To get a known ground truth for the ranking, I’m going to simulate an order of pizza toppings. I assign probabilities that sum up to one to the toppings by drawing a random sample from a &lt;a href=&#34;https://en.wikipedia.org/wiki/Dirichlet_distribution&#34;&gt;Dirichlet distribution&lt;/a&gt;. The Dirichlet distribution is the generalization of the Beta distribution. It has a concentration parameter, usually &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{\alpha}\)&lt;/span&gt;, which is a vector as long as the probabilities we are sampling (25 here). When the vector is full of ones, the distribution is uniform: All probabilities are equally likely, so on average each one is &lt;span class=&#34;math inline&#34;&gt;\(\frac{1}{vector \text{  } length}\)&lt;/span&gt; (&lt;span class=&#34;math inline&#34;&gt;\(\frac{1}{25}\)&lt;/span&gt; here). By setting all the concentration parameters below one (namely &lt;span class=&#34;math inline&#34;&gt;\(.2\)&lt;/span&gt;), I’m enforcing sparsity in the random values that I’m generating, that is, many probability values close to zero.&lt;/p&gt;
&lt;p&gt;These is the true order that I’m assuming here:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# all elements of the vector are .5
alpha &amp;lt;- rep(.2, N_toppings)
# Generate one draw from a Dirichlet distribution
P_toppings &amp;lt;- c(rdirichlet(1, alpha)) %&amp;gt;%
  # Add names
  setNames(toppings) %&amp;gt;%
  # Sort from the best exemplar
  sort(decreasing = TRUE)
P_toppings %&amp;gt;%
  round(3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;       artichoke  breakfast bacon           onions 
           0.387            0.140            0.124 
     ground beef        hot sauce          spinach 
           0.091            0.090            0.072 
sun-dried tomato        anchovies             feta 
           0.036            0.016            0.010 
   green peppers            bacon   Canadian bacon 
           0.006            0.006            0.006 
   chili peppers        pepperoni           garlic 
           0.005            0.005            0.003 
        tomatoes              ham        mushrooms 
           0.002            0.001            0.001 
       meatballs          sausage           olives 
           0.000            0.000            0.000 
  grilled onions          chicken           cheese 
           0.000            0.000            0.000 
       pineapple 
           0.000 &lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Given these values, if I were to ask a participant “What’s the most appropriate topping for a pizza?” I would assume that 38.74 percent of the time, I would get &lt;em&gt;artichoke&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Essentially, we expect something like this to be happening.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34; id=&#34;eq:probicat&#34;&gt;\[\begin{equation}
response \sim Categorical(\Theta_{toppings})
\tag{1}
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;With &lt;span class=&#34;math inline&#34;&gt;\(\Theta_{toppings}\)&lt;/span&gt; representing the different probabilities for each topping. The probability mass function of the categorical distribution is absurdly simple: It’s just the probability of the outcome.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34; id=&#34;eq:probi&#34;&gt;\[\begin{equation}
p(x = i) = \Theta_i
\tag{2}
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(i = \{\)&lt;/span&gt;artichoke, breakfast bacon, onions, ground beef, hot sauce, spinach, sun-dried tomato, anchovies, feta, green peppers, bacon, Canadian bacon, chili peppers, pepperoni, garlic, tomatoes, ham, mushrooms, meatballs, sausage, olives, grilled onions, chicken, cheese, pineapple&lt;span class=&#34;math inline&#34;&gt;\(\}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;We can simulate this with 100 participants as follows:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;response &amp;lt;- rcat(100, P_toppings, names(P_toppings))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And this should match approximately &lt;code&gt;P_toppings&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table(response)/100&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;response
       artichoke  breakfast bacon           onions 
            0.31             0.12             0.17 
     ground beef        hot sauce          spinach 
            0.11             0.14             0.04 
sun-dried tomato        anchovies             feta 
            0.07             0.02             0.02 
   green peppers            bacon   Canadian bacon 
            0.00             0.00             0.00 
   chili peppers        pepperoni           garlic 
            0.00             0.00             0.00 
        tomatoes              ham        mushrooms 
            0.00             0.00             0.00 
       meatballs          sausage           olives 
            0.00             0.00             0.00 
  grilled onions          chicken           cheese 
            0.00             0.00             0.00 
       pineapple 
            0.00 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;It seems that by only asking participants to give the best topping we could already deduce the underlying order…&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;True, but one motivation for considering ranking data is the amount of information that we gather with a list due to their combinatorial nature. If we ask participants to rank &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; items, an answer consists in making a single selection out of &lt;span class=&#34;math inline&#34;&gt;\(n!\)&lt;/span&gt; possibilities. Ordering 7 pizza toppings, for example, constitutes making a single selection out of 5040 possibilities!&lt;/p&gt;
&lt;p&gt;If we don’t relay on lists and there is sparcity, it requires a large number of participants until we get answers of low probability. (For example, we’ll need a very large number of participants until we hear something else but &lt;em&gt;hammer&lt;/em&gt; as an exemplar of tools).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Now, what happens if we ask about the second most appropriate topping for a pizza?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Now we need to exclude the first topping that was given, and draw another sample from a categorical distribution. (We don’t allow the participant to repeat toppings, that is, to say that the best topping is pineapple and the second best is also pineapple). This means that now the probability of the topping already given is zero, and that we need to normalize our original probability values by dividing them by the new total probability (which will be lower than 1).&lt;/p&gt;
&lt;p&gt;Here, the probability of getting the element &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; (where &lt;span class=&#34;math inline&#34;&gt;\(j \neq i\)&lt;/span&gt;) is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34; id=&#34;eq:probj&#34;&gt;\[\begin{equation}
p(x = j) = \frac{\Theta_j}{\sum \Theta_{[-i]}}
\tag{3}
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\Theta_{[-i]}\)&lt;/span&gt; represents the probabilities of all the outcomes except of &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;, which was the first one.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We can go on with the third best topping, where we need to normalize the remaining probabilities by dividing by the new sum of probabilities.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34; id=&#34;eq:probk&#34;&gt;\[\begin{equation}
p(x = k) = \frac{\Theta_k}{\sum \Theta_{[-i,-j]}}
\tag{4}
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We can do this until we get to the last element, which will be drawn with probability 1.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;And this is the exploded logit distribution.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This process can be simulated in R as follows:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rexploded &amp;lt;-  function(n, ranked = 3, prob, labels = NULL){
  #run n times
  lapply(1:n, function(nn){
    res &amp;lt;- rep(NA,ranked)
    if(!is.null(labels)){
      res &amp;lt;- factor(res, labels)
    } else {
      # if there are no labels, just 1,2,3,...
      labels &amp;lt;- seq_along(prob)
    }
  for(i in 1:ranked){
    # normalize the probability so that it sums to 1
    prob &amp;lt;- prob/sum(prob)
    res[i] &amp;lt;- rcat(1, prob = prob, labels = labels)
    # remove the choice from the set:
    prob[res[i]] &amp;lt;- 0
  }
    res
  })
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we would like to simulate 50 subjects creating a ranking of the best 7 toppings, we would do the following:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;res &amp;lt;- rexploded(n = 50,
                 ranked = 7,
                 prob = P_toppings,
                 labels = names(P_toppings))
# subject 1:
res[[1]]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] hot sauce        artichoke        breakfast bacon 
[4] onions           ground beef      sun-dried tomato
[7] spinach         
25 Levels: artichoke breakfast bacon onions ... pineapple&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/2021/03/21/a-simple-way-to-model-rankings-with-stan//2021/03/21/a-simple-way-to-model-rankings-with-stan/index_files/figure-html/subjects.gif&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We have simulated ranking data of pizza toppings, can we recover the original probability values and “discover” the underlying order?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;fitting-the-exploded-logistic-distribution-in-stan&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Fitting the exploded logistic distribution in Stan&lt;/h2&gt;
&lt;p&gt;To fit the model in Stan, I’m going to create a custom probability mass function that takes an array of integers, &lt;code&gt;x&lt;/code&gt;, which represents a set of rankings, and a vector of probability values, &lt;code&gt;theta&lt;/code&gt;, that sums up to one.&lt;/p&gt;
&lt;p&gt;The logic of this function is that the probability mass function of a ranking &lt;span class=&#34;math inline&#34;&gt;\(\{i,j,k, \ldots, N \}\)&lt;/span&gt; can be written as a product of normalized categorical distributions (where the first one is just divided by 1).&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34; id=&#34;eq:probijk&#34;&gt;\[\begin{equation}
p(x = \{i,j,k,\ldots\}) = \frac{\Theta_i}{\sum \Theta} \cdot \frac{\Theta_j}{\sum \Theta_{[-i]}} \cdot \frac{\Theta_k}{\sum \Theta_{[-i, -j]}} \ldots
\tag{5}
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;For Stan, we need the log-PDF. In log-space, products become sums, and divisions differences, and the log of &lt;span class=&#34;math inline&#34;&gt;\(\sum \Theta\)&lt;/span&gt; will be zero:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34; id=&#34;eq:probijk&#34;&gt;\[\begin{equation}
\begin{aligned}
log(p(x = \{i,j,k,\ldots\})) =&amp;amp; \log(\Theta_i) - log(\sum \Theta) \\
&amp;amp; + \log(\Theta_j) -  \log(\sum \Theta_{[-i]}) \\
&amp;amp;+ \log(\Theta_k) -\log(\sum \Theta_{[-i, -j]}) \\
&amp;amp; + \ldots
\end{aligned}
\tag{5}
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The following Stan custom function follows this logic but iterating over the rankings. In each iteration, it aggregates in the variable &lt;code&gt;out&lt;/code&gt; the addends of the log probability mass function, and turns the probability of selecting again the already ranked element to zero.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt; real exploded_lpmf(array[] int x, vector Theta){
    real out = 0;
    vector[num_elements(Theta)] thetar = Theta;
    for(pos in x){
      out += log(thetar[pos]) - log(sum(thetar));
      thetar[pos] = 0;
      }
     return(out);
 }&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The whole model named &lt;code&gt;exploded.stan&lt;/code&gt; includes the usual data declaration, the parameter &lt;code&gt;Theta&lt;/code&gt; declared as a simplex (i.e., it sums to one), and a uniform Dirichlet prior for &lt;code&gt;Theta&lt;/code&gt;. (I’m assuming that I don’t know how sparse the probabilities are).&lt;/p&gt;
&lt;pre class=&#34;stan&#34;&gt;&lt;code&gt;functions {
 real exploded_lpmf(array[] int x, vector Theta){
    real out = 0;
    vector[num_elements(Theta)] thetar = Theta;
    for(pos in x){
      out += log(thetar[pos]) - log(sum(thetar));
      thetar[pos] = 0;
      }
     return(out);
 }
}
data{
  int N_ranking; //total times the choices were ranked
  int N_ranked; //total choices ranked
  int N_options; //total options
  array[N_ranking, N_ranked] int res;
}
parameters {
  simplex[N_options] Theta;
}
model {
  target += dirichlet_lpdf(Theta| rep_vector(1, N_options));
  for(r in 1:N_ranking){
    target += exploded_lpmf(res[r]|Theta);
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s see if I can recover the parameter values.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Makethe list of lists into a matrix
res_matrix &amp;lt;- t(sapply(res, as.numeric))
ldata &amp;lt;- list(res = res_matrix, 
              N_ranked = length(res[[1]]), 
              N_options = length(P_toppings), 
              N_ranking = length(res)) 

m_expl &amp;lt;- cmdstan_model(&amp;quot;./exploded.stan&amp;quot;)

f_exploded &amp;lt;- m_expl$sample(
  data = ldata,
  seed = 123,
  parallel_chains = 4
)

f_exploded&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt; variable    mean  median   sd  mad      q5     q95 rhat ess_bulk ess_tail
 lp__     -688.83 -688.43 3.67 3.67 -695.50 -683.60 1.00     1381     2171
 Theta[1]    0.35    0.35 0.04 0.04    0.28    0.42 1.00     5554     2867
 Theta[2]    0.16    0.16 0.02 0.02    0.13    0.20 1.00     5578     2692
 Theta[3]    0.13    0.13 0.02 0.02    0.10    0.17 1.00     5535     2966
 Theta[4]    0.08    0.08 0.01 0.01    0.06    0.11 1.00     5656     3210
 Theta[5]    0.10    0.10 0.01 0.01    0.08    0.12 1.00     6052     2845
 Theta[6]    0.06    0.06 0.01 0.01    0.04    0.08 1.00     5893     3232
 Theta[7]    0.04    0.04 0.01 0.01    0.03    0.05 1.00     5861     3159
 Theta[8]    0.02    0.02 0.00 0.00    0.01    0.03 1.00     6491     3193
 Theta[9]    0.01    0.01 0.00 0.00    0.00    0.01 1.00     5362     2888

 # showing 10 of 26 rows (change via &amp;#39;max_rows&amp;#39; argument or &amp;#39;cmdstanr_max_rows&amp;#39; option)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I plot the posterior distributions of the probability values and the true probability values below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mcmc_recover_hist(f_exploded$draws(&amp;quot;Theta&amp;quot;),
                  P_toppings,
                  facet_args =
                    list(scales = &amp;quot;fixed&amp;quot;, ncol = 3)) +
  theme(legend.position=&amp;quot;bottom&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/2021/03/21/a-simple-way-to-model-rankings-with-stan//2021/03/21/a-simple-way-to-model-rankings-with-stan/index_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It looks reasonable. However, if we really want to be sure that this is working, we should probably use simulation based calibration &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;/2021/03/21/a-simple-way-to-model-rankings-with-stan/#ref-taltsValidatingBayesianInference2018&#34;&gt;Talts et al. 2018&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;what-is-this-good-for&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What is this good for?&lt;/h2&gt;
&lt;p&gt;This super simple example shows how to get an underlying ranking based on a set of responses from a number of subjects. It’s straightforward to adapt this model to data from participants ranking elements from different sets of the &lt;em&gt;same size&lt;/em&gt; (e.g., 7 out of 25 toppings, 7 out of 25 tools). It’s a little less straightforward if the sets are of different sizes, e.g., rank 7 toppings out 25, but 7 tools out 50. This is just because Stan doesn’t allow ragged arrays. See &lt;a href=&#34;https://discourse.mc-stan.org/t/ragged-array-of-simplexes/1382/31&#34;&gt;here&lt;/a&gt; some tips for implementing the latter model.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;could-this-be-used-as-a-cognitive-model-of-peoples-rankings&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Could this be used as a cognitive model of people’s rankings?&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;/2021/03/21/a-simple-way-to-model-rankings-with-stan/https://media.giphy.com/media/dXcwxFuXCd8sI3VcFb/giphy.gif&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Maybe. And I enter here in the realm of half baked research, ideal for a blog post.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;citation&#34;&gt;Lee, Steyvers, and Miller (&lt;a href=&#34;/2021/03/21/a-simple-way-to-model-rankings-with-stan/#ref-Lee2014&#34;&gt;2014&lt;/a&gt;)&lt;/span&gt; show the implementation of a cognitive model for rank order data from the latent knowledge of participants, which is
based on Thurstonian models &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;/2021/03/21/a-simple-way-to-model-rankings-with-stan/#ref-thurstone1927law&#34;&gt;Thurstone 1927&lt;/a&gt;, &lt;a href=&#34;/2021/03/21/a-simple-way-to-model-rankings-with-stan/#ref-thurstone1931rank&#34;&gt;1931&lt;/a&gt;)&lt;/span&gt; fitted with Bayesian methods in JAGS &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;/2021/03/21/a-simple-way-to-model-rankings-with-stan/#ref-Johnson2013&#34;&gt;Johnson and Kuhn 2013&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The exploded logit model seems to be closely related to the Thurstonian model. The Thurstonian model assumes that each participant assigns an underlying score to each item of a set, which is drawn from a true score with normally distributed error. The score determines the order that the participant gives. We can think about the exploded logit similarly. While I modeled the underlying ranking based on probability values, one could assume that each participant &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt; had their own score &lt;span class=&#34;math inline&#34;&gt;\(mu_{is}\)&lt;/span&gt; for each item (or pizza topping) &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;, which is built as a common score &lt;span class=&#34;math inline&#34;&gt;\(mu_i\)&lt;/span&gt; together with some individual deviation &lt;span class=&#34;math inline&#34;&gt;\(\epsilon_{is}\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34; id=&#34;eq:muis&#34;&gt;\[\begin{equation}
\mu_{is}  = \mu_i + \epsilon_{is}
\tag{6}
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;If we assume that &lt;span class=&#34;math inline&#34;&gt;\(\epsilon_{is}\)&lt;/span&gt; has a &lt;a href=&#34;https://en.wikipedia.org/wiki/Gumbel_distribution&#34;&gt;Gumbel&lt;/a&gt; distribution, then the probability of &lt;span class=&#34;math inline&#34;&gt;\(\mu_{is}\)&lt;/span&gt; being ranked first out of N options is determined by a softmax function:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34; id=&#34;eq:probi2&#34;&gt;\[\begin{equation}
P(i) = \frac{\exp(\mu_i)}{\sum \exp(\mu)}
\tag{7}
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; is the vector of scores for all elements of the set.&lt;/p&gt;
&lt;p&gt;And the probability of ordering &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; second is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34; id=&#34;eq:probj2&#34;&gt;\[\begin{equation}
P(i,j,\ldots) = \frac{\exp(\mu_j)}{\sum \exp(\mu_{[-i]} )}
\tag{8}
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;and so forth.&lt;/p&gt;
&lt;p&gt;These last equations are essentially the same categorical distributions that I used before in &lt;a href=&#34;/2021/03/21/a-simple-way-to-model-rankings-with-stan/#eq:probi&#34;&gt;(2)&lt;/a&gt; and &lt;a href=&#34;/2021/03/21/a-simple-way-to-model-rankings-with-stan/#eq:probj&#34;&gt;(3)&lt;/a&gt;, but the softmax function converts the unbounded scores into probabilities first. However, with the exploded logit, the error term goes away leading to a more tractable model. This is not the case for the Thurstonian model. The Thurstonian model is more complex, but at the same time we gain more flexibility. With the error term, the Thurstonian model can incorporate the reliability of the participants’ judgments and even correlations, which, as far as I know, can’t be included in the exploded logit model.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;session-info&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Session info &lt;a href=&#34;/2021/03/21/a-simple-way-to-model-rankings-with-stan/#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sessionInfo()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;R version 4.3.2 (2023-10-31)
Platform: x86_64-pc-linux-gnu (64-bit)
Running under: Ubuntu 22.04.3 LTS

Matrix products: default
BLAS:   /usr/lib/x86_64-linux-gnu/blas/libblas.so.3.10.0 
LAPACK: /usr/lib/x86_64-linux-gnu/lapack/liblapack.so.3.10.0

locale:
 [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C               LC_TIME=nl_NL.UTF-8        LC_COLLATE=en_US.UTF-8     LC_MONETARY=nl_NL.UTF-8   
 [6] LC_MESSAGES=en_US.UTF-8    LC_PAPER=nl_NL.UTF-8       LC_NAME=C                  LC_ADDRESS=C               LC_TELEPHONE=C            
[11] LC_MEASUREMENT=nl_NL.UTF-8 LC_IDENTIFICATION=C       

time zone: Europe/Amsterdam
tzcode source: system (glibc)

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base     

other attached packages:
[1] bayesplot_1.10.0   cmdstanr_0.7.1     rcorpora_2.0.0     extraDistr_1.9.1   ggplot2_3.4.4.9000 purrr_1.0.2        dplyr_1.1.3       

loaded via a namespace (and not attached):
 [1] gtable_0.3.4         tensorA_0.36.2.1     xfun_0.41            bslib_0.5.1          processx_3.8.3       papaja_0.1.2         vctrs_0.6.5         
 [8] tools_4.3.2          ps_1.7.6             generics_0.1.3       tibble_3.2.1         fansi_1.0.6          highr_0.10           RefManageR_1.4.0    
[15] pkgconfig_2.0.3      data.table_1.14.10   checkmate_2.3.1      distributional_0.3.2 lifecycle_1.0.4      compiler_4.3.2       farver_2.1.1        
[22] stringr_1.5.0        munsell_0.5.0        tinylabels_0.2.4     httpuv_1.6.12        htmltools_0.5.7      sass_0.4.7           yaml_2.3.7          
[29] later_1.3.1          pillar_1.9.0         jquerylib_0.1.4      ellipsis_0.3.2       cachem_1.0.8         abind_1.4-5          mime_0.12           
[36] posterior_1.5.0      tidyselect_1.2.0     digest_0.6.34        stringi_1.8.1        reshape2_1.4.4       bookdown_0.36        labeling_0.4.3      
[43] bibtex_0.5.1         fastmap_1.1.1        grid_4.3.2           colorspace_2.1-0     cli_3.6.2            magrittr_2.0.3       utf8_1.2.4          
[50] withr_3.0.0          promises_1.2.1       scales_1.3.0         backports_1.4.1      lubridate_1.9.3      timechange_0.2.0     rmarkdown_2.25      
[57] httr_1.4.7           matrixStats_1.2.0    blogdown_1.18        shiny_1.7.5.1        evaluate_0.23        knitr_1.45           rlang_1.1.3         
[64] Rcpp_1.0.12          xtable_1.8-4         glue_1.7.0           xml2_1.3.5           rstudioapi_0.15.0    jsonlite_1.8.8       R6_2.5.1            
[71] plyr_1.8.9          &lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2 unnumbered&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34;&gt;
&lt;div id=&#34;ref-Barsalou1985&#34; class=&#34;csl-entry&#34;&gt;
Barsalou, Lawrence W. 1985. &lt;span&gt;“Ideals, Central Tendency, and Frequency of Instantiation as Determinants of Graded Structure in Categories.”&lt;/span&gt; &lt;em&gt;Journal of Experimental Psychology: Learning, Memory, and Cognition&lt;/em&gt; 11 (4): 629.
&lt;/div&gt;
&lt;div id=&#34;ref-BEGGS19811&#34; class=&#34;csl-entry&#34;&gt;
Beggs, S, S Cardell, and J Hausman. 1981. &lt;span&gt;“Assessing the Potential Demand for Electric Cars.”&lt;/span&gt; &lt;em&gt;Journal of Econometrics&lt;/em&gt; 17 (1): 1–19. https://doi.org/&lt;a href=&#34;https://doi.org/10.1016/0304-4076(81)90056-7&#34;&gt;https://doi.org/10.1016/0304-4076(81)90056-7&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-R-shiny&#34; class=&#34;csl-entry&#34;&gt;
Chang, Winston, Joe Cheng, JJ Allaire, Carson Sievert, Barret Schloerke, Yihui Xie, Jeff Allen, Jonathan McPherson, Alan Dipert, and Barbara Borges. 2021. &lt;em&gt;Shiny: Web Application Framework for r&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=shiny&#34;&gt;https://CRAN.R-project.org/package=shiny&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-R-htmltools&#34; class=&#34;csl-entry&#34;&gt;
Cheng, Joe, Carson Sievert, Barret Schloerke, Winston Chang, Yihui Xie, and Jeff Allen. 2023. &lt;em&gt;Htmltools: Tools for HTML&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=htmltools&#34;&gt;https://CRAN.R-project.org/package=htmltools&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-R-cmdstanr&#34; class=&#34;csl-entry&#34;&gt;
Gabry, Jonah, and Rok Češnovar. 2020. &lt;em&gt;Cmdstanr: R Interface to ’CmdStan’&lt;/em&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-R-bayesplot&#34; class=&#34;csl-entry&#34;&gt;
Gabry, Jonah, Daniel Simpson, Aki Vehtari, Michael Betancourt, and Andrew Gelman. 2019. &lt;span&gt;“Visualization in Bayesian Workflow.”&lt;/span&gt; &lt;em&gt;J. R. Stat. Soc. A&lt;/em&gt; 182: 389–402. &lt;a href=&#34;https://doi.org/10.1111/rssa.12378&#34;&gt;https://doi.org/10.1111/rssa.12378&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-gakisetal2018&#34; class=&#34;csl-entry&#34;&gt;
Gakis, Konstantinos, Panos Pardalos, Chang-Hwan Choi, Jae-Hyeon Park, and Jiwun Yoon. 2018. &lt;span&gt;“Simulation of a Probabilistic Model for Multi-Contestant Races.”&lt;/span&gt; &lt;em&gt;Athens Journal of Sports&lt;/em&gt; 5 (2): 95–114.
&lt;/div&gt;
&lt;div id=&#34;ref-R-purrr&#34; class=&#34;csl-entry&#34;&gt;
Henry, Lionel, and Hadley Wickham. 2020. &lt;em&gt;Purrr: Functional Programming Tools&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=purrr&#34;&gt;https://CRAN.R-project.org/package=purrr&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-Johnson2013&#34; class=&#34;csl-entry&#34;&gt;
Johnson, Timothy R., and Kristine M. Kuhn. 2013. &lt;span&gt;“Bayesian &lt;span&gt;Thurstonian&lt;/span&gt; Models for Ranking Data Using &lt;span&gt;JAGS&lt;/span&gt;.”&lt;/span&gt; &lt;em&gt;Behavior Research Methods&lt;/em&gt; 45 (3): 857–72. &lt;a href=&#34;https://doi.org/10.3758/s13428-012-0300-3&#34;&gt;https://doi.org/10.3758/s13428-012-0300-3&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-R-rcorpora&#34; class=&#34;csl-entry&#34;&gt;
Kazemi, Darius, Cole Willsea, Serin Delaunay, Karl Swedberg, Matthew Rothenberg, Greg Kennedy, Nathaniel Mitchell, et al. 2018. &lt;em&gt;Rcorpora: A Collection of Small Text Corpora of Interesting Data&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=rcorpora&#34;&gt;https://CRAN.R-project.org/package=rcorpora&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-Lee2014&#34; class=&#34;csl-entry&#34;&gt;
Lee, Michael D., Mark Steyvers, and Brent Miller. 2014. &lt;span&gt;“A Cognitive Model for Aggregating People’s Rankings.”&lt;/span&gt; &lt;em&gt;PLOS ONE&lt;/em&gt; 9 (5): e96431. &lt;a href=&#34;https://doi.org/10.1371/journal.pone.0096431&#34;&gt;https://doi.org/10.1371/journal.pone.0096431&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-Luce1959&#34; class=&#34;csl-entry&#34;&gt;
Luce, R. Duncan. 1959. &lt;em&gt;Individual Choice Behavior : A Theoretical Analysis&lt;/em&gt;. Book. Wiley N.Y.
&lt;/div&gt;
&lt;div id=&#34;ref-R-RefManageR&#34; class=&#34;csl-entry&#34;&gt;
McLean, Mathew William. 2017. &lt;span&gt;“RefManageR: Import and Manage BibTeX and BibLaTeX References in r.”&lt;/span&gt; &lt;em&gt;The Journal of Open Source Software&lt;/em&gt;. &lt;a href=&#34;https://doi.org/10.21105/joss.00338&#34;&gt;https://doi.org/10.21105/joss.00338&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-Plackett&#34; class=&#34;csl-entry&#34;&gt;
Plackett, R. L. 1975. &lt;span&gt;“The Analysis of Permutations.”&lt;/span&gt; &lt;em&gt;Journal of the Royal Statistical Society. Series C (Applied Statistics)&lt;/em&gt; 24 (2): 193–202. &lt;a href=&#34;http://www.jstor.org/stable/2346567&#34;&gt;http://www.jstor.org/stable/2346567&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-R-base&#34; class=&#34;csl-entry&#34;&gt;
R Core Team. 2020. &lt;em&gt;R: A Language and Environment for Statistical Computing&lt;/em&gt;. Vienna, Austria: R Foundation for Statistical Computing. &lt;a href=&#34;https://www.R-project.org/&#34;&gt;https://www.R-project.org/&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-taltsValidatingBayesianInference2018&#34; class=&#34;csl-entry&#34;&gt;
Talts, Sean, Michael Betancourt, Daniel Simpson, Aki Vehtari, and Andrew Gelman. 2018. &lt;span&gt;“Validating &lt;span&gt;Bayesian Inference Algorithms&lt;/span&gt; with &lt;span&gt;Simulation&lt;/span&gt;-&lt;span&gt;Based Calibration&lt;/span&gt;,”&lt;/span&gt; April. &lt;a href=&#34;http://arxiv.org/abs/1804.06788&#34;&gt;http://arxiv.org/abs/1804.06788&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-thurstone1927law&#34; class=&#34;csl-entry&#34;&gt;
Thurstone, Louis L. 1927. &lt;span&gt;“A Law of Comparative Judgement.”&lt;/span&gt; &lt;em&gt;Psychological Reviews&lt;/em&gt; 34: 273–86.
&lt;/div&gt;
&lt;div id=&#34;ref-thurstone1931rank&#34; class=&#34;csl-entry&#34;&gt;
———. 1931. &lt;span&gt;“Rank Order as a Psycho-Physical Method.”&lt;/span&gt; &lt;em&gt;Journal of Experimental Psychology&lt;/em&gt; 14 (3): 187.
&lt;/div&gt;
&lt;div id=&#34;ref-R-ggplot2&#34; class=&#34;csl-entry&#34;&gt;
Wickham, Hadley. 2016. &lt;em&gt;Ggplot2: Elegant Graphics for Data Analysis&lt;/em&gt;. Springer-Verlag New York. &lt;a href=&#34;https://ggplot2.tidyverse.org&#34;&gt;https://ggplot2.tidyverse.org&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-R-stringr&#34; class=&#34;csl-entry&#34;&gt;
———. 2022. &lt;em&gt;Stringr: Simple, Consistent Wrappers for Common String Operations&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=stringr&#34;&gt;https://CRAN.R-project.org/package=stringr&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-R-dplyr&#34; class=&#34;csl-entry&#34;&gt;
Wickham, Hadley, Romain François, Lionel Henry, and Kirill Müller. 2021. &lt;em&gt;Dplyr: A Grammar of Data Manipulation&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=dplyr&#34;&gt;https://CRAN.R-project.org/package=dplyr&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-R-extraDistr&#34; class=&#34;csl-entry&#34;&gt;
Wolodzko, Tymoteusz. 2020. &lt;em&gt;extraDistr: Additional Univariate and Multivariate Distributions&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=extraDistr&#34;&gt;https://CRAN.R-project.org/package=extraDistr&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-R-knitr&#34; class=&#34;csl-entry&#34;&gt;
Xie, Yihui. 2015. &lt;em&gt;Dynamic Documents with &lt;span&gt;R&lt;/span&gt; and Knitr&lt;/em&gt;. 2nd ed. Boca Raton, Florida: Chapman; Hall/CRC. &lt;a href=&#34;https://yihui.org/knitr/&#34;&gt;https://yihui.org/knitr/&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-R-rmarkdown_a&#34; class=&#34;csl-entry&#34;&gt;
Xie, Yihui, J. J. Allaire, and Garrett Grolemund. 2018. &lt;em&gt;R Markdown: The Definitive Guide&lt;/em&gt;. Boca Raton, Florida: Chapman; Hall/CRC. &lt;a href=&#34;https://bookdown.org/yihui/rmarkdown&#34;&gt;https://bookdown.org/yihui/rmarkdown&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-R-rmarkdown_b&#34; class=&#34;csl-entry&#34;&gt;
Xie, Yihui, Christophe Dervieux, and Emily Riederer. 2020. &lt;em&gt;R Markdown Cookbook&lt;/em&gt;. Boca Raton, Florida: Chapman; Hall/CRC. &lt;a href=&#34;https://bookdown.org/yihui/rmarkdown-cookbook&#34;&gt;https://bookdown.org/yihui/rmarkdown-cookbook&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-R-mediumr&#34; class=&#34;csl-entry&#34;&gt;
Yutani, Hiroaki. 2021. &lt;em&gt;Mediumr: R Interface to ’Medium’ API&lt;/em&gt;.
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes footnotes-end-of-document&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;This model is also called the &lt;em&gt;rank ordered logit model&lt;/em&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;/2021/03/21/a-simple-way-to-model-rankings-with-stan/#ref-BEGGS19811&#34;&gt;Beggs, Cardell, and Hausman 1981&lt;/a&gt;)&lt;/span&gt; or Plackett–Luce model due to &lt;span class=&#34;citation&#34;&gt;Plackett (&lt;a href=&#34;/2021/03/21/a-simple-way-to-model-rankings-with-stan/#ref-Plackett&#34;&gt;1975&lt;/a&gt;)&lt;/span&gt; and &lt;span class=&#34;citation&#34;&gt;Luce (&lt;a href=&#34;/2021/03/21/a-simple-way-to-model-rankings-with-stan/#ref-Luce1959&#34;&gt;1959&lt;/a&gt;)&lt;/span&gt;, but I liked the explosion part more.&lt;a href=&#34;/2021/03/21/a-simple-way-to-model-rankings-with-stan/#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;I used R &lt;span class=&#34;citation&#34;&gt;(Version 4.3.2; &lt;a href=&#34;/2021/03/21/a-simple-way-to-model-rankings-with-stan/#ref-R-base&#34;&gt;R Core Team 2020&lt;/a&gt;)&lt;/span&gt; and the R-packages &lt;em&gt;bayesplot&lt;/em&gt; &lt;span class=&#34;citation&#34;&gt;(Version 1.10.0; &lt;a href=&#34;/2021/03/21/a-simple-way-to-model-rankings-with-stan/#ref-R-bayesplot&#34;&gt;Gabry et al. 2019&lt;/a&gt;)&lt;/span&gt;, &lt;em&gt;cmdstanr&lt;/em&gt; &lt;span class=&#34;citation&#34;&gt;(Version 0.7.1; &lt;a href=&#34;/2021/03/21/a-simple-way-to-model-rankings-with-stan/#ref-R-cmdstanr&#34;&gt;Gabry and Češnovar 2020&lt;/a&gt;)&lt;/span&gt;, &lt;em&gt;dplyr&lt;/em&gt; &lt;span class=&#34;citation&#34;&gt;(Version 1.1.3; &lt;a href=&#34;/2021/03/21/a-simple-way-to-model-rankings-with-stan/#ref-R-dplyr&#34;&gt;Wickham et al. 2021&lt;/a&gt;)&lt;/span&gt;, &lt;em&gt;extraDistr&lt;/em&gt; &lt;span class=&#34;citation&#34;&gt;(Version 1.9.1; &lt;a href=&#34;/2021/03/21/a-simple-way-to-model-rankings-with-stan/#ref-R-extraDistr&#34;&gt;Wolodzko 2020&lt;/a&gt;)&lt;/span&gt;, &lt;em&gt;ggplot2&lt;/em&gt; &lt;span class=&#34;citation&#34;&gt;(Version 3.4.4.9000; &lt;a href=&#34;/2021/03/21/a-simple-way-to-model-rankings-with-stan/#ref-R-ggplot2&#34;&gt;Wickham 2016&lt;/a&gt;)&lt;/span&gt;, &lt;em&gt;htmltools&lt;/em&gt; &lt;span class=&#34;citation&#34;&gt;(Version 0.5.7; &lt;a href=&#34;/2021/03/21/a-simple-way-to-model-rankings-with-stan/#ref-R-htmltools&#34;&gt;Cheng et al. 2023&lt;/a&gt;)&lt;/span&gt;, &lt;em&gt;knitr&lt;/em&gt; &lt;span class=&#34;citation&#34;&gt;(Version 1.45; &lt;a href=&#34;/2021/03/21/a-simple-way-to-model-rankings-with-stan/#ref-R-knitr&#34;&gt;Xie 2015&lt;/a&gt;)&lt;/span&gt;, &lt;em&gt;mediumr&lt;/em&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;/2021/03/21/a-simple-way-to-model-rankings-with-stan/#ref-R-mediumr&#34;&gt;Yutani 2021&lt;/a&gt;)&lt;/span&gt;, &lt;em&gt;purrr&lt;/em&gt; &lt;span class=&#34;citation&#34;&gt;(Version 1.0.2; &lt;a href=&#34;/2021/03/21/a-simple-way-to-model-rankings-with-stan/#ref-R-purrr&#34;&gt;Henry and Wickham 2020&lt;/a&gt;)&lt;/span&gt;, &lt;em&gt;rcorpora&lt;/em&gt; &lt;span class=&#34;citation&#34;&gt;(Version 2.0.0; &lt;a href=&#34;/2021/03/21/a-simple-way-to-model-rankings-with-stan/#ref-R-rcorpora&#34;&gt;Kazemi et al. 2018&lt;/a&gt;)&lt;/span&gt;, &lt;em&gt;RefManageR&lt;/em&gt; &lt;span class=&#34;citation&#34;&gt;(Version 1.4.0; &lt;a href=&#34;/2021/03/21/a-simple-way-to-model-rankings-with-stan/#ref-R-RefManageR&#34;&gt;McLean 2017&lt;/a&gt;)&lt;/span&gt;, &lt;em&gt;rmarkdown&lt;/em&gt; &lt;span class=&#34;citation&#34;&gt;(Version 2.25; &lt;a href=&#34;/2021/03/21/a-simple-way-to-model-rankings-with-stan/#ref-R-rmarkdown_a&#34;&gt;Xie, Allaire, and Grolemund 2018&lt;/a&gt;; &lt;a href=&#34;/2021/03/21/a-simple-way-to-model-rankings-with-stan/#ref-R-rmarkdown_b&#34;&gt;Xie, Dervieux, and Riederer 2020&lt;/a&gt;)&lt;/span&gt;, &lt;em&gt;shiny&lt;/em&gt; &lt;span class=&#34;citation&#34;&gt;(Version 1.7.5.1; &lt;a href=&#34;/2021/03/21/a-simple-way-to-model-rankings-with-stan/#ref-R-shiny&#34;&gt;Chang et al. 2021&lt;/a&gt;)&lt;/span&gt;, and &lt;em&gt;stringr&lt;/em&gt; &lt;span class=&#34;citation&#34;&gt;(Version 1.5.0; &lt;a href=&#34;/2021/03/21/a-simple-way-to-model-rankings-with-stan/#ref-R-stringr&#34;&gt;Wickham 2022&lt;/a&gt;)&lt;/span&gt; to generate this document.&lt;a href=&#34;/2021/03/21/a-simple-way-to-model-rankings-with-stan/#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
