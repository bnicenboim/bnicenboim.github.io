---
title: "A simple way to model rankings with Stan using the exploded logit distribution"
author: "Bruno Nicenboim"
date: "2021-03-17"
categories: ["stats"]
tags: ["stan", "Bayesian", "r"]
bibliography: ["biblio.bib","r-references.bib"]
draft: false
---



<div id="the-initial-problem" class="section level2">
<h2>The initial problem</h2>
<p>I’ve been asking around about this:</p>
<p>{{% tweet "1369217174699732995" %}}</p>
<p>One useful clue was this one:</p>
<p>{{% tweet "1369226616312901637" %}}</p>
<p>The distribution that I was describing is called an <a href="https://en.wikipedia.org/wiki/Discrete_choice#exploded_logit"><em>exploded logit distribution</em></a>.</p>
<p><img src="https://media.giphy.com/media/3osxYCsLd9qgsgqpwI/giphy.gif" /></p>
<p>(This model is also called the <em>rank ordered logit model</em> <span class="citation">(Beggs, Cardell, and Hausman <a href="#ref-BEGGS19811">1981</a>)</span> or Plackett–Luce model due to <span class="citation">Plackett (<a href="#ref-Plackett">1975</a>)</span> and <span class="citation">Luce (<a href="#ref-Luce1959">1959</a>)</span>.)</p>
<p>We’ll see that it can be useful to describe <em>some</em> types of ranking data.</p>
<p>I’m going to load some R packages that will be useful throughout this post.</p>
<pre class="r"><code>library(dplyr) # Data manipulation
library(purrr) # List manipulation
library(ggplot2) # Nice plots
library(extraDistr) # More distributions
library(rcorpora) # Get random words
library(cmdstanr) # Lightweight Stan interface
library(bayesplot) # Nice Bayesian plots</code></pre>
</div>
<div id="ranking-data" class="section level2">
<h2>Ranking data</h2>
<p>Ranking data appear when we care about the <em>underlying</em> order or ranking that certain elements have. We might want to know which are the best horses after looking at several races <span class="citation">(Gakis et al. <a href="#ref-gakisetal2018">2018</a>)</span>, which is the best candidate for a job after a series of interviewers talked to several candidates. More in line with cognitive science, we might want to know which are the best possible completions for a sentence or the best exemplars of a category.</p>
<p>One way to assess get a ranking of exemplars of a category is to present them to participants and ask them to order them or a subset of them <span class="citation">(see Barsalou <a href="#ref-Barsalou1985">1985</a>)</span>.</p>
<div id="a-ranking-simulation-using-pizza-toppings." class="section level3">
<h3>A ranking simulation using pizza toppings.</h3>
<p><img src="https://media.giphy.com/media/3oEjHZhG9COPG6XjzO/giphy.gif" /></p>
<p>Let’s consider the following pizza toppings:</p>
<pre class="r"><code>toppings &lt;- corpora(&quot;foods/pizzaToppings&quot;)$pizzaToppings
N_toppings &lt;- length(toppings)</code></pre>
<p>Let’s say that there is an underlying universal order of pizza toppings. The toppings are ordered according to an underlying value, a probability, that also represents how likely they are to be given as <em>the</em> exemplar of their category.</p>
<p>To assign probabilities to the toppings, I draw random samples from a <a href="https://en.wikipedia.org/wiki/Dirichlet_distribution">Dirichlet distribution</a>. The Dirichlet distribution is the generalization of the Beta distribution, and similarly to the Beta distribution, one its concentration parameters are all ones it’s uniform. By setting all the concentration parameters below one, I’m assuming sparsity, that is, many values close to zero.</p>
<p>These is the true ranking that I’m assuming here:</p>
<pre class="r"><code>alpha &lt;- rep(.5, N_toppings)
P_toppings &lt;- c(rdirichlet(1, alpha)) %&gt;%
  setNames(toppings) %&gt;%
  sort(decreasing = TRUE) %&gt;%
  round(3)
P_toppings</code></pre>
<pre><code>           bacon             feta  breakfast bacon        anchovies 
           0.193            0.156            0.153            0.078 
          onions        hot sauce          sausage         tomatoes 
           0.075            0.066            0.046            0.038 
       artichoke    green peppers      ground beef   Canadian bacon 
           0.030            0.027            0.023            0.022 
          cheese           garlic   grilled onions          spinach 
           0.022            0.021            0.016            0.013 
       mushrooms        meatballs        pineapple           olives 
           0.008            0.005            0.004            0.003 
             ham    chili peppers          chicken        pepperoni 
           0.000            0.000            0.000            0.000 
sun-dried tomato 
           0.000 </code></pre>
<p>Given these values, if I were to ask a participant “What’s the most appropriate topping for a pizza?” I would assume that 19.3 percent of the time, I would get bacon.</p>
<p>Essentially, we expect something like this to be happening.</p>
<p>$
response Categorical(_{toppings})
$</p>
<p>With <span class="math inline">\(\Theta_{toppings}\)</span> representing the different probabilities for each topping. The probability mass function of the categorical distribution is absurdly simple: It’s just the probability of the outcome.</p>
<p><span class="math display">\[p(x = i) = \Theta_i\]</span>, where <span class="math inline">\(i = \{\)</span>bacon, feta, breakfast bacon, anchovies, onions, hot sauce, sausage, tomatoes, artichoke, green peppers, ground beef, Canadian bacon, cheese, garlic, grilled onions, spinach, mushrooms, meatballs, pineapple, olives, ham, chili peppers, chicken, pepperoni, sun-dried tomato<span class="math inline">\(\}\)</span>.</p>
<p>We can simulate this with 100 participants as follows:</p>
<pre class="r"><code>response &lt;- rcat(100, P_toppings, names(P_toppings))</code></pre>
<p>And this should match approximately <code>P_toppings</code>.</p>
<pre class="r"><code>table(response)/100</code></pre>
<pre><code>response
           bacon             feta  breakfast bacon        anchovies 
            0.20             0.11             0.13             0.08 
          onions        hot sauce          sausage         tomatoes 
            0.10             0.04             0.10             0.07 
       artichoke    green peppers      ground beef   Canadian bacon 
            0.04             0.03             0.00             0.03 
          cheese           garlic   grilled onions          spinach 
            0.01             0.04             0.02             0.00 
       mushrooms        meatballs        pineapple           olives 
            0.00             0.00             0.00             0.00 
             ham    chili peppers          chicken        pepperoni 
            0.00             0.00             0.00             0.00 
sun-dried tomato 
            0.00 </code></pre>
<p>Now, what happens if we ask about the second most appropriate topping for a pizza?</p>
<p>Now, for each participant, we need to exclude the first topping, and draw another sample from a categorical distribution. (We don’t allow them to repeat, that is, to say that best topping is pineapple and second best is also pineapple). This means that now the probability of the item already given is zero, and that we need to normalize our original probability values by dividing it by the new total probability (which will be lower than 1).</p>
<p>Here, the probability of getting the element <span class="math inline">\(j\)</span> (where <span class="math inline">\(j \neq i\)</span>) is</p>
<p><span class="math display">\[p(x = j) = \frac{\Theta_j}{\sum \Theta_{-i}}\]</span></p>
<p>where <span class="math inline">\(\Theta_{-i}\)</span> represents the probabilities of all the outcomes except of <span class="math inline">\(i\)</span>, which was the first one. We can go on with the third best topping, where we need to normalize the remaining probabilities by dividing by the new sum of remaining probabilities.</p>
<p><span class="math display">\[p(x = k) = \frac{\Theta_k}{\sum \Theta_{-i,-j}}\]</span></p>
<p>Until we get to the last element, which will be drawn with probability 1.</p>
<p><strong>This is the exploded logit distribution.</strong></p>
<p>This process can be simulated in R as follows:</p>
<pre class="r"><code>rexploded &lt;-  function(n, ranked = 3, prob, labels = NULL){
  #run n times
  lapply(1:n, function(nn){
    res &lt;- rep(NA,ranked)
    if(!is.null(labels)){
      res &lt;- factor(res, labels)
    } else {
      # if there are no labels, just 1,2,3,...
      labels &lt;- seq_along(prob)
    }
  for(i in 1:ranked){
    # normalize the probability so that it sums to 1
    prob &lt;- prob/sum(prob)
    res[i] &lt;- rcat(1, prob = prob, labels = labels)
    # remove the choice from the set:
    prob[res[i]] &lt;- 0
  }
    res
  })
}</code></pre>
<p>If we would like to simulate 50 subjects that rank 5 ingredients out of the total in order of how good toppings they are, we would do the following:</p>
<pre class="r"><code>res &lt;- rexploded(n = 50, ranked = 5, prob = P_toppings, labels = names(P_toppings))
# subject 1:
res[[1]]</code></pre>
<pre><code>[1] sausage         cheese          breakfast bacon green peppers  
[5] feta           
25 Levels: bacon feta breakfast bacon anchovies onions hot sauce ... sun-dried tomato</code></pre>
<pre class="r"><code>data_plot &lt;- map_dfr(res,
                     ~ tibble(topping= .x) %&gt;%
                       mutate(order = 1:n()),
                     .id=&quot;subject&quot;) %&gt;%
  mutate(subject = as.numeric(subject))
ggplot(data_plot, aes(x=order, y = subject, label = topping, group = subject)) +
  geom_line() +
  geom_label() +
  theme_minimal()</code></pre>
<p><img src="/post/2021-03-17-exploded_draft/rankings_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<p>We have simulated a ranking of pizza toppings, can we recover the original probability values and “discover” the underlying order?</p>
</div>
</div>
<div id="fitting-the-exploded-logistic-distribution-in-stan" class="section level2">
<h2>Fitting the exploded logistic distribution in Stan</h2>
<p>To fit the model in Stan, I’m going to create a custom probability mass function that takes an array of integers, <code>x</code>, which represent a set of rankings, and a vector of probability values, <code>theta</code>, that sums up to one.</p>
<p>The logic of this function is that the probability mass function of a ranking <span class="math inline">\(\{i,j,k, \ldots, N \}\)</span> can be written as a product of normalized categorical distributions (where the first one is just divided by 1).</p>
<p><span class="math display">\[p(x = \{i,j,k,\ldots\}) = \frac{\Theta_i}{\sum \Theta} \cdot \frac{\Theta_j}{ \Theta_{-i}} \cdot \frac{\Theta_k}{ \Theta_{-i, -j}} \ldots\]</span></p>
<p>In log-space products become sums, and divisions a difference, and the log of will be zero:</p>
<p><span class="math display">\[log(p(x = \{i,j,k,\ldots\})) = \log(\Theta_i) - log(\sum \Theta) + \log(\Theta_j) -  \log(\Theta_{-i}) + \log(\Theta_k) -\log( \Theta_{-i, -j}) + \ldots\]</span></p>
<p>The custom function follows this logic but iterating over the rankings, and in each iteration, it aggregates in <code>out</code> the addends of the log probability mass function, and turns the probability of selecting again the ranked element to zero.</p>
<pre><code> real exploded_lpmf(int[] x, vector Theta){
    real out = 0;
    vector[num_elements(Theta)] thetar = Theta;
    for(pos in x){
      out += log(thetar[pos]) - log(sum(thetar));
      thetar[pos] = 0;
      }
     return(out);
 }</code></pre>
<p>The whole model named <code>exploded.stan</code> includes, the usual data declaration, the parameter <code>Theta</code> declared as a simplex (i.e., it sums to one), and a uniform Dirichlet prior for <code>Theta</code>.</p>
<pre class="stan"><code>functions {
 real exploded_lpmf(int[] x, vector Theta){
    real out = 0;
    vector[num_elements(Theta)] thetar = Theta;
    for(pos in x){
      out += log(thetar[pos]) - log(sum(thetar));
      thetar[pos] = 0;
      }
     return(out);
 }
}
data{
  int N_ranking; //total times the choices were ranked
  int N_ranked; //total choices ranked
  int N_options; //total options
  int res[N_ranking, N_ranked];
}
parameters {
  simplex[N_options] Theta;
}
model {
  target += dirichlet_lpdf(Theta| rep_vector(1, N_options));
  for(r in 1:N_ranking){
    target += exploded_lpmf(res[r]|Theta);
  }
}</code></pre>
<p>Let’s see if I can recover the parameter values.</p>
<pre class="r"><code>ldata &lt;- list(res = lapply(res, as.numeric),
              N_ranked = length(res[[1]]), #5
              N_options = length(P_toppings), #25
              N_ranking = length(res)) # 10


m_expl &lt;- cmdstan_model(&quot;./exploded.stan&quot;)

f_exploded &lt;- m_expl$sample(
  data = ldata,
  seed = 123,
  parallel_chains = 4
)</code></pre>
<pre><code>Running MCMC with 4 parallel chains...

Chain 1 Iteration:    1 / 2000 [  0%]  (Warmup) 
Chain 1 Iteration:  100 / 2000 [  5%]  (Warmup) 
Chain 2 Iteration:    1 / 2000 [  0%]  (Warmup) 
Chain 2 Iteration:  100 / 2000 [  5%]  (Warmup) 
Chain 3 Iteration:    1 / 2000 [  0%]  (Warmup) 
Chain 3 Iteration:  100 / 2000 [  5%]  (Warmup) 
Chain 3 Iteration:  200 / 2000 [ 10%]  (Warmup) 
Chain 4 Iteration:    1 / 2000 [  0%]  (Warmup) 
Chain 4 Iteration:  100 / 2000 [  5%]  (Warmup) 
Chain 4 Iteration:  200 / 2000 [ 10%]  (Warmup) 
Chain 1 Iteration:  200 / 2000 [ 10%]  (Warmup) 
Chain 1 Iteration:  300 / 2000 [ 15%]  (Warmup) 
Chain 1 Iteration:  400 / 2000 [ 20%]  (Warmup) 
Chain 1 Iteration:  500 / 2000 [ 25%]  (Warmup) 
Chain 1 Iteration:  600 / 2000 [ 30%]  (Warmup) 
Chain 1 Iteration:  700 / 2000 [ 35%]  (Warmup) 
Chain 2 Iteration:  200 / 2000 [ 10%]  (Warmup) 
Chain 2 Iteration:  300 / 2000 [ 15%]  (Warmup) 
Chain 2 Iteration:  400 / 2000 [ 20%]  (Warmup) 
Chain 2 Iteration:  500 / 2000 [ 25%]  (Warmup) 
Chain 2 Iteration:  600 / 2000 [ 30%]  (Warmup) 
Chain 3 Iteration:  300 / 2000 [ 15%]  (Warmup) 
Chain 3 Iteration:  400 / 2000 [ 20%]  (Warmup) 
Chain 3 Iteration:  500 / 2000 [ 25%]  (Warmup) 
Chain 4 Iteration:  300 / 2000 [ 15%]  (Warmup) 
Chain 4 Iteration:  400 / 2000 [ 20%]  (Warmup) 
Chain 4 Iteration:  500 / 2000 [ 25%]  (Warmup) 
Chain 1 Iteration:  800 / 2000 [ 40%]  (Warmup) 
Chain 1 Iteration:  900 / 2000 [ 45%]  (Warmup) 
Chain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) 
Chain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) 
Chain 2 Iteration:  700 / 2000 [ 35%]  (Warmup) 
Chain 2 Iteration:  800 / 2000 [ 40%]  (Warmup) 
Chain 2 Iteration:  900 / 2000 [ 45%]  (Warmup) 
Chain 3 Iteration:  600 / 2000 [ 30%]  (Warmup) 
Chain 3 Iteration:  700 / 2000 [ 35%]  (Warmup) 
Chain 3 Iteration:  800 / 2000 [ 40%]  (Warmup) 
Chain 3 Iteration:  900 / 2000 [ 45%]  (Warmup) 
Chain 4 Iteration:  600 / 2000 [ 30%]  (Warmup) 
Chain 4 Iteration:  700 / 2000 [ 35%]  (Warmup) 
Chain 4 Iteration:  800 / 2000 [ 40%]  (Warmup) 
Chain 1 Iteration: 1100 / 2000 [ 55%]  (Sampling) 
Chain 1 Iteration: 1200 / 2000 [ 60%]  (Sampling) 
Chain 1 Iteration: 1300 / 2000 [ 65%]  (Sampling) 
Chain 2 Iteration: 1000 / 2000 [ 50%]  (Warmup) 
Chain 2 Iteration: 1001 / 2000 [ 50%]  (Sampling) 
Chain 2 Iteration: 1100 / 2000 [ 55%]  (Sampling) 
Chain 2 Iteration: 1200 / 2000 [ 60%]  (Sampling) 
Chain 3 Iteration: 1000 / 2000 [ 50%]  (Warmup) 
Chain 3 Iteration: 1001 / 2000 [ 50%]  (Sampling) 
Chain 3 Iteration: 1100 / 2000 [ 55%]  (Sampling) 
Chain 3 Iteration: 1200 / 2000 [ 60%]  (Sampling) 
Chain 4 Iteration:  900 / 2000 [ 45%]  (Warmup) 
Chain 4 Iteration: 1000 / 2000 [ 50%]  (Warmup) 
Chain 4 Iteration: 1001 / 2000 [ 50%]  (Sampling) 
Chain 4 Iteration: 1100 / 2000 [ 55%]  (Sampling) 
Chain 1 Iteration: 1400 / 2000 [ 70%]  (Sampling) 
Chain 1 Iteration: 1500 / 2000 [ 75%]  (Sampling) 
Chain 1 Iteration: 1600 / 2000 [ 80%]  (Sampling) 
Chain 2 Iteration: 1300 / 2000 [ 65%]  (Sampling) 
Chain 2 Iteration: 1400 / 2000 [ 70%]  (Sampling) 
Chain 2 Iteration: 1500 / 2000 [ 75%]  (Sampling) 
Chain 3 Iteration: 1300 / 2000 [ 65%]  (Sampling) 
Chain 3 Iteration: 1400 / 2000 [ 70%]  (Sampling) 
Chain 3 Iteration: 1500 / 2000 [ 75%]  (Sampling) 
Chain 4 Iteration: 1200 / 2000 [ 60%]  (Sampling) 
Chain 4 Iteration: 1300 / 2000 [ 65%]  (Sampling) 
Chain 4 Iteration: 1400 / 2000 [ 70%]  (Sampling) 
Chain 1 Iteration: 1700 / 2000 [ 85%]  (Sampling) 
Chain 1 Iteration: 1800 / 2000 [ 90%]  (Sampling) 
Chain 1 Iteration: 1900 / 2000 [ 95%]  (Sampling) 
Chain 2 Iteration: 1600 / 2000 [ 80%]  (Sampling) 
Chain 2 Iteration: 1700 / 2000 [ 85%]  (Sampling) 
Chain 2 Iteration: 1800 / 2000 [ 90%]  (Sampling) 
Chain 3 Iteration: 1600 / 2000 [ 80%]  (Sampling) 
Chain 3 Iteration: 1700 / 2000 [ 85%]  (Sampling) 
Chain 3 Iteration: 1800 / 2000 [ 90%]  (Sampling) 
Chain 4 Iteration: 1500 / 2000 [ 75%]  (Sampling) 
Chain 4 Iteration: 1600 / 2000 [ 80%]  (Sampling) 
Chain 4 Iteration: 1700 / 2000 [ 85%]  (Sampling) 
Chain 4 Iteration: 1800 / 2000 [ 90%]  (Sampling) 
Chain 1 Iteration: 2000 / 2000 [100%]  (Sampling) 
Chain 2 Iteration: 1900 / 2000 [ 95%]  (Sampling) 
Chain 2 Iteration: 2000 / 2000 [100%]  (Sampling) 
Chain 3 Iteration: 1900 / 2000 [ 95%]  (Sampling) 
Chain 3 Iteration: 2000 / 2000 [100%]  (Sampling) 
Chain 4 Iteration: 1900 / 2000 [ 95%]  (Sampling) 
Chain 4 Iteration: 2000 / 2000 [100%]  (Sampling) 
Chain 1 finished in 0.7 seconds.
Chain 2 finished in 0.7 seconds.
Chain 3 finished in 0.7 seconds.
Chain 4 finished in 0.7 seconds.

All 4 chains finished successfully.
Mean chain execution time: 0.7 seconds.
Total execution time: 1.0 seconds.</code></pre>
<pre class="r"><code>f_exploded</code></pre>
<pre><code> variable    mean  median   sd  mad      q5     q95 rhat ess_bulk ess_tail
 lp__     -631.55 -631.15 3.62 3.49 -638.04 -626.26 1.00     1495     2225
 Theta[1]    0.20    0.19 0.03 0.03    0.15    0.25 1.00     9269     3242
 Theta[2]    0.18    0.18 0.03 0.02    0.14    0.23 1.00     8692     2867
 Theta[3]    0.15    0.15 0.02 0.02    0.11    0.19 1.00     7806     2826
 Theta[4]    0.08    0.08 0.02 0.02    0.06    0.11 1.00     8939     2880
 Theta[5]    0.06    0.06 0.01 0.01    0.04    0.09 1.00     7868     2831
 Theta[6]    0.06    0.06 0.01 0.01    0.04    0.09 1.00     8288     2804
 Theta[7]    0.04    0.04 0.01 0.01    0.03    0.06 1.00     9224     3105
 Theta[8]    0.01    0.01 0.01 0.01    0.01    0.02 1.00     7123     2958
 Theta[9]    0.01    0.01 0.01 0.00    0.00    0.02 1.00     7596     2478

 # showing 10 of 26 rows (change via &#39;max_rows&#39; argument)</code></pre>
<p>I plot the posterior distributions of the probability values and the true probability values below.</p>
<pre class="r"><code>mcmc_recover_hist(f_exploded$draws(&quot;Theta&quot;), P_toppings)</code></pre>
<p><img src="/post/2021-03-17-exploded_draft/rankings_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
</div>
<div id="caveats" class="section level2">
<h2>Caveats</h2>
<p>Not a cognitive model ….</p>
</div>
<div id="session-info" class="section level2">
<h2>Session info</h2>
<p>I used R <span class="citation">(Version 4.0.3; R Core Team <a href="#ref-R-base">2020</a>)</span> and the R-packages <em>bayesplot</em> <span class="citation">(Version 1.8.0.9000; Gabry et al. <a href="#ref-R-bayesplot">2019</a>)</span>, <em>cmdstanr</em> <span class="citation">(Version 0.3.0.9000; Gabry and Češnovar <a href="#ref-R-cmdstanr">2020</a>)</span>, <em>dplyr</em> <span class="citation">(Version 1.0.4; Wickham et al. <a href="#ref-R-dplyr">2021</a>)</span>, <em>extraDistr</em> <span class="citation">(Version 1.9.1; Wolodzko <a href="#ref-R-extraDistr">2020</a>)</span>, <em>ggplot2</em> <span class="citation">(Version 3.3.3; Wickham <a href="#ref-R-ggplot2">2016</a>)</span>, <em>mediumr</em> <span class="citation">(Version 0.0.0.9000; Yutani <a href="#ref-R-mediumr">2021</a>)</span>, <em>purrr</em> <span class="citation">(Version 0.3.4; Henry and Wickham <a href="#ref-R-purrr">2020</a>)</span>, <em>rcorpora</em> <span class="citation">(Version 2.0.0; Kazemi et al. <a href="#ref-R-rcorpora">2018</a>)</span>, and <em>shiny</em> <span class="citation">(Version 1.6.0; Chang et al. <a href="#ref-R-shiny">2021</a>)</span> to generate this document.</p>
<pre class="r"><code>sessionInfo()</code></pre>
<pre><code>R version 4.0.3 (2020-10-10)
Platform: x86_64-pc-linux-gnu (64-bit)
Running under: Ubuntu 20.04.2 LTS

Matrix products: default
BLAS:   /usr/lib/x86_64-linux-gnu/blas/libblas.so.3.9.0
LAPACK: /usr/lib/x86_64-linux-gnu/lapack/liblapack.so.3.9.0

locale:
 [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C              
 [3] LC_TIME=nl_NL.UTF-8        LC_COLLATE=en_US.UTF-8    
 [5] LC_MONETARY=nl_NL.UTF-8    LC_MESSAGES=en_US.UTF-8   
 [7] LC_PAPER=nl_NL.UTF-8       LC_NAME=C                 
 [9] LC_ADDRESS=C               LC_TELEPHONE=C            
[11] LC_MEASUREMENT=nl_NL.UTF-8 LC_IDENTIFICATION=C       

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base     

other attached packages:
[1] bayesplot_1.8.0.9000 cmdstanr_0.3.0.9000  rcorpora_2.0.0      
[4] extraDistr_1.9.1     ggplot2_3.3.3        purrr_0.3.4         
[7] dplyr_1.0.4         

loaded via a namespace (and not attached):
 [1] tidyselect_1.1.0  xfun_0.22         bslib_0.2.4       reshape2_1.4.4   
 [5] colorspace_2.0-0  vctrs_0.3.6       generics_0.1.0    htmltools_0.5.1.1
 [9] yaml_2.2.1        utf8_1.1.4        rlang_0.4.10      jquerylib_0.1.3  
[13] pillar_1.5.0      glue_1.4.2        withr_2.4.1       DBI_1.1.1        
[17] lifecycle_1.0.0   plyr_1.8.6        stringr_1.4.0     posterior_0.1.3  
[21] munsell_0.5.0     blogdown_1.2.2    gtable_0.3.0      evaluate_0.14    
[25] labeling_0.4.2    papaja_0.1.0.9997 knitr_1.31        ps_1.6.0         
[29] fansi_0.4.2       highr_0.8         Rcpp_1.0.6        scales_1.1.1     
[33] backports_1.2.1   checkmate_2.0.0   jsonlite_1.7.2    abind_1.4-5      
[37] farver_2.1.0      digest_0.6.27     stringi_1.5.3     bookdown_0.21.6  
[41] processx_3.4.5    grid_4.0.3        tools_4.0.3       magrittr_2.0.1   
[45] sass_0.3.1        tibble_3.1.0      crayon_1.4.1      pkgconfig_2.0.3  
[49] ellipsis_0.3.1    data.table_1.13.6 ggridges_0.5.3    assertthat_0.2.1 
[53] rmarkdown_2.7.4   R6_2.5.0          compiler_4.0.3   </code></pre>
</div>
<div id="references" class="section level2 unnumbered">
<h2>References</h2>
<div id="refs" class="references">
<div id="ref-Barsalou1985">
<p>Barsalou, Lawrence W. 1985. “Ideals, Central Tendency, and Frequency of Instantiation as Determinants of Graded Structure in Categories.” <em>Journal of Experimental Psychology: Learning, Memory, and Cognition</em> 11 (4): 629.</p>
</div>
<div id="ref-BEGGS19811">
<p>Beggs, S, S Cardell, and J Hausman. 1981. “Assessing the Potential Demand for Electric Cars.” <em>Journal of Econometrics</em> 17 (1): 1–19. <a href="https://doi.org/https://doi.org/10.1016/0304-4076(81)90056-7">https://doi.org/https://doi.org/10.1016/0304-4076(81)90056-7</a>.</p>
</div>
<div id="ref-R-shiny">
<p>Chang, Winston, Joe Cheng, JJ Allaire, Carson Sievert, Barret Schloerke, Yihui Xie, Jeff Allen, Jonathan McPherson, Alan Dipert, and Barbara Borges. 2021. <em>Shiny: Web Application Framework for R</em>. <a href="https://CRAN.R-project.org/package=shiny">https://CRAN.R-project.org/package=shiny</a>.</p>
</div>
<div id="ref-R-cmdstanr">
<p>Gabry, Jonah, and Rok Češnovar. 2020. <em>Cmdstanr: R Interface to ’Cmdstan’</em>.</p>
</div>
<div id="ref-R-bayesplot">
<p>Gabry, Jonah, Daniel Simpson, Aki Vehtari, Michael Betancourt, and Andrew Gelman. 2019. “Visualization in Bayesian Workflow.” <em>J. R. Stat. Soc. A</em> 182 (2): 389–402. <a href="https://doi.org/10.1111/rssa.12378">https://doi.org/10.1111/rssa.12378</a>.</p>
</div>
<div id="ref-gakisetal2018">
<p>Gakis, Konstantinos, Panos Pardalos, Chang-Hwan Choi, Jae-Hyeon Park, and Jiwun Yoon. 2018. “Simulation of a Probabilistic Model for Multi-Contestant Races.” <em>Athens Journal of Sports</em> 5 (2): 95–114.</p>
</div>
<div id="ref-R-purrr">
<p>Henry, Lionel, and Hadley Wickham. 2020. <em>Purrr: Functional Programming Tools</em>. <a href="https://CRAN.R-project.org/package=purrr">https://CRAN.R-project.org/package=purrr</a>.</p>
</div>
<div id="ref-R-rcorpora">
<p>Kazemi, Darius, Cole Willsea, Serin Delaunay, Karl Swedberg, Matthew Rothenberg, Greg Kennedy, Nathaniel Mitchell, et al. 2018. <em>Rcorpora: A Collection of Small Text Corpora of Interesting Data</em>. <a href="https://CRAN.R-project.org/package=rcorpora">https://CRAN.R-project.org/package=rcorpora</a>.</p>
</div>
<div id="ref-Luce1959">
<p>Luce, R. Duncan. 1959. <em>Individual Choice Behavior : A Theoretical Analysis</em>. Book. Wiley N.Y.</p>
</div>
<div id="ref-Plackett">
<p>Plackett, R. L. 1975. “The Analysis of Permutations.” <em>Journal of the Royal Statistical Society. Series C (Applied Statistics)</em> 24 (2): 193–202. <a href="http://www.jstor.org/stable/2346567">http://www.jstor.org/stable/2346567</a>.</p>
</div>
<div id="ref-R-base">
<p>R Core Team. 2020. <em>R: A Language and Environment for Statistical Computing</em>. Vienna, Austria: R Foundation for Statistical Computing. <a href="https://www.R-project.org/">https://www.R-project.org/</a>.</p>
</div>
<div id="ref-R-ggplot2">
<p>Wickham, Hadley. 2016. <em>Ggplot2: Elegant Graphics for Data Analysis</em>. Springer-Verlag New York. <a href="https://ggplot2.tidyverse.org">https://ggplot2.tidyverse.org</a>.</p>
</div>
<div id="ref-R-dplyr">
<p>Wickham, Hadley, Romain François, Lionel Henry, and Kirill Müller. 2021. <em>Dplyr: A Grammar of Data Manipulation</em>. <a href="https://CRAN.R-project.org/package=dplyr">https://CRAN.R-project.org/package=dplyr</a>.</p>
</div>
<div id="ref-R-extraDistr">
<p>Wolodzko, Tymoteusz. 2020. <em>ExtraDistr: Additional Univariate and Multivariate Distributions</em>. <a href="https://CRAN.R-project.org/package=extraDistr">https://CRAN.R-project.org/package=extraDistr</a>.</p>
</div>
<div id="ref-R-mediumr">
<p>Yutani, Hiroaki. 2021. <em>Mediumr: R Interface to ’Medium’ Api</em>.</p>
</div>
</div>
</div>
