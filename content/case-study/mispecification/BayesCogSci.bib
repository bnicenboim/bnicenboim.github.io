% Encoding: UTF-8

@article{schad2022data,
  title={Data aggregation can lead to biased inferences in {B}ayesian linear mixed models and {B}ayesian analysis of variance},
  author={Schad, Daniel J. and Nicenboim, Bruno and Vasishth, Shravan},
  journal={Psychological Methods},
  doi = {https://doi.org/10.1037/met0000621},
  pdf = {arXiv:2203.02361},
  year={2023}
}

@article{vstrumbelj2024past,
  title={Past, Present and Future of Software for {B}ayesian Inference},
  author={{\v{S}}trumbelj, Erik and Bouchard-C{\^o}t{\'e}, Alexandre and Corander, Jukka and Gelman, Andrew and Rue, H{\aa}vard and Murray, Lawrence and Pesonen, Henri and Plummer, Martyn and Vehtari, Aki},
  journal={Statistical Science},
  volume={39},
  number={1},
  pages={46--61},
  year={2024},
  publisher={Institute of Mathematical Statistics}
}


@book{spivak,
		  title={Calculus},
		  author={M. Spivak},
		  year={2010},
		  publisher={Cambridge}
		}

@book{adams2018calculus,
  title={Calculus: {A} complete course},
  author={Adams, Robert A. and Essex, Christopher},
  year={2018},
  publisher={Pearson}
}

@book{resnick2019probability,
  title={A probability path},
  author={Resnick, Sidney},
  year={2019},
  publisher={Springer}
}


@article{feldman2017true,
      title={What are the “true” statistics of the environment?},
      author={Feldman, Jacob},
      journal={Cognitive Science},
      volume={41},
      number={7},
      pages={1871--1903},
      doi = {https://doi.org/10.1111/cogs.12444},
      year={2017},
      publisher={Wiley Online Library}
}

@article{jurafsky96,
	Author = {Daniel Jurafsky},
	Journal = {Cognition},
	Pages = {137--194},
	Title = {A Probabilistic Model of Lexical and Syntactic Access and Disambiguation},
	Volume = {20},
    doi = {https://doi.org/10.1207/s15516709cog2002_1},
	Year = {1996}}

@phdthesis{frazier79,
	Address = {Amherst},
	Author = {Frazier, Lyn},
	School = {University of Massachusetts},
	Title = {On Comprehending Sentences: {S}yntactic Parsing Strategies},
	Year = 1979}


@article{box1964analysis,
	  title={An analysis of transformations},
	  author={Box, George E.P. and Cox, David R.},
	  journal={Journal of the Royal Statistical Society. Series B (Methodological)},
	  pages={211--252},
	  year={1964},
	  publisher={JSTOR}
	}

@article{szollosi2019preregistration,
  title={Is Preregistration Worthwhile?},
  author={Szollosi, Aba and Kellen, David and Navarro, Danielle J. and Shiffrin, Richard M and van Rooij, Iris and Van Zandt, Trisha and Donkin, Christopher},
  journal={Trends in Cognitive Sciences},
  volume={24},
  number={2},
  pages={94--95},
  year={2020},
  publisher={Elsevier}
}

@article{VasishthEtAlTiCS2019,
title={Computational models of retrieval processes in sentence processing},
  author={Shravan Vasishth and Bruno Nicenboim and Felix Engelmann and Frank Burchert},
  journal = {Trends in Cognitive Sciences},
  year = {2019},
  volume = {23},
  OPTissue = {11},
  pages = {968--982},
  pdf = {https://psyarxiv.com/e4jds},
  code = {https://osf.io/qtvxj/},
  doi = {https://doi.org/10.1016/j.tics.2019.09.003}
  }

  @book{VasishthEngelmann2022,
    title={Sentence Comprehension as a Cognitive Process: {A} Computational Approach},
    author={Vasishth, Shravan and Engelmann, Felix},
    isbn={9781107133112},
    code = {https://vasishth.github.io/RetrievalModels/},
    url={https://books.google.de/books?id=6KZKzgEACAAJ},
    year={2022},
    publisher={Cambridge University Press},
    address = {Cambridge, UK}
  }

  @article{JaegerMertzenVanDykeVasishth2019,
    Author = {J\"ager, Lena A. and Mertzen, Daniela and Van Dyke, Julie A. and Vasishth, Shravan},
    Title = {Interference patterns in subject-verb agreement and reflexives revisited: {A} large-sample study},
    Year = {2020},
    volume = {111},
    journal = {Journal of Memory and Language},
    code = {https://osf.io/reavs/},
    doi = {https://doi.org/10.1016/j.jml.2019.104063}
    }

    @article{stoneNOL2021,
    title={Understanding the effects of constraint and predictability in {ERP}},
    journal = {Neurobiology of Language},
    year = {2023},
    code = {https://osf.io/fndk5/},
    DOI = {10.1162/nol_a_00094},
    author={Kate Stone and Bruno Nicenboim and Shravan Vasishth and Frank Roesler}
    }


@article{VasishthMertzenJaegerGelman2018,
      Author = {Vasishth, Shravan and Mertzen, Daniela and J{\"a}ger, Lena A. and Gelman, Andrew},
      journal = {Journal of Memory and Language},
      url = {https://osf.io/eyphj/},
      doi = {https://doi.org/10.1016/j.jml.2018.07.004},
      Title = {The statistical significance filter leads to overoptimistic expectations of replicability},
      Year = {2018},
      volume = {103},
      pages = {151-175}
      }


@book{johnson,
  title={Continuous univariate distributions},
  volume = {2},
  author={Johnson, Norman L and Kotz, Samuel and Balakrishnan, Narayanaswamy},
  volume={289},
  year={1995},
  publisher={John Wiley \& Sons}
}

@article{lindgren2015bayesian,
  title={Bayesian spatial modelling with {R-INLA}},
  author={Lindgren, Finn and Rue, H{\aa}vard},
  journal={Journal of Statistical Software},
  volume={63},
  number={1},
  pages={1--25},
  doi = { 10.18637/jss.v063.i19},
  year={2015}
}


@article{van2021bayes,
  title={Bayes factors for mixed models},
  author={{van~Doorn}, Johnny and Aust, Frederik and Haaf, Julia M. and Stefan, Angelika and Wagenmakers, Eric-Jan},
  year={2021},
  doi = {https://doi.org/10.1007/s42113-021-00113-2},
  journal={Computational Brain and Behavior}
}

@article{kruschke2018bayesian,
  title={The {B}ayesian New Statistics: {H}ypothesis testing, estimation, meta-analysis, and power analysis from a {B}ayesian perspective},
  author={Kruschke, John K. and Liddell, Torrin M.},
  journal={Psychonomic Bulletin \& Review},
  volume={25},
  number={1},
  pages={178--206},
  doi = {https://doi.org/10.3758/s13423-016-1221-4},
  year={2018},
  publisher={Springer}
}


@article{nosek2019preregistration,
  title={Preregistration is hard, and worthwhile},
  author={Nosek, Brian A. and Beck, Emorie D and Campbell, Lorne and Flake, Jessica K and Hardwicke, Tom E and Mellor, David T and van’t Veer, Anna E and Vazire, Simine},
  journal={Trends in Cognitive Sciences},
  volume={23},
  number={10},
  pages={815--818},
  year={2019},
  publisher={Elsevier}
}


@article{lee2020application,
  title={An application of multinomial processing tree models and {B}ayesian methods to understanding memory impairment},
  author={Lee, Michael D. and Bock, Jason R. and Cushman, Isaiah and Shankle, William R.},
  journal={Journal of Mathematical Psychology},
  volume={95},
  pages={102328},
  year={2020},
  doi = {https://doi.org/10.1016/j.jmp.2020.102328},
  publisher={Elsevier}
}

@book{fieller,
  Address = {Boca Raton, FL},
  Author = {Nick Fieller},
  Publisher = {CRC Press},
  Title = {Basics of matrix algebra for statistics with {R}},
  Year = {2016}}


@article{carney2010power,
  title={Power posing: {B}rief nonverbal displays affect neuroendocrine levels and risk tolerance},
  author={Carney, Dana R. and Cuddy, Amy J.C. and Yap, Andy J.},
  journal={Psychological Science},
  volume={21},
  number={10},
  pages={1363--1368},
  year={2010},
  publisher={Sage Publications Sage},
  address = {Los Angeles, CA}
}

@misc{FossePowerPose,
author = {Fosse, Nathan E.},
publisher = {Harvard Dataverse},
title = {{Replication Data for "Power Posing: Brief Nonverbal Displays Affect Neuroendocrine Levels and Risk Tolerance" by Carney, Cuddy, Yap (2010)}},
UNF = {UNF:6:5wXH4UJhgg+HYYIFeAP4aQ==},
year = {2016},
version = {V3},
doi = {10.7910/DVN/FMEGS6},
url = {https://doi.org/10.7910/DVN/FMEGS6}
}


@article{smith2010beta,
  title={Beta-{MPT}: {M}ultinomial processing tree models for addressing individual differences},
  author={Smith, Jared B. and Batchelder, William H.},
  journal={Journal of Mathematical Psychology},
  volume={54},
  number={1},
  pages={167--183},
  year={2010},
  publisher={Elsevier}
}

@book{mahajan2010street,
  title={Street-fighting mathematics: {T}he art of educated guessing and opportunistic problem solving},
  author={Mahajan, Sanjoy},
  year={2010},
  publisher={The MIT Press},
  address = {Cambridge, MA}
}

@book{mahajan2014art,
  title={The art of insight in science and engineering: mastering complexity},
  author={Mahajan, Sanjoy},
  year={2014},
  publisher={The MIT Press},
 address = {Cambridge, MA}
}

@incollection{box1979robustness,
  Author = {Box, George E.P.},
  Booktitle = {Robustness in statistics},
  Date-Added = {2020-02-03 19:20:46 +0100},
  Date-Modified = {2020-02-03 19:23:56 +0100},
  Pages = {201--236},
  Publisher = {Elsevier},
  Title = {Robustness in the strategy of scientific model building},
  Year = {1979}}

  @article{SchadEtAlBF,
    Author = {Daniel J. Schad and Bruno Nicenboim and Paul-Christian B{\"u}rkner and Michael Betancourt and Shravan Vasishth},
    Title = {Workflow Techniques for the Robust Use of {B}ayes Factors},
    Year = {2022},
    journal = {Psychological Methods},
    pdf = {https://arxiv.org/abs/2103.08744v2},
    code = {https://osf.io/y354c/},
    doi = {10.1037/met0000472}
  }


@Manual{designr,
    title = {designr: Balanced Factorial Designs},
    author = {Maximilian M. Rabe and Reinhold Kliegl and Daniel J. Schad},
    year = {2021},
    note = {R package version 0.1.11},
    url = {https://maxrabe.com/designr},
}


@Manual{rethinking,
    title = {rethinking: Statistical Rethinking book package},
    author = {Richard McElreath},
    year = {2020},
    note = {R package version 2.13},
  }

@Manual{extraDistr,
    title = {extraDistr: Additional Univariate and Multivariate Distributions},
    author = {Tymoteusz Wolodzko},
    year = {2020},
    note = {R package version 1.9.1},
    url = {https://CRAN.R-project.org/package=extraDistr},
  }


@article{schad2020toward,
  title={Toward a principled {Bayesian} workflow in cognitive science},
  author={Schad, Daniel J. and Betancourt, Michael J. and Vasishth, Shravan},
  journal={Psychological Methods},
  year={2020},
  publisher={American Psychological Association},
  Volume = {26},
  Number = {1},
  doi = {https://doi.org/10.1037/met0000275},
  Pages = {103--126}
}

@article{schad2019towardarXiv,
  doi = {10.48550/ARXIV.1904.12765},

  url = {https://arxiv.org/abs/1904.12765},
  Journal = {arXiv preprint},
  author = {Schad, Daniel J. and Betancourt, Michael J. and Vasishth, Shravan},

  keywords = {Methodology (stat.ME), FOS: Computer and information sciences, FOS: Computer and information sciences},

  title = {Toward a principled {Bayesian} workflow in cognitive science},

  publisher = {arXiv},

  year = {2019},

  copyright = {Creative Commons Attribution 4.0 International}
}


@book{baguley2012serious,
  title={Serious stats: {A} guide to advanced statistics for the behavioral sciences},
  author={Baguley, Thomas},
  year={2012},
  publisher={Macmillan International Higher Education}
}


@manual{RCoreTeam:2016aa,
  Address = {Vienna, Austria},
  Author = {{R Core Team}},
  Date-Added = {2018-12-20 16:53:38 +0100},
  Date-Modified = {2018-12-20 16:54:31 +0100},
  Organization = {R Foundation for Statistical Computing},
  Title = {R: A Language and Environment for Statistical Computing},
  Year = {2016}
}

@article{Gabry:2017aa,
  Author = {Gabry, Jonah and Simpson, Daniel P. and Vehtari, Aki and Betancourt, Michael J. and Gelman, Andrew},
  Date-Added = {2018-12-20 16:41:09 +0100},
  Date-Modified = {2018-12-20 16:41:53 +0100},
  Journal = {arXiv preprint arXiv:1709.01449},
  Title = {Visualization in {B}ayesian workflow},
  Year = {2017}
}

@misc{Betancourt:2018aa,
  Author = {Betancourt, Michael J.},
  Month = {October},
  Title = {Towards A Principled {B}ayesian Workflow},
 howpublished = {\url{https://betanalpha.github.io/assets/case_studies/principled_bayesian_workflow.html}},
  Year = {2018},
  note = {Accessed: 2024-02-08}
}

@article{gelman2017prior,
  Author = {Gelman, Andrew and Simpson, Daniel P. and Betancourt, Michael J.},
  Date-Added = {2020-02-19 20:28:35 +0100},
  Date-Modified = {2020-02-19 20:29:59 +0100},
  Journal = {Entropy},
  Number = {10},
  Pages = {555},
  Title = {The prior can often only be understood in the context of the likelihood},
  Volume = {19},
  Year = {2017}
}


@book{lee2012bayesian,
  title={Bayesian statistics: {A}n introduction},
  author={Lee, Peter M},
  year={2012},
  publisher={John Wiley \& Sons}
}


@article{lee2019robust,
  Author = {Lee, Michael D. and Criss, Amy H and Devezer, Berna and Donkin, Christopher and Etz, Alexander and Leite, Fábio P. and Matzke, Dora and Rouder, Jeffrey N. and Trueblood, Jennifer S. and White, Corey N. and others},
  Date-Added = {2020-02-07 14:30:12 +0100},
  Date-Modified = {2020-02-07 14:30:54 +0100},
  Journal = {Computational Brain \& Behavior},
  Number = {3-4},
  Pages = {141--153},
  Title = {Robust modeling in cognitive science},
  Volume = {2},
  Year = {2019}
}

@book{Good:1950aa,
  Author = {Good, I. J.},
  Date-Added = {2018-12-12 11:53:08 +0100},
  Date-Modified = {2018-12-12 11:54:45 +0100},
  Publisher = {New York: Hafners},
  Title = {Probability and the weighing of evidence},
  Year = {1950}
}



@article{ratcliff2000diffusion,
  Author = {Ratcliff, Roger and Rouder, Jeffrey N.},
  Date-Added = {2020-01-16 17:25:19 +0100},
  Date-Modified = {2020-01-16 17:26:24 +0100},
  Journal = {Journal of Experimental Psychology: Human Perception and Performance},
  Number = {1},
  Pages = {127},
  Title = {A diffusion model account of masking in two-choice letter identification.},
  Volume = {26},
  Year = {2000}
}


@article{Spiegelhalter:2014aa,
  Author = {Spiegelhalter, David J. and Best, Nichola G. and Carlin, Bradley P., and Linde, Angelika},
  Date-Added = {2019-01-22 19:47:28 +0100},
  Date-Modified = {2019-02-07 13:39:34 +0100},
  Journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  Number = {3},
  Pages = {485--493},
  Title = {The {Deviance Information Criterion}: 12 years on},
  Volume = {76},
  Year = {2014}
}


@book{danlevy,
  title={Maxims for Thinking Analytically: {T}he Wisdom of Legendary {H}arvard {Professor Richard Zeckhauser}},
  author={Dan Levy},
  year={2021},
  publisher={Dan Levy}
}

@book{tetlock,
  title={Superforecasting: {T}he Art and Science of Prediction},
  author={Philip Tetlock and Dan Gardner},
  year={2015},
  publisher={Crown Publishers}
}

@book{gamerman,
  title={{Markov chain Monte Carlo: Stochastic simulation for Bayesian inference}},
  author={Gamerman, Dani and Lopes, Hedibert F.},
  year={2006},
  publisher={CRC Press}
}

@article{robert202250,
  title={50 shades of Bayesian testing of hypotheses},
  author={Robert, Christian P.},
  journal={arXiv preprint arXiv:2206.06659},
  year={2022}
}


@book{bernardosmith,
  title={Bayesian theory},
  author={Bernardo, Jos{\'e} M. and Smith, Adrian F.M.},
  volume={405},
  year={2009},
  publisher={John Wiley \& Sons}
}

@misc{kendall2004,
  title={Kendall's Advanced Theory of Statistics, Vol. 2B: Bayesian Inference},
  author={O'Hagan, Anthony and Forster, Jonathan J.},
  year={2004},
  edition = {Second},
  publisher={Wiley}
}

@article{kass1989investigating,
  title={Investigating therapies of potentially great benefit: {ECMO}: {C}omment: {A} {B}ayesian perspective},
  author={Kass, Robert E. and Greenhouse, Joel B.},
  journal={Statistical Science},
  volume={4},
  number={4},
  pages={310--317},
  doi = {https://www.jstor.org/stable/2245831},
  year={1989},
  publisher={JSTOR}
}

@article{gibson2013need,
  title={The need for quantitative methods in syntax and semantics research},
  author={Gibson, Edward and Fedorenko, Evelina},
  journal={Language and Cognitive Processes},
  volume={28},
  number={1-2},
  pages={88--124},
  year={2013},
  publisher={Taylor \& Francis}
}

@article{fedorenko2006nature,
  title={The nature of working memory capacity in sentence comprehension: {E}vidence against domain-specific working memory resources},
  author={Fedorenko, Evelina and Gibson, Edward and Rohde, Douglas},
  journal={Journal of memory and language},
  volume={54},
  number={4},
  pages={541--553},
  doi = {https://doi.org/10.1016/j.jml.2005.12.006},
  year={2006},
  publisher={Elsevier}
}


@article{rayner1998emr,
  Author = {Rayner, K.},
  Journal = {Psychological Bulletin},
  Number = {3},
  Pages = {372--422},
  Title = {{Eye movements in reading and information processing: 20 years of research}},
  Volume = {124},
  doi = {https://doi.org/10.1037/0033-2909.124.3.372},
  Year = {1998}}

@article{von1988fermi,
  title={How {F}ermi would have fixed it},
  author={Von Baeyer, Hans Christian},
  journal={The Sciences},
  volume={28},
  number={5},
  pages={2--4},
  year={1988},
  publisher={Blackwell Publishing Ltd Oxford, UK}
}

@article{laird1982random,
  title={Random-effects models for longitudinal data},
  author={Laird, Nan M. and Ware, James H.},
  journal={Biometrics},
  pages={963--974},
  year={1982},
  doi = {https://doi.org/10.2307/2529876},
  publisher={JSTOR}
}

@article{Vehtari:2017aa,
  Author = {Vehtari, Aki and Gelman, Andrew and Gabry, Jonah},
  Date-Added = {2019-01-22 19:45:10 +0100},
  Date-Modified = {2019-01-22 19:46:00 +0100},
  Journal = {Statistics and Computing},
  Number = {5},
  Pages = {1413--1432},
  Title = {Practical {Bayesian model evaluation using leave-one-out cross-validation and WAIC}},
  Volume = {27},
  Year = {2017}
}

@book{rosenthal2000contrasts,
  title={Contrasts and effect sizes in behavioral research: {A} correlational approach},
  author={Rosenthal, Robert and Rosnow, Ralph L. and Rubin, Donald B.},
  year={2000},
  publisher={Cambridge University Press}
}

@article{Gentle,
  title={Matrix algebra: {T}heory, Computations, and Applications in Statistics},
  author={Gentle, James E},
  journal={Springer Texts in Statistics},
  volume={10},
  year={2007},
  publisher={Springer},
  address = {New York, NY}
}


@article{shiffrin2008survey,
  Author = {Shiffrin, Richard M and Lee, Michael D. and Kim, Woojae and Wagenmakers, Eric-Jan},
  Date-Added = {2020-01-15 21:01:09 +0100},
  Date-Modified = {2020-02-04 21:17:09 +0100},
  Journal = {Cognitive Science},
  Number = {8},
  Pages = {1248--1284},
  Title = {A survey of model evaluation approaches with a tutorial on hierarchical {B}ayesian methods},
  Volume = {32},
  Year = {2008}
}



@article{Paape:2017aa,
  Author = {Paape, Dario and Nicenboim, Bruno and Vasishth, Shravan},
  Date-Added = {2019-01-23 14:32:52 +0100},
  Date-Modified = {2019-01-23 14:33:25 +0100},
  Journal = {Glossa: {A} journal of general linguistics},
  Number = {1},
  Title = {Does antecedent complexity affect ellipsis processing? An empirical investigation},
  Volume = {2},
  Year = {2017}
}

xxx
@article{Vasishth2018aa,
  Author = {Vasishth, Shravan and Mertzen, Daniela and J{\"a}ger, Lena A. and Gelman, Andrew},
  Journal = {Journal of Memory and Language},
  Pages = {151--175},
  Title = {The statistical significance filter leads to overoptimistic expectations of replicability},
  Volume = {103},
  Year = {2018}
}

@article{Lewandowski:2009aa,
  Author = {Lewandowski, Daniel and Kurowicka, Dorota and Joe, Harry},
  Date-Added = {2018-12-17 18:39:26 +0100},
  Date-Modified = {2018-12-17 18:40:26 +0100},
  Journal = {Journal of Multivariate Analysis},
  Number = {9},
  Pages = {1989--2001},
  Title = {Generating random correlation matrices based on vines and extended onion method},
  Volume = {100},
  Year = {2009}
}

@article{fisher2022fundamental,
  title={Fundamental tools for developing likelihood functions within {ACT-R}},
  author={Fisher, Christopher R. and Houpt, Joseph W. and Gunzelmann, Glenn},
  journal={Journal of Mathematical Psychology},
  volume={107},
  pages={102636},
  doi = {https://doi.org/10.1016/j.jmp.2021.102636},
  year={2022},
  publisher={Elsevier}
}

@article{wang2002simulation,
  title={A simulation-based approach to {B}ayesian sample size determination for performance under a given model and for separating models},
  author={Wang, Fei and Gelfand, Alan E},
  journal={Statistical Science},
  pages={193--208},
  year={2002},
  publisher={JSTOR}
}


@article{SampleSizeCBB2021,
title={Sample size determination for {B}ayesian hierarchical models commonly used in psycholinguistics},
  author={Shravan Vasishth and Himanshu Yadav and Daniel J. Schad and Bruno Nicenboim},
  year = {2022},
  code = {https://osf.io/hjgrm/},
  pdf = {https://psyarxiv.com/u8yvc},
  journal = {Computational Brain and Behavior},
  note = {Accepted}
  }

  @book{parmigiani2009decision,
   title={Decision theory: Principles and approaches},
   author={Parmigiani, Giovanni and Inoue, Lurdes},
   year={2009},
   publisher={John Wiley \& Sons}
 }


@article{VasishthetalPLoSOne2013,
  Author = {Vasishth, Shravan and Chen, Zhong and Li, Qiang and Guo, Gueilan},
  Code = {http://www.ling.uni-potsdam.de/~vasishth/code/PLoSOneVasishthetaldata.zip},
  Date-Modified = {2019-02-06 16:20:44 +0100},
  Journal = {PLoS ONE},
  Month = {10},
  Number = {10},
  Pages = {1--14},
  Publisher = {Public Library of Science},
  Title = {Processing {C}hinese relative clauses: {E}vidence for the subject-relative advantage},
  Volume = {8},
  Year = {2013}
}

@article{VBLD07,
  Author = {Shravan Vasishth and Sven Bruessow and Richard L. Lewis and Heiner Drenhaus},
  Date-Modified = {2009-08-21 11:25:25 +0200},
  Journal = {Cognitive Science},
  Number = {4},
  Title = {Processing Polarity: {H}ow the ungrammatical intrudes on the grammatical},
  abstract = {A central question in online human sentence comprehension is: how are linguistic
  relations established between different parts of a sentence? Previous work has shown that
  this dependency resolution process can be computationally expensive, but the underlying
  reasons for this are still unclear. We argue that dependency resolution is mediated by
  cue-based retrieval, constrained by independently motivated working memory principles
  defined in a cognitive architecture (ACT-R). To demonstrate this, we investigate an
  unusual instance of dependency resolution, the processing of negative and positive
  polarity items, and confirm a surprising prediction of the cue-based retrieval model:
  partial cue-matches, which constitute a kind of similarity-based interference, can give
  rise to the intrusion of ungrammatical retrieval candidates, leading to both processing
  slow-downs and even errors of judgment that take the form of illusions of grammaticality
  in patently ungrammatical structures. A notable achievement is that good quantitative
  fits are achieved without adjusting the key model parameters.},
  pdf = {http://www.ling.uni-potsdam.de/~vasishth/pdfs/Vasishth-Bruessow-Lewis-Drenhaus-CogSci2008.pdf},
  Volume = {32},
  issue = {4},
  Year = {2008},
  pages = {685--712},
  code = {https://github.com/vasishth/ProcessingPolarity}
  }


@article{SafaviEtAlFrontiers2016,
  Author = {Molood Sadat Safavi and Samar Husain and Shravan Vasishth},
  journal = {Frontiers in Psychology},
  Title = {Dependency resolution difficulty increases with distance in {P}ersian separable complex predicates: Implications for expectation and memory-based accounts},
  OPTnote = {Special Issue on Encoding and Navigating Linguistic Representations in Memory},
  pdf = {http://journal.frontiersin.org/article/10.3389/fpsyg.2016.00403/full},
  doi = {10.3389/fpsyg.2016.00403},
  code={https://github.com/vasishth/SafaviEtAl2016},
  year = {2016},
  volume = {7},
  issue = {403},
  OPTdoi = {10.3389/fpsyg.2016.00403},
    abstract = {Delaying the appearance of a verb in a noun-verb dependency tends to increase processing difficulty at the verb; one explanation for this locality effect is decay and/or interference of the noun in working memory. Surprisal, an expectation-based account, predicts that delaying the appearance of a verb either renders it no more predictable or more predictable, leading respectively to a prediction of no effect of distance or a facilitation.  Recently, Husain et al (2014) suggested that when the exact identity of the upcoming verb is predictable (strong predictability), increasing argument-verb distance leads to facilitation effects, which is consistent with surprisal; but when the exact identity of the upcoming verb is not predictable (weak predictability), locality effects are seen. We investigated Husain et al.'s proposal using Persian complex predicates (CPs), which consist of a non-verbal element---a noun in the current study---and a verb. In CPs, once the noun has been read, the exact identity of the verb is highly predictable (strong predictability); this was confirmed using a sentence completion study. In two self-paced reading (SPR) and two eye-tracking (ET) experiments, we delayed the appearance of the verb by interposing a relative clause (Expt. 1 and 3) or a long PP (Expt. 2 and 4).
We also included a simple Noun-Verb predicate configuration with the same distance manipulation; here, the exact identity of the verb was not predictable (weak predictability). Thus, the design crossed Predictability Strength and Distance.  We found that, consistent with surprisal, the verb in the strong predictability conditions was read faster than in the weak predictability conditions. Furthermore, greater verb-argument distance led to slower reading times;  strong predictability did not neutralize or attenuate the locality effects. As regards the effect of distance on dependency resolution difficulty, these four experiments  present evidence in favor of  working memory accounts of argument-verb dependency resolution, and against the surprisal-based expectation account of Levy (2008). However, another expectation-based measure, entropy, which was computed using the offline sentence completion data, predicts reading times in Experiment 1. We suggest that forgetting due to memory overload leads to greater entropy at the verb.}
}


@book{venablesripley,
  Address = {New York},
  Annote = {Terse and tight-lipped. Assumes that you know a lot. It has a brief but excellent section on lme.},
  Author = {William N. Venables and Brian D. Ripley},
  Publisher = {Springer},
  Title = {Modern Applied Statistics with {S-PLUS}},
  Year = {2002}}


@article{vehtari2019rank,
  Author = {Vehtari, Aki and Gelman, Andrew and Simpson, Daniel P. and Carpenter, Bob and B{\"u}rkner, Paul-Christian},
  Date-Added = {2020-01-13 15:56:25 +0100},
  Date-Modified = {2020-02-04 21:19:28 +0100},
  Journal = {arXiv preprint arXiv:1903.08008},
  Title = {Rank-normalization, folding, and localization: An improved $\widehat R$ for assessing convergence of {MCMC}},
  Year = {2019}
}


@book{kmenta1971elements,
  Author = {Kmenta, Jan},
  Date-Added = {2020-01-13 11:30:37 +0100},
  Date-Modified = {2020-01-13 11:31:59 +0100},
  Publisher = {New York: Macmillan; London: Collier Macmillan},
  Title = {Elements of econometrics},
  Year = {1971}
}

@article{link2012thinning,
  Author = {Link, William A and Eaton, Mitchell J},
  Date-Added = {2020-01-13 18:36:43 +0100},
  Date-Modified = {2020-02-04 21:13:42 +0100},
  Journal = {Methods in Ecology and Evolution},
  Number = {1},
  Pages = {112--115},
  Title = {On thinning of chains in {MCMC}},
  Volume = {3},
  Year = {2012}
}

@article{ly2016harold,
  Author = {Ly, Alexander and Verhagen, Josine and Wagenmakers, Eric-Jan},
  Date-Added = {2020-02-04 22:00:06 +0100},
  Date-Modified = {2020-02-04 22:01:48 +0100},
  Journal = {Journal of Mathematical Psychology},
  Pages = {19--32},
  Title = {Harold {J}effreys's default {B}ayes factor hypothesis tests: Explanation, extension, and application in psychology},
  Volume = {72},
  Year = {2016}
}

@article{mulder2016editors,
  Author = {Mulder, Joris and Wagenmakers, Eric-Jan},
  Date-Added = {2020-01-19 17:39:41 +0100},
  Date-Modified = {2020-02-04 21:14:56 +0100},
  Journal = {Journal of Mathematical Psychology},
  Pages = {1--5},
  Title = {Editors' introduction to the special issue ``{B}ayes factors for testing hypotheses in psychological research: Practical relevance and new developments''},
  Volume = {72},
  Year = {2016}
}


@article{vandekerckhove2018bayesian,
  Author = {Vandekerckhove, Joachim and Rouder, Jeffrey N. and Kruschke, John K.},
  Date-Added = {2020-01-19 18:41:49 +0100},
  Date-Modified = {2020-01-19 18:51:34 +0100},
  Journal = {Psychonomic Bulletin \& Review},
  Number = {1},
  Pages = {1--4},
  Title = {Bayesian methods for advancing psychological science},
  Volume = {25},
  Year = {2018}
}

@book{lindley2013understanding,
  title={Understanding uncertainty},
  author={Lindley, Dennis V},
  year={2013},
  publisher={John Wiley \& Sons}
}

@book{lindley1991,
  title={Making decisions},
  author={Lindley, Dennis V},
  edition = {Second},
  year={1991},
  publisher={John Wiley \& Sons}
}


@article{lindley1957statistical,
  Author = {Lindley, Dennis V},
  Date-Added = {2020-01-16 22:25:44 +0100},
  Date-Modified = {2020-01-16 22:26:20 +0100},
  Journal = {Biometrika},
  Number = {1/2},
  Pages = {187--192},
  Title = {A statistical paradox},
  Volume = {44},
  Year = {1957}
}


@article{gelman2020bayesian,
  title={Bayesian workflow},
  author={Gelman, Andrew and Vehtari, Aki and Simpson, Daniel P. and Margossian, Charles C. and Carpenter, Bob and Yao, Yuling and Kennedy, Lauren and Gabry, Jonah and B{\"u}rkner, Paul-Christian and Modr{\'a}k, Martin},
  journal={arXiv preprint arXiv:2011.01808},
  year={2020}
}

@Article{stantargets,
     title = {The stantargets {R} package: a workflow framework for efficient reproducible {S}tan-powered {B}ayesian data analysis pipelines},
     author = {William Michael Landau},
     journal = {Journal of Open Source Software},
     year = {2021},
     volume = {6},
     number = {60},
     pages = {3193},
     url = {https://doi.org/10.21105/joss.03193},
  }

@article{wilson2017good,
  title={Good enough practices in scientific computing},
  author={Wilson, Greg and Bryan, Jennifer and Cranston, Karen and Kitzes, Justin and Nederbragt, Lex and Teal, Tracy K.},
  journal={PLoS computational biology},
  volume={13},
  number={6},
  pages={e1005510},
  year={2017},
  publisher={Public Library of Science San Francisco, CA USA}
}

@phdthesis{dillon2011structured,
  title={Structured access in sentence comprehension},
  author={Dillon, Brian W.},
  school ={University of Maryland},
  year={2011}
}


@incollection{phillips2011grammatical,
  title={Grammatical illusions and selective fallibility in real-time language comprehension},
  author={Phillips, Colin and Wagers, Matthew W. and Lau, Ellen F.},
  booktitle={Experiments at the Interfaces},
  volume={37},
  pages={147--180},
  year={2011},
  publisher={Emerald Bingley, UK}
}

@article{Dillon-EtAl-2013,
  title={Contrasting intrusion profiles for agreement and anaphora: {E}xperimental and modeling evidence},
  author={Dillon, Brian W. and Mishler, Alan and Sloggett, Shayne and Phillips, Colin},
  journal={Journal of Memory and Language},
  volume={69},
  number={2},
  doi = {https://doi.org/10.1016/j.jml.2013.04.003},
  pages={85--103},
  year={2013},
  publisher={Elsevier}
}


@book{bishop2006pattern,
  title={Pattern recognition and machine learning},
  author={Bishop, Christopher M},
  year={2006},
  publisher={Springer}
}


@article{burkner2019ordinal,
  title={Ordinal regression models in psychology: {A} tutorial},
  author={B{\"u}rkner, Paul-Christian and Vuorre, Matti},
  journal={Advances in Methods and Practices in Psychological Science},
  volume={2},
  number={1},
  pages={77--101},
  year={2019},
  doi = {https://doi.org/10.1177/2515245918823199},
  publisher={Sage Publications Sage CA: Los Angeles, CA}
}

@article{burkner2020modelling,
  title={Modelling monotonic effects of ordinal predictors in {Bayesian} regression models},
  author={B{\"u}rkner, Paul-Christian and Charpentier, Emmanuel},
  journal={British Journal of Mathematical and Statistical Psychology},
  year={2020},
  doi = {https://doi.org/10.1111/bmsp.12195},
  publisher={Wiley Online Library}
}

@Manual{friendly_matlib,
    title = {matlib: {Matrix} Functions for Teaching and Learning Linear Algebra and
Multivariate Statistics},
    author = {Michael Friendly and John Fox and Phil Chalmers},
    year = {2020},
    note = {R package version 0.9.3},
    url = {https://CRAN.R-project.org/package=matlib},
}

@article{schonbrodt2018bayes,
  Author = {Sch{\"o}nbrodt, Felix D. and Wagenmakers, Eric-Jan},
  Journal = {Psychonomic Bulletin \& Review},
  Number = {1},
  Pages = {128--142},
  Title = {Bayes factor design analysis: {Planning} for compelling evidence},
  Volume = {25},
  Year = {2018}
}


@article{kass1995bayes,
  title={Bayes factors},
  author={Kass, Robert E. and Raftery, Adrian E.},
  journal={Journal of the American Statistical Association},
  volume={90},
  number={430},
  pages={773--795},
  doi = {10.1080/01621459.1995.10476572},
  year={1995},
  publisher={Taylor \& Francis}
}


@article{hammerly2019grammaticality,
  Author = {Hammerly, Christopher and Staub, Adrian and Dillon, Brian W.},
  Journal = {Cognitive Psychology},
  Pages = {70--104},
  Title = {The grammaticality asymmetry in agreement attraction reflects response bias: {E}xperimental and modeling evidence},
  Volume = {110},
  doi = {https://doi.org/10.1016/j.cogpsych.2019.01.001},
  Year = {2019}
}

@article{turner2008bias,
  title={Bias modelling in evidence synthesis},
  author={Turner, R.M. and Spiegelhalter, David J. and Smith, G. and Thompson, Simon G.},
  journal={Journal of the Royal Statistical Society: Series A (Statistics in Society)},
  volume={172},
  number={1},
  pages={21--47},
  year={2008},
  publisher={Wiley Online Library}
}


@book{navarro2015learning,
  Author = {Navarro, Danielle J.},
  Publisher = {https://learningstatisticswithr.com},
  Title = {Learning statistics with R},
  Year = {2015}
}


@article{rouder2009bayesian,
  Author = {Rouder, Jeffrey N. and Speckman, Paul L. and Sun, Dongchu and Morey, Richard D. and Iverson, Geoffrey},
  Journal = {Psychonomic Bulletin \& Review},
  Number = {2},
  Pages = {225--237},
  Title = {Bayesian {t} tests for accepting and rejecting the null hypothesis},
  Volume = {16},
  doi = {https://doi.org/10.3758/PBR.16.2.225},
  Year = {2009}
}


@article{cumming2014new,
  Author = {Cumming, Geoff},
  Journal = {Psychological Science},
  Number = {1},
  Pages = {7--29},
  Title = {The new statistics: Why and how},
  Volume = {25},
  Year = {2014}
}

@article{DickeyLientz1970,
  Author = {Dickey, James M. and Lientz, B.P.},
  Journal = {The Annals of Mathematical Statistics},
  Number = {1},
  Pages = {214--226},
  Publisher = {Institute of Mathematical Statistics},
  Title = {The weighted likelihood ratio, sharp hypotheses about chances, the order of a Markov chain},
  Volume = {41},
  url = {https://www.jstor.org/stable/2239734},
  Year = {1970}
}


@article{gronauTutorialBridgeSampling2017,
  Author = {Gronau, Quentin F. and Sarafoglou, Alexandra and Matzke, Dora and Ly, Alexander and Boehm, Udo and Marsman, Maarten and Leslie, David S. and Forster, Jonathan J. and Wagenmakers, Eric-Jan and Steingroever, Helen},
  Year = {2017},
  Doi = {10.1016/j.jmp.2017.09.005},
  Issn = {00222496},
  Journal = {Journal of Mathematical Psychology},
  Pages = {80-97},
  Title = {A Tutorial on Bridge Sampling},
  Volume = {81}
}


@article{gronauBridgesamplingPackageEstimating2017,
  Abstract = {Statistical procedures such as Bayes factor model selection and Bayesian model averaging require the computation of normalizing constants (e.g., marginal likelihoods). These normalizing constants are notoriously difficult to obtain, as they usually involve highdimensional integrals that cannot be solved analytically. Here we introduce an R package that uses bridge sampling (Meng and Wong 1996; Meng and Schilling 2002) to estimate normalizing constants in a generic and easy-to-use fashion. For models implemented in Stan, the estimation procedure is automatic. We illustrate the functionality of the package with three examples.},
  Archiveprefix = {arXiv},
  Author = {Gronau, Quentin F. and Singmann, Henrik and Wagenmakers, Eric-Jan},
  Year = {2017},
  Eprint = {1710.08162},
  Journal = {arxiv},
  Keywords = {Statistics - Computation},
  Shorttitle = {Bridgesampling},
  Title = {Bridgesampling: {{An R Package}} for {{Estimating Normalizing Constants}}},
  Url = {http://arxiv.org/abs/1710.08162},
  Urldate = {2019-05-17},
  Bdsk-Url-1 = {http://arxiv.org/abs/1710.08162}
}


@article{kutas1980reading,
  Author = {Kutas, Marta and Hillyard, Steven A.},
  Journal = {Science},
  Title = {Reading senseless sentences: Brain potentials reflect semantic incongruity},
  Volume = {207},
  Year = {1980}
}

@article{rabovsky2018modelling,
  Author = {Rabovsky, Milena and Hansen, Steven S and McClelland, James L.},
  Journal = {Nature Human Behaviour},
  Number = {9},
  Pages = {693--705},
  Title = {Modelling the N400 brain potential as change in a probabilistic representation of meaning},
  Volume = {2},
  Year = {2018}
}

@article{kutas1984brain,
  Author = {Kutas, Marta and Hillyard, Steven A.},
  Journal = {Nature},
  Number = {5947},
  Pages = {161--163},
  Title = {Brain potentials during reading reflect word expectancy and semantic association},
  Volume = {307},
  Year = {1984}
}

@unpublished{SchadEtAlBF,
  Author = {Daniel J. Schad and Bruno Nicenboim and Paul-Christian B{\"u}rkner and Michael J. Betancourt and Shravan Vasishth},
  Title = {Workflow Techniques for the Robust Use of {Bayes} Factors},
  Year = {2021},
  pdf = {https://arxiv.org/abs/2103.08744},
  code = {https://osf.io/y354c/}
}

@article{schad2022workflow,
  title={Workflow techniques for the robust use of {B}ayes factors},
  author={Schad, Daniel J. and Nicenboim, Bruno and B{\"u}rkner, Paul-Christian and Betancourt, Michael J. and Vasishth, Shravan},
  journal={Psychological Methods},
  year={2022},
  doi = {https://doi.org/10.1037/met0000472},
  publisher={American Psychological Association}
}

@book{jeffreys1939theory,
  Author = {Harold Jeffreys},
  Publisher = {Oxford: Clarendon Press},
  Title = {Theory of probability},
  note = {Republished in 1998},
  Year = {1939}
  }


@Inbook{wagenmakersPrinciplePredictiveIrrelevance2020,
author="Wagenmakers, Eric-Jan
and Lee, Michael D.
and Rouder, Jeffrey N.
and Morey, Richard D.",
editor="Gruber, Craig W.",
title="The Principle of Predictive Irrelevance or Why Intervals Should Not be Used for Model Comparison Featuring a Point Null Hypothesis",
bookTitle="The Theory of Statistics in Psychology: Applications, Use, and Misunderstandings",
year="2020",
publisher="Springer International Publishing",
address="Cham",
pages="111--129",
doi="10.1007/978-3-030-48043-1_8",
url="https://doi.org/10.1007/978-3-030-48043-1_8"
}

@article{BlanchardEtAl2020,
    author = {Blanchard, Pierre and Higham, Desmond J. and Higham, Nicholas J.},
    title = "{Accurately computing the log-sum-exp and softmax functions}",
    journal = {IMA Journal of Numerical Analysis},
    volume = {41},
    number = {4},
    pages = {2311-2330},
    year = {2020},
    month = {08},
    abstract = "{Evaluating the log-sum-exp function or the softmax function is a key step in many modern data science algorithms, notably in inference and classification. Because of the exponentials that these functions contain, the evaluation is prone to overflow and underflow, especially in low-precision arithmetic. Software implementations commonly use alternative formulas that avoid overflow and reduce the chance of harmful underflow, employing a shift or another rewriting. Although mathematically equivalent, these variants behave differently in floating-point arithmetic and shifting can introduce subtractive cancellation. We give rounding error analyses of different evaluation algorithms and interpret the error bounds using condition numbers for the functions. We conclude, based on the analysis and numerical experiments, that the shifted formulas are of similar accuracy to the unshifted ones, so can safely be used, but that a division-free variant of softmax can suffer from loss of accuracy.}",
    issn = {0272-4979},
    doi = {10.1093/imanum/draa038},
    url = {https://doi.org/10.1093/imanum/draa038},
    eprint = {https://academic.oup.com/imajna/article-pdf/41/4/2311/40758053/draa038.pdf},
}




@article{mengSimulatingRatiosNormalizing1996,
  Abstract = {Abstract: Let pi(w),i =1, 2, be two densities with common support where each density is known up to a normalizing constant: pi(w) =qi(w)/ci. We have draws from each density (e.g., via Markov chain Monte Carlo), and we want to use these draws to simulate the ratio of the normalizing constants, c1/c2. Such a computational problem is often encountered in likelihood and Bayesian inference, and arises in fields such as physics and genetics. Many methods proposed in statistical and other literature (e.g., computational physics) for dealing with this problem are based on various special cases of the following simple identity: c1 c2 = E2[q1(w)α(w)] E1[q2(w)α(w)]. Here Ei denotes the expectation with respect to pi (i =1, 2), and α is an arbitrary function such that the denominator is non-zero. A main purpose of this paper is to provide a theoretical study of the usefulness of this identity, with focus on (asymptotically) optimal and practical choices of α. Using a simple but informative example, we demonstrate that with sensible (not necessarily optimal) choices of α, we can reduce the simulation error by orders of magnitude when compared to the conventional importance sampling method, which corresponds to α =1/q2. We also introduce several generalizations of this identity for handling more complicated settings (e.g., estimating several ratios simultaneously) and pose several open problems that appear to have practical as well as theoretical value. Furthermore, we discuss related theoretical and empirical work.},
  Author = {Meng, Xiao-li and Wong, Wing Hung},
  Year = {1996},
  Journal = {Statistica Sinica},
  Pages = {831--860},
  doi = {http://www.jstor.org/stable/24306045},
  Shorttitle = {Simulating Ratios of Normalizing Constants via a Simple Identity},
  Title = {Simulating Ratios of Normalizing Constants via a Simple Identity: {{A}} Theoretical Exploration}}

@article{bennettEfficientEstimationFree1976,
  Abstract = {Near-optimal strategies are developed for estimating the free energy difference between two canonical ensembles, given a Metropolis-type Monte Carlo program for sampling each one. The estimation strategy depends on the extent of overlap between the two ensembles, on the smoothness of the density-of-states as a function of the difference potential, and on the relative Monte Carlo sampling costs, per statistically independent data point. The best estimate of the free energy difference is usually obtained by dividing the available computer time approximately equally between the two ensembles; its efficiency (variance x computer time)-1 is never less, and may be several orders of magnitude greater, than that obtained by sampling only one ensemble, as is done in perturbation theory.},
  Author = {Bennett, Charles H},
  Year = {1976},
  Doi = {10.1016/0021-9991(76)90078-4},
  File = {/home/bruno/Zotero/storage/6R9UX665/0021999176900784.html},
  Issn = {0021-9991},
  Journal = {Journal of Computational Physics},
  Number = {2},
  Pages = {245-268},
  Shortjournal = {Journal of Computational Physics},
  Title = {Efficient Estimation of Free Energy Differences from {{Monte Carlo}} Data},
  Url = {http://www.sciencedirect.com/science/article/pii/0021999176900784},
  Urldate = {2019-09-06},
  Volume = {22},
  Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/0021999176900784},
  Bdsk-Url-2 = {https://doi.org/10.1016/0021-9991(76)90078-4}
}


@article{wagenmakers2010BayesianHypothesisTesting,
  title={Bayesian hypothesis testing for psychologists: A tutorial on the Savage--Dickey method},
  author={Wagenmakers, Eric-Jan and Lodewyckx, Tom and Kuriyal, Himanshu and Grasman, Raoul P. P. P.},
  journal={Cognitive Psychology},
  volume={60},
  number={3},
  pages={158--189},
  year={2010},
  publisher={Elsevier}
}

@article{haaf2019some,
  title={Some do and some don’t? Accounting for variability of individual difference structures},
  author={Haaf, Julia M. and Rouder, Jeffrey N.},
  journal={Psychonomic Bulletin \& Review},
  volume={26},
  number={3},
  pages={772--789},
  year={2019},
  publisher={Springer}
}


@article{rouder2018bayesian,
  Author = {Rouder, Jeffrey N. and Haaf, Julia M. and Vandekerckhove, Joachim},
  Journal = {Psychonomic Bulletin \& Review},
  Number = {1},
  Pages = {102--113},
  Title = {Bayesian inference for psychology, part {IV}: {Parameter} estimation and {Bayes} factors},
  Volume = {25},
  doi = {https://doi.org/10.3758/s13423-017-1420-7},
  Year = {2018}
}



@article{schad2018posterior,
  Author = {Schad, Daniel J. and Vasishth, Shravan},
  Journal = {arXiv preprint arXiv:1901.06889},
  Title = {The posterior probability of a null hypothesis given a statistically significant result},
  Year = {2019}
}

@Article{lme4,
  title = {Fitting Linear Mixed-Effects Models Using {lme4}},
  author = {Douglas M. Bates and Martin M{\"a}chler and Ben Bolker and Steve Walker},
  journal = {Journal of Statistical Software},
  year = {2015},
  volume = {67},
  number = {1},
  pages = {1--48},
  doi = {10.18637/jss.v067.i01}
}

@Article{deFinetti,
  title = {Funcione caratteristica di un fenomeno aleatorio},
  journal = {Atti dela Reale Accademia Nazionale dei Lincei, Serie 6. Memorie, Classe di Scienze Fisiche, Mathematice e Naturale},
  author = {Bruno {de Finetti}},
  year = {1931},
  volume = {4},
  pages = {251--299}
}

@book{pinheirobates,
  Address = {New York},
  Author = {Jos{\'e} C Pinheiro and Douglas M. Bates},
  Optannote = {Excellent introduction to nlme package, but it is at a higher level than Raudenbush and Bryk (2002).},
  Publisher = {Springer-Verlag},
  Title = {Mixed-Effects Models in {S} and {S-PLUS}},
  Year = 2000}

@unpublished{VasishthEtAlFreq2021,
  title={Linear Mixed Models for Linguistics and Psychology: {A} Comprehensive Introduction},
  author={Shravan Vasishth and Daniel J. Schad and Audrey B{\"u}rki and Reinhold Kliegl},
  year={2021},
  url = {https://vasishth.github.io/Freq_CogSci/},
  note={Under contract with Chapman and Hall/CRC Statistics in the Social and Behavioral Sciences Series}
}

@Article{VehtariLampinen2002,
  author   = {Vehtari, Aki and Lampinen, Jouko},
  title    = {Bayesian Model Assessment and Comparison Using Cross-Validation Predictive Densities},
  doi      = {10.1162/08997660260293292},
  eprint   = {https://doi.org/10.1162/08997660260293292},
  number   = {10},
  pages    = {2439-2468},
  url      = {https://doi.org/10.1162/08997660260293292},
  volume   = {14},
  abstract = {In this work, we discuss practical methods for the assessment, comparison, and selection of complex hierarchical Bayesian models. A natural way to assess the goodness of the model is to estimate its future predictive capability by estimating expected utilities. Instead of just making a point estimate, it is important to obtain the distribution of the expected utility estimate because it describes the uncertainty in the estimate. The distributions of the expected utility estimates can also be used to compare models, for example, by computing the probability of one model having a better expected utility than some other model. We propose an approach using cross-validation predictive densities to obtain expected utility estimates and Bayesian bootstrap to obtain samples from their distributions. We also discuss the probabilistic assumptions made and properties of two practical cross-validation methods, importance sampling and k-fold cross-validation. As illustrative examples, we use multilayer perceptron neural networks and gaussian processes with Markov chain Monte Carlo sampling in one toy problem and two challenging real-world problems.},
  journal  = {Neural Computation},
  year     = {2002},
}

@book{mackay,
  title={Information theory, inference and learning algorithms},
  author={MacKay, David J.C.},
  year={2003},
  publisher={Cambridge University Press},
  address = {Cambridge, UK}
}

@book{carlin2008bayesian,
  title={Bayesian methods for data analysis},
  author={Carlin, Bradley P. and Louis, Thomas A},
  year={2008},
  publisher={CRC Press}
}

@misc{christensen2011,
  title={Bayesian Ideas and Data Analysis},
  author={Christensen, Ronald and Johnson, Wesley and Branscum, Adam and Hanson, Timothy},
  year={2011},
  publisher={CRC Press}
}


@article{gelmancarlin,
  title={Beyond Power Calculations: {A}ssessing {Type S} (Sign) and {Type M} (Magnitude) Errors},
  author={Gelman, Andrew and Carlin, John B.},
  journal={Perspectives on Psychological Science},
  volume={9},
  number={6},
  pages={641--651},
  year={2014},
  doi = {https://doi.org/10.1177/1745691614551642},
  publisher={SAGE Publications}
}

@Manual{OakleyOHagan,
  title        = {{SHELF: The Sheffield Elicitation Framework (version 2.0)}},
  author       = {Jeremy E. Oakley  and  Anthony O'Hagan},
  organization = {School of Mathematics and Statistics, University of Sheffield},
  address      = {University of Sheffield, UK},
  year         = 2010,
  url          = {http://tonyohagan.co.uk/shelf}
}

@article{brownedraper,
  title={A comparison of {B}ayesian and likelihood-based methods for fitting multilevel models},
  author={Browne, William J. and Draper, David},
  journal={Bayesian Analysis},
  volume={1},
  number={3},
  pages={473--514},
  year={2006},
  publisher={International Society for Bayesian Analysis}
}

@article{gelman06,
  title={Prior distributions for variance parameters in hierarchical models (comment on article by {Browne and Draper})},
  author={Gelman, Andrew},
  journal={Bayesian Analysis},
  volume={1},
  number={3},
  pages={515--534},
  year={2006},
  publisher={International Society for Bayesian Analysis}
}


@book{ohagan2006uncertain,
  title={Uncertain judgements: {E}liciting experts' probabilities},
  author={O'Hagan, Anthony and Buck, Caitlin E. and Daneshkhah, Alireza and Eiser, J. Richard and Garthwaite, Paul H. and Jenkinson, David J. and Oakley, Jeremy E. and Rakow, Tim},
  year={2006},
  publisher={John Wiley \& Sons}
}


@article{gronauLimitationsBayesianLeaveOneOut2018,
  Abstract = {Cross-validation (CV) is increasingly popular as a generic method to adjudicate between mathematical models of cognition and behavior. In order to measure model generalizability, CV quantifies out-of-sample predictive performance, and the CV preference goes to the model that predicted the out-of-sample data best. The advantages of CV include theoretic simplicity and practical feasibility. Despite its prominence, however, the limitations of CV are often underappreciated. Here, we demonstrate the limitations of a particular form of CV---Bayesian leave-one-out cross-validation or LOO---with three concrete examples. In each example, a data set of infinite size is perfectly in line with the predictions of a simple model (i.e., a general law or invariance). Nevertheless, LOO shows bounded and relatively modest support for the simple model. We conclude that CV is not a panacea for model selection.},
  Author = {Gronau, Quentin F. and Wagenmakers, Eric-Jan},
  Year = {2018},
  Doi = {10.1007/s42113-018-0011-7},
  Issn = {2522-0861, 2522-087X},
  Journal = {Computational Brain \& Behavior},
  Langid = {english},
  Title = {Limitations of {{Bayesian Leave}}-{{One}}-{{Out Cross}}-{{Validation}} for {{Model Selection}}},
  Url = {http://link.springer.com/10.1007/s42113-018-0011-7},
  Urldate = {2019-05-17},
  Bdsk-Url-1 = {http://link.springer.com/10.1007/s42113-018-0011-7},
  Bdsk-Url-2 = {https://doi.org/10.1007/s42113-018-0011-7}}


@article{smithEffectWordPredictability2013,
  Abstract = {It is well known that real-time human language processing is highly incremental and context-driven, and that the strength of a comprehender's expectation for each word encountered is a key determinant of the difficulty of integrating that word into the preceding context. In reading, this differential difficulty is largely manifested in the amount of time taken to read each word. While numerous studies over the past thirty years have shown expectation-based effects on reading times driven by lexical, syntactic, semantic, pragmatic, and other information sources, there has been little progress in establishing the quantitative relationship between expectation (or prediction) and reading times. Here, by combining a state-of-the-art computational language model, two large behavioral data-sets, and non-parametric statistical techniques, we establish for the first time the quantitative form of this relationship, finding that it is logarithmic over six orders of magnitude in estimated predictability. This result is problematic for a number of established models of eye movement control in reading, but lends partial support to an optimal perceptual discrimination account of word recognition. We also present a novel model in which language processing is highly incremental well below the level of the individual word, and show that it predicts both the shape and time-course of this effect. At a more general level, this result provides challenges for both anticipatory processing and semantic integration accounts of lexical predictability effects. And finally, this result provides evidence that comprehenders are highly sensitive to relative differences in predictability --even for differences between highly unpredictable words -- and thus helps bring theoretical unity to our understanding of the role of prediction at multiple levels of linguistic structure in real-time language comprehension.},
  Author = {Smith, Nathaniel J and Levy, Roger P.},
  Year = {2013},
  Doi = {10.1016/j.cognition.2013.02.013},
  File = {/home/bruno/ownCloud/bib_papers/Smith and Levy - 2013 - The effect of word predictability on reading time .pdf},
  Issn = {00100277},
  Journal = {Cognition},
  Langid = {english},
  Number = {3},
  Pages = {302-319},
  Title = {The Effect of Word Predictability on Reading Time Is Logarithmic},
  Url = {https://linkinghub.elsevier.com/retrieve/pii/S0010027713000413},
  Urldate = {2019-05-28},
  Volume = {128},
  Bdsk-Url-1 = {https://linkinghub.elsevier.com/retrieve/pii/S0010027713000413},
  Bdsk-Url-2 = {https://doi.org/10.1016/j.cognition.2013.02.013}}

@article{vehtariLimitationsLimitationsBayesian2019,
  Abstract = {In an earlier article in this journal, Gronau and Wagenmakers (2018) discuss some problems with leave-one-out cross-validation (LOO) for Bayesian model selection. However, the variant of LOO that Gronau and Wagenmakers discuss is at odds with a long literature on how to use LOO well. In this discussion, we discuss the use of LOO in practical data analysis, from the perspective that we need to abandon the idea that there is a device that will produce a single-number decision rule.},
  Author = {Vehtari, Aki and Simpson, Daniel P. and Yao, Yuling and Gelman, Andrew},
  Year = {2019},
  Doi = {10.1007/s42113-018-0020-6},
  Issn = {2522-087X},
  Journal = {Computational Brain \& Behavior},
  Keywords = {M-closed,M-open,Principle of complexity,Reality,Statistical convenience},
  Langid = {english},
  Number = {1},
  Pages = {22-27},
  Shortjournal = {Comput Brain Behav},
  Title = {Limitations of ``{{Limitations}} of {{Bayesian Leave}}-One-out {{Cross}}-{{Validation}} for {{Model Selection}}''},
  Url = {https://doi.org/10.1007/s42113-018-0020-6},
  Urldate = {2019-06-02},
  Volume = {2},
  Bdsk-Url-1 = {https://doi.org/10.1007/s42113-018-0020-6}}

@article{dienes2011bayesian,
  Author = {Dienes, Zoltan},
  Journal = {Perspectives on Psychological Science},
  Number = {3},
  Pages = {274--290},
  Publisher = {Sage Publications},
  Title = {Bayesian versus orthodox statistics: Which side are you on?},
  Volume = {6},
  Year = {2011}}

@article{WangGelman2014difficulty,
  Author = {Wang, Wei and Gelman, Andrew},
  File = {:Wang Gelman 2014.pdf:PDF},
  Journal = {Statistics at its Interface},
  Pages = {1--8},
  Title = {Difficulty of selecting among multilevel models using predictive accuracy},
  Volume = {7},
  Year = {2014}}

@article{VehtariOjanen2012,
  Author = {Vehtari, Aki and Ojanen, Janne},
  Doi = {10.1214/12-ss102},
  Issn = {1935-7516},
  Journal = {Statistical Surveys},
  Number = {0},
  Pages = {142--228},
  Publisher = {Institute of Mathematical Statistics},
  Title = {A survey of {Bayes}ian predictive methods for model assessment, selection and comparison},
  Url = {http://dx.doi.org/10.1214/12-SS102},
  Volume = {6},
  Year = {2012},
  Bdsk-Url-1 = {http://dx.doi.org/10.1214/12-SS102},
  Bdsk-Url-2 = {http://dx.doi.org/10.1214/12-ss102}}

@article{Watanabe2010,
  Author = {Watanabe, Sumio},
  Journal = {The Journal of Machine Learning Research},
  Pages = {3571--3594},
  Publisher = {JMLR. org},
  Title = {Asymptotic equivalence of {Bayes} cross validation and widely applicable information criterion in singular learning theory},
  Volume = {11},
  Year = {2010}}

@article{PiironenVehtari2017,
  Author = {Piironen, Juho and Vehtari, Aki},
  File = {:home/bruno/ownCloud/Papers/stats/Piironen Vehtari 2015 Model selection.pdf:PDF},
  Journal = {arXiv preprint arXiv:1503.08650},
  Title = {Comparison of {B}ayesian predictive methods for model selection},
  Year = {2015}}

@article{GeisserEddy1979,
  Author = {Geisser, Seymour and Eddy, William F.},
  Journal = {Journal of the American Statistical Association},
  Number = {365},
  Pages = {153--160},
  Publisher = {Taylor \& Francis Group},
  Title = {A predictive approach to model selection},
  Volume = {74},
  Year = {1979}}

@book{Watanabe2009,
  Author = {Watanabe, Sumio},
  Publisher = {Cambridge University Press},
  Title = {Algebraic geometry and statistical learning theory},
  Volume = {25},
  Year = {2009}}

@article{GelmanEtAl2014understanding,
  Author = {Gelman, Andrew and Hwang, Jessica and Vehtari, Aki},
  Journal = {Statistics and Computing},
  Number = {6},
  Pages = {997--1016},
  Publisher = {Springer},
  Title = {Understanding predictive information criteria for {B}ayesian models},
  Volume = {24},
  Year = {2014}}

@article{ChenGoodman1999,
  Abstract = {We survey the most widely-used algorithms for smoothing models for language n -gram modeling. We then present an extensive empirical comparison of several of these smoothing techniques, including those described by Jelinek and Mercer (1980); Katz (1987); Bell, Cleary and Witten (1990); Ney, Essen and Kneser (1994), and Kneser and Ney (1995). We investigate how factors such as training data size, training corpus (e.g. Brown vs. Wall Street Journal), count cutoffs, and n -gram order (bigram vs. trigram) affect the relative performance of these methods, which is measured through the cross-entropy of test data. We find that these factors can significantly affect the relative performance of models, with the most significant factor being training data size. Since no previous comparisons have examined these factors systematically, this is the first thorough characterization of the relative performance of various algorithms. In addition, we introduce methodologies for analyzing smoothing algorithm efficacy in detail, and using these techniques we motivate a novel variation of Kneser--Ney smoothing that consistently outperforms all other algorithms evaluated. Finally, results showing that improved language model smoothing leads to improved speech recognition performance are presented.},
  Author = {Stanley F. Chen and Joshua Goodman},
  Doi = {https://doi.org/10.1006/csla.1999.0128},
  Issn = {0885-2308},
  Journal = {Computer Speech \& Language},
  Number = {4},
  Pages = {359-394},
  Title = {An empirical study of smoothing techniques for language modeling},
  Url = {http://www.sciencedirect.com/science/article/pii/S0885230899901286},
  Volume = {13},
  Year = {1999},
  Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0885230899901286},
  Bdsk-Url-2 = {https://doi.org/10.1006/csla.1999.0128}}

@article{Lidstone1920,
  Author = {Lidstone, George James},
  Journal = {Transactions of the Faculty of Actuaries},
  Number = {182-192},
  Pages = {13},
  Title = {Note on the general case of the { Bayes-Laplace } formula for inductive or a posteriori probabilities},
  Volume = {8},
  Year = {1920}}

@article{VehtariGelman2015Pareto,
  Author = {Vehtari, Aki and Gelman, Andrew},
  Journal = {arXiv preprint arXiv:1507.02646},
  Title = {Pareto Smoothed Importance Sampling},
  Year = {2015}}

@article{vehtariPracticalBayesianModel2017,
  Abstract = {Leave-one-out cross-validation (LOO) and the widely applicable information criterion (WAIC) are methods for estimating pointwise out-of-sample prediction accuracy from a fitted Bayesian model using the log-likelihood evaluated at the posterior simulations of the parameter values. LOO and WAIC have various advantages over simpler estimates of predictive error such as AIC and DIC but are less used in practice because they involve additional computational steps. Here we lay out fast and stable computations for LOO and WAIC that can be performed using existing simulation draws. We introduce an efficient computation of LOO using Pareto-smoothed importance sampling (PSIS), a new procedure for regularizing importance weights. Although WAIC is asymptotically equal to LOO, we demonstrate that PSIS-LOO is more robust in the finite case with weak priors or influential observations. As a byproduct of our calculations, we also obtain approximate standard errors for estimated predictive errors and for comparison of predictive errors between two models. We implement the computations in an R package called loo and demonstrate using models fit with the Bayesian inference package Stan.},
  Author = {Vehtari, Aki and Gelman, Andrew and Gabry, Jonah},
  Year = {2017},
  Doi = {10.1007/s11222-016-9696-4},
  Issn = {1573-1375},
  Journal = {Statistics and Computing},
  Keywords = {Bayesian computation,K-fold cross-validation,Leave-one-out cross-validation (LOO),Pareto smoothed importance sampling (PSIS),Stan,Widely applicable information criterion (WAIC)},
  Langid = {english},
  Number = {5},
  Pages = {1413-1432},
  Shortjournal = {Stat Comput},
  Title = {Practical {{Bayesian}} Model Evaluation Using Leave-One-out Cross-Validation and {{WAIC}}},
  Url = {https://doi.org/10.1007/s11222-016-9696-4},
  Urldate = {2019-09-07},
  Volume = {27},
  Bdsk-Url-1 = {https://doi.org/10.1007/s11222-016-9696-4}}


@book{laird2019soar,
      title={The {S}oar cognitive architecture},
      author={Laird, John E},
      year={2019},
      publisher={MIT Press}
}

@article{justetal99,
	Author = {Just, Marcel Adam and Carpenter, Patricia A. and Varma, S.},
	Journal = {Human brain mapping},
	Pages = {128--136},
	Title = {Computational modeling of high-level cognition and brain     function},
	Volume = {8},
    doi = {https://doi.org/10.1002/(SICI)1097-0193(1999)8:2/3<128::AID-HBM10>3.0.CO;2-G},
	Year = {1999}}


    @article{epstein2008model,
       title = {Why Model?},
       author = {Epstein, Joshua M.},
       journal = {Journal of Artificial Societies and Social Simulation},
       ISSN = {1460-7425},
       volume = {11},
       number = {4},
       pages = {12},
       year = {2008},
       url = {https://www.jasss.org/11/4/12.html},
       abstract = {This address treats some enduring misconceptions about modeling. One of these is that the goal is always prediction. The lecture distinguishes between explanation and prediction as modeling goals, and offers sixteen reasons other than prediction to build a model. It also challenges the common assumption that scientific theories arise from and 'summarize' data, when often, theories precede and guide data collection; without theory, in other words, it is not clear what data to collect. Among other things, it also argues that the modeling enterprise enforces habits of mind essential to freedom.  It is based on the author's 2008 Bastille Day keynote address to the Second World Congress on Social Simulation, George Mason University, and earlier addresses at the Institute of Medicine, the University of Michigan, and the Santa Fe Institute.},
    }

@article{nosek2022replicability,
      title={Replicability, robustness, and reproducibility in psychological science},
      author={Nosek, Brian A. and Hardwicke, Tom E and Moshontz, Hannah and Allard, Aur{\'e}lien and Corker, Katherine S and Dreber, Anna and Fidler, Fiona and Hilgard, Joe and Kline Struhl, Melissa and Nuijten, Mich{\`e}le B and others},
      journal={Annual Review of Psychology},
      volume={73},
      pages={719--748},
      year={2022},
      doi = {https://doi.org/10.1146/annurev-psych-020821-114157},
      publisher={Annual Reviews}
    }


@article{verhagenBayesianTestsQuantify2014,
  Abstract = {Replication attempts are essential to the empirical sciences. Successful replication attempts increase researchers' confidence in the presence of an effect, whereas failed replication attempts induce skepticism and doubt. However, it is often unclear to what extent a replication attempt results in success or failure. To quantify replication outcomes we propose a novel Bayesian replication test that compares the adequacy of two competing hypotheses. The first hypothesis is that of the skeptic and holds that the effect is spurious; this is the null hypothesis that postulates a zero effect size, H0 : δ = 0. The second hypothesis is that of the proponent and holds that the effect is consistent with the one found in the original study, an effect that can be quantified by a posterior distribution. Hence, the second hypothesis --the replication hypothesis-- is given by Hr : δ ∼ ``posterior distribution from original study''. The weighted likelihood ratio between H0 and Hr quantifies the evidence that the data provide for replication success and failure. In addition to the new test, we present several other Bayesian tests that address different but related questions concerning a replication study. These tests pertain to the independent conclusions of the separate experiments, the difference in effect size between the original experiment and the replication attempt, and the overall conclusion based on the pooled results. Together, this suite of Bayesian tests allows a relatively complete formalization of the way in which the result of a replication attempt alters our knowledge of the phenomenon at hand. The use of all Bayesian replication tests is illustrated with three examples from the literature. For experiments analyzed using the t test, computation of the new replication test only requires the t values and the numbers of participants from the original study and the replication study.},
  Author = {Verhagen, Josine and Wagenmakers, Eric-Jan},
  Year = {2014},
  Doi = {10.1037/a0036731},
  Issn = {1939-2222, 0096-3445},
  Journal = {Journal of Experimental Psychology: General},
  Langid = {english},
  Number = {4},
  Pages = {1457-1475},
  Title = {Bayesian Tests to Quantify the Result of a Replication Attempt.},
  Url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/a0036731},
  Urldate = {2019-05-17},
  Volume = {143},
  Bdsk-Url-1 = {http://doi.apa.org/getdoi.cfm?doi=10.1037/a0036731},
  Bdsk-Url-2 = {https://doi.org/10.1037/a0036731}}


@article{R-brms_a,
  Author = {Paul-Christian B{\"u}rkner},
  Doi = {10.18637/jss.v080.i01},
  Encoding = {UTF-8},
  Journal = {Journal of Statistical Software},
  Number = {1},
  Pages = {1--28},
  Title = {{brms}: An {R} Package for {Bayesian} Multilevel Models Using {Stan}},
  Volume = {80},
  Year = {2017},
  Bdsk-Url-1 = {https://doi.org/10.18637/jss.v080.i01}}

@article{R-brms_b,
  Author = {Paul-Christian B{\"u}rkner},
  Doi = {10.32614/RJ-2018-017},
  Encoding = {UTF-8},
  Journal = {The R Journal},
  Number = {1},
  Pages = {395--411},
  Title = {Advanced {Bayesian} Multilevel Modeling with the {R} Package {brms}},
  Volume = {10},
  Year = {2018},
  Bdsk-Url-1 = {https://doi.org/10.32614/RJ-2018-017}}


@Article{Buerkner2017brms,
    title = {{brms}: An {R} Package for {Bayesian} Multilevel Models Using {Stan}},
    author = {Paul-Christian Bürkner},
    journal = {Journal of Statistical Software},
    year = {2017},
    volume = {80},
    number = {1},
    pages = {1--28},
    doi = {10.18637/jss.v080.i01},
    encoding = {UTF-8}
}


@Misc{Bolker2018,
  Author = {Ben Bolker},
  Title = {Contrasts},
  howpublished = {\url{https://computationalcognitivescience.github.io/lovelace/}},
  note = {Accessed: 2018-06-10},
  Year = {2018}
}

@article{rabe2020hypr,
  title={{hypr}: An R package for hypothesis-driven contrast coding},
  author={Rabe, Maximilian M. and Vasishth, Shravan and Hohenstein, Sven and Kliegl, Reinhold and Schad, Daniel J.},
  journal={Journal of Open Source Software},
  volume={5},
  number={48},
  pages={2134},
  year={2020}
}

@article{heister2012analysing,
  Author = {Heister, Julian and W{\"u}rzner, Kay-Michael and Kliegl, Reinhold},
  Date-Added = {2019-07-17 15:29:04 +0200},
  Date-Modified = {2019-07-17 15:30:03 +0200},
  Journal = {Visual word recognition},
  Pages = {102--130},
  Title = {Analysing large datasets of eye movements during reading},
  Volume = {2},
  Year = {2012}
}

@article{beall2013women,
  title={Women are more likely to wear red or pink at peak fertility},
  author={Beall, Alec T. and Tracy, Jessica L.},
  journal={Psychological Science},
  volume={24},
  number={9},
  pages={1837--1841},
  year={2013},
  publisher={Sage Publications Sage CA: Los Angeles, CA}
}


@book{dobson2011introduction,
  Author = {Dobson, Annette J. and Barnett, Adrian},
  Publisher = {CRC Press},
  Title = {An introduction to generalized linear models},
  Year = {2011}
}

@book{mccullagh2019generalized,
  title={Generalized linear models},
  author={McCullagh, Peter and Nelder, J.A.},
  year={2019},
  edition = {Second Edition},
  publisher={Chapman and Hall/CRC},
  address = {Boca Raton, Florida}
}

@book{fox2015applied,
  title={Applied regression analysis and generalized linear models},
  author={Fox, John},
  year={2015},
  publisher={Sage Publications}
}

@book{harrell2015regression,
  title={Regression modeling strategies: with applications to linear models, logistic and ordinal regression, and survival analysis},
  author={Harrell Jr., Frank E.},
  year={2015},
  publisher={Springer},
  address = {New York, NY}
}

@article{makowski2019bayestestr,
  title={bayestestR: {D}escribing effects and their uncertainty, existence and significance within the {B}ayesian framework},
  author={Makowski, Dominique and Ben-Shachar, Mattan S. and L{\"u}decke, Daniel},
  journal={Journal of Open Source Software},
  volume={4},
  number={40},
  pages={1541},
  doi = {https://doi.org/10.21105/joss.01541},
  year={2019}
}


@book{gelman2020regression,
  title={Regression and other stories},
  author={Gelman, Andrew and Hill, Jennifer and Vehtari, Aki},
  year={2020},
  publisher={Cambridge University Press},
  address = {Cambridge, UK}
}


@book{snedecor1967statistical,
  Address = {Ames, Iowa},
  Author = {Snedecor, George W and Cochran, William G},
  Date-Added = {2019-02-18 15:54:37 +0100},
  Date-Modified = {2019-02-18 15:55:52 +0100},
  Publisher = {Iowa State University Press},
  Title = {Statistical Methods},
  Year = {1967}
}

@article{van2011cue,
  title={Cue-dependent interference in comprehension},
  author={Van Dyke, Julie A. and McElree, Brian},
  journal={Journal of Memory and Language},
  volume={65},
  number={3},
  pages={247--263},
  year={2011},
  publisher={Elsevier}
}

@book{blitzstein2014introduction,
  title={Introduction to probability},
  author={Blitzstein, Joseph K. and Hwang, Jessica},
  year={2014},
  publisher={Chapman and Hall/CRC}
}

@article{WalkerEtAl2018,
  title={A cognitive psychometric model for assessment of picture naming abilities in aphasia.},
  author={Walker, Grant M and Hickok, Gregory and Fridriksson, Julius},
  journal={Psychological assessment},
  year={2018},
  pages = {809-826},
  volume = {6},
  doi = {10.1037/pas0000529},
  publisher={American Psychological Association}
}

@article{matzkeBayesianEstimationMultinomial2015,
  langid = {english},
  title = {Bayesian {{Estimation}} of {{Multinomial Processing Tree Models}} with {{Heterogeneity}} in {{Participants}} and {{Items}}},
  volume = {80},
  issn = {0033-3123, 1860-0980},
  url = {http://link.springer.com/10.1007/s11336-013-9374-9},
  doi = {10.1007/s11336-013-9374-9},
  abstract = {Multinomial processing tree (MPT) models are theoretically motivated stochastic models for the analysis of categorical data. Here we focus on a crossed–random eﬀects extension of the Bayesian latent–trait pair-clustering MPT model. Our approach assumes that participant and item eﬀects combine additively on the probit scale and postulates (multivariate) normal distributions for the random eﬀects. We provide a WinBUGS implementation of the crossed–random eﬀects pair–clustering model and an application to novel experimental data. The present approach may be adapted to handle other MPT models.},
  number = {1},
  journal = {Psychometrika},
  year = {2015},
  pages = {205-235},
  author = {Matzke, Dora and Dolan, Conor V. and Batchelder, William H. and Wagenmakers, Eric-Jan}
}

@Article{BatchelderRiefer1999,
  Title                    = {Theoretical and empirical review of multinomial process tree modeling},
  Author                   = {Batchelder, William H. and Riefer, David M.},
  Year                     = {1999},
  Number                   = {1},
  Pages                    = {57--86},
  Volume                   = {6},
  Journal                  = {Psychonomic Bulletin \& Review},
  Publisher                = {Springer}
}

@book{burger,
  title={The 5 elements of effective thinking},
  author={Burger, Edward B. and Starbird, Michael},
  year={2012},
  publisher={Princeton University Press}
}

@article{baayen2008mixed,
  title={Mixed-effects modeling with crossed random effects for subjects and items},
  author={Baayen, R. Harald and Davidson, Douglas J. and Bates, Douglas M.},
  journal={Journal of Memory and Language},
  volume={59},
  number={4},
  pages={390--412},
  year={2008},
  publisher={Elsevier}
}

@article{wagenmakers2007linear,
  title={On the linear relation between the mean and the standard deviation of a response time distribution.},
  author={Wagenmakers, Eric-Jan and Brown, Scott D.},
  journal={Psychological review},
  volume={114},
  number={3},
  pages={830},
  year={2007},
  publisher={American Psychological Association}
}

@book{morin2016probability,
  title={Probability: {For} the Enthusiastic Beginner},
  author={Morin, David J},
  year={2016},
  publisher={Createspace Independent Publishing Platform}
}
@book{fox2009mathematical,
  title={A mathematical primer for social statistics},
  author={Fox, John},
  Volume={159},
  year={2009},
  publisher={Sage}
}

@book{Royall,
	Author = {Richard Royall},
	Publisher = {Chapman and Hall, CRC Press},
  address = {New York},
	Title = {Statistical Evidence: {A} likelihood paradigm},
	Year = {1997}}


@article{mahowald2016meta,
  title={A meta-analysis of syntactic priming in language production},
  author={Mahowald, Kyle and James, Ariel and Futrell, Richard and Gibson, Edward},
  journal={Journal of Memory and Language},
  volume={91},
  pages={5--27},
  year={2016},
  doi = {https://doi.org/10.1016/j.jml.2016.03.009},
  publisher={Elsevier}
}


@book{kolmogorov2018foundations,
  title={Foundations of the Theory of Probability: {S}econd English Edition},
  author={Kolmogorov, Andre{\u\i} Nikolaevich},
  year={1933},
  note = {Reprinted in 2018},
  publisher={Courier Dover Publications}
}

@book{albert2009bayesian,
  title={Bayesian computation with R},
  author={Albert, Jim},
  year={2009},
  publisher={Springer}
}

@book{Gelman14,
  Author = {Andrew Gelman and John B. Carlin and Hal S. Stern and  David B. Dunson and Aki Vehtari and Donald B. Rubin},
  Edition = {Third Edition},
  Publisher = {Chapman and Hall/CRC Press},
  address = {Boca Raton, FL},
  Title = {Bayesian Data Analysis},
  Year = {2014}}

@article{lewis2006computational,
  Author = {Lewis, Richard L. and Vasishth, Shravan and Van Dyke, Julie A.},
  Journal = {Trends in Cognitive Sciences},
  Number = {10},
  Pages = {447--454},
  Title = {Computational principles of working memory in sentence comprehension},
  Volume = {10},
  Year = {2006}
}

@article{vasishth2019computational,
  Author = {Vasishth, Shravan and Nicenboim, Bruno and Engelmann, Felix and Burchert, Frank},
  Journal = {Trends in Cognitive Sciences},
  Number = {11},
  Pages = {968--982},
  Title = {Computational models of retrieval processes in sentence processing},
  Volume = {23},
  Year = {2019}
}

@article{arehalli2020neural,
  title={Neural language models capture some, but not all, agreement attraction effects},
  author={Arehalli, Suhas and Linzen, Tal},
  year={2020},
  journal={PsyArXiv preprint}
}

@book{mcclelland1989explorations,
  title={Explorations in parallel distributed processing: A handbook of models, programs, and exercises},
  author={McClelland, James L. and Rumelhart, David E.},
  year={1989},
  publisher={MIT Press}
}

@book{port1995mind,
  title={Mind as motion: {E}xplorations in the dynamics of cognition},
  author={Port, Robert F. and Van Gelder, Timothy},
  year={1995},
  publisher={MIT Press}
}

@article{beer2000dynamical,
  title={Dynamical approaches to cognitive science},
  author={Beer, Randall D},
  journal={Trends in Cognitive Sciences},
  volume={4},
  number={3},
  pages={91--99},
  year={2000},
  publisher={Elsevier}
}

@book{sivia2006data,
  title={Data analysis: a Bayesian tutorial},
  author={Sivia, Devinderjit and Skilling, John},
  year={2006},
  publisher={OUP Oxford}
}



@article{JaegerEngelmannVasishth2017,
  Author = {J{\"a}ger, Lena A. and Engelmann, Felix and Vasishth, Shravan},
  Title = {Similarity-based interference in sentence comprehension: {Literature review and Bayesian meta-analysis}},
  pdf = {http://www.ling.uni-potsdam.de/~vasishth/pdfs/JaegerEngelmannVasishthJML2017.pdf},
  abstract = {We report a comprehensive review of the published reading studies on retrieval interference in reflexive-/reciprocal-antecedent and subject-verb dependencies. We also provide a quantitative random-effects meta-analysis of self-paced and eyetracking reading studies. We show that the empirical evidence is only partly consistent with cue-based retrieval as implemented in the ACT-R-based model of sentence processing by Lewis \& Vasishth 2005 (LV05) and that there are important differences between the reviewed dependency types. In non-agreement subject-verb dependencies, there is evidence for inhibitory interference in configurations where the correct dependent fully matches the retrieval cues. This is consistent with the LV05 cue-based retrieval account. By contrast, in subject-verb agreement as well as in reflexive-/reciprocal-antecedent dependencies, no evidence for interference is found in configurations with a fully cue-matching subject. In configurations with only a partially cue-matching subject or antecedent, the meta-analysis revealed facilitatory interference in subject-verb agreement and inhibitory interference in reflexives/reciprocals. The former is consistent with the LV05 account, but the latter is not. Moreover, the meta-analysis revealed that  (i) interference type (proactive versus retroactive) leads to different effects in the reviewed dependency types; and (ii) the prominence of the distractor has an important impact on the interference effect. In sum, the meta-analysis suggests that the LV05 needs important modifications to account for (i) the unexplained interference patterns and (ii) the differences between the dependency types. More generally, the meta-analysis provides a quantitative empirical basis for comparing the predictions of competing accounts of retrieval processes in sentence comprehension.},
  Year = {2017},
  volume = {94},
  pages = {316-339},
  journal={Journal of Memory and Language},
  code = {https://github.com/vasishth/MetaAnalysisJaegerEngelmannVasishth2017},
  doi = {https://doi.org/10.1016/j.jml.2017.01.004}
  }

@article{VasishthBeckmanetal,
  Author = {Shravan Vasishth and Bruno Nicenboim and Mary E. Beckman and Fangfang Li and Eun Jong Kong},
  Title = {Bayesian data analysis in the phonetic sciences: {A} tutorial introduction},
  Year = {2018},
  journal = {Journal of Phonetics},
  url = {https://osf.io/g4zpv/},
  doi= {10.1016/j.wocn.2018.07.008},
  volume = {71},
  pages = {141-161}
  }

@article{BuerkiEtAl2020,
title={What did we learn from forty years of research on semantic interference? {A Bayesian} meta-analysis},
  author={Audrey B{\"u}rki and Shereen Elbuy and Sylvain Madec and Shravan Vasishth},
  year = {2020},
  volume = {114},
  doi = {10.1016/j.jml.2020.104125},
  code = {https://osf.io/k6f4c/},
  journal = {Journal of Memory and Language}
  }

  @article{Buerki2022,
    title={When words collide: {B}ayesian meta-analyses of distractor and target properties in the picture-word interference paradigm},
    author={Audrey B{\"u}rki and Francois-Xavier Alario and Shravan Vasishth},
    year = {2023},
    code = {https://osf.io/sjn5b/},
    volume = {76},
    issue = {6},
    pages = {1410-1430},
    doi = {https://doi.org/10.1177/17470218221114644},
    journal = {Quarterly Journal of Experimental Psychology},
    pdf = {https://arxiv.org/abs/2008.03972}
    }

    @article{cox2022bayesian,
      title={A {B}ayesian meta-analysis of infants’ ability to perceive audio--visual congruence for speech},
      author={Cox, Christopher Martin Mikkelsen and Keren-Portnoy, Tamar and Roepstorff, Andreas and Fusaroli, Riccardo},
      journal={Infancy},
      volume={27},
      number={1},
      pages={67--96},
      year={2022},
      publisher={Wiley Online Library}
    }


  @article {VasishthNicenboimStatMeth,
author = {Vasishth, Shravan and Nicenboim, Bruno},
title = {Statistical Methods for Linguistic Research: {F}oundational Ideas – {Part I}},
journal = {Language and Linguistics Compass},
volume = {10},
number = {8},
issn = {1749-818X},
pdf = {http://dx.doi.org/10.1111/lnc3.12201},
OPTdoi = {10.1111/lnc3.12201},
code = {https://github.com/vasishth/VasishthNicenboimPart1},
pages = {349--369},
year = {2016}
}



@article{NicenboimRoettgeretal,
  Author = {Bruno Nicenboim and Timo B. Roettger and Shravan Vasishth},
  Title = {Using meta-analysis for evidence synthesis: {The case of incomplete neutralization in German}},
  Year = {2018},
  journal = {Journal of Phonetics},
  doi = {https://doi.org/10.1016/j.wocn.2018.06.001},
  url = {https://osf.io/g5ndw/},
  pdf = {https://mfr.osf.io/render?url=https://osf.io/4k25w/?action=download%26mode=render},
  volume = {70},
  pages = {39-55}
  }


@article{lago2015agreement,
  Author = {Lago, Sol and Shalom, Diego and Sigman, Mariano and Lau, Ellen F. and Phillips, Colin},
  Journal = {Journal of Memory and Language},
  Pages = {133--149},
  Title = {Agreement processes in Spanish comprehension},
  Volume = {82},
  doi = {https://doi.org/10.1016/j.jml.2015.02.002},
  Year = {2015}
}

@book{gill2006essential,
  title={Essential mathematics for political and social research},
  author={Gill, Jeff},
  year={2006},
  publisher={Cambridge University Press Cambridge}
}


@book{kruschke2014doing,
  title={Doing {B}ayesian data analysis: {A tutorial with R, JAGS, and Stan}},
  author={Kruschke, John K.},
  year={2014},
  publisher={Academic Press},
  addess = {San Diego, CA}
}

@article{dersimonian1986meta,
  title={Meta-analysis in clinical trials},
  author={DerSimonian, Rebecca and Laird, Nan M.},
  journal={Controlled clinical trials},
  volume={7},
  number={3},
  doi = {https://doi.org/10.1016/0197-2456(86)90046-2},
  pages={177--188},
  year={1986},
  publisher={Elsevier}
}

@article{normand1999tutorial,
  title={Tutorial in biostatistics meta-analysis: formulating, evaluating, combining, and reporting},
  author={Normand, S.L.T.},
  journal={Statistics in Medicine},
  volume={18},
  number={3},
  pages={321--359},
  year={1999},
  doi = {https://doi.org/10.1002/(SICI)1097-0258(19990215)18:3<321::AID-SIM28>3.0.CO;2-P}
}

@article{swets2008underspecification,
  title={Underspecification of syntactic ambiguities: {E}vidence from self-paced reading},
  author={Swets, Benjamin and Desmet, Timothy and Clifton, Charles and Ferreira, Fernanda},
  journal={Memory and Cognition},
  volume={36},
  number={1},
  pages={201--216},
  year={2008},
  publisher={Springer}
}


@mastersthesis{VasishthMScStatistics,
  Address = {Sheffield, UK},
  Author = {Shravan Vasishth},
  School = {School of Mathematics and Statistics, University of Sheffield},
  Title = {A meta-analysis of relative clause processing in {M}andarin {C}hinese using bias modelling},
  Url = {http://www.ling.uni-potsdam.de/~vasishth/pdfs/VasishthMScStatistics.pdf},
  Year = {2015},
  code = {https://github.com/vasishth/MScDissertationVasishth}
  }

@article{reali2007,
  title={Processing of relative clauses is made easier by frequency of occurrence},
  author={Reali, Florencia and Christiansen, Morten H.},
  journal={Journal of memory and language},
  volume={57},
  number={1},
  pages={1--23},
  year={2007},
  doi = {https://doi.org/10.1016/j.jml.2006.08.014},
  publisher={Elsevier}
}

@article{jc92,
  Author = {Just, Marcel Adam and Carpenter, Patricia A.},
  Journal = {Psychological Review},
  Pages = {122--149},
  Title = {A capacity theory of comprehension: {I}ndividual differences in working memory},
  Volume = {99},
  issue = {1},
  doi = {https://doi.org/10.1037/0033-295X.99.1.122},
  Year = {1992}}


@article{carpenter2017stan,
  title={Stan: {A} probabilistic programming language},
  author={Carpenter, Bob and Gelman, Andrew and Hoffman, Matthew D. and Lee, Daniel and Goodrich, Ben and Betancourt, Michael J. and Brubaker, Marcus and Guo, Jiqiang and Li, Peter and Riddell, Allen},
  journal={Journal of Statistical Software},
  volume={76},
  number={1},
  year={2017},
  publisher={Columbia Univ., New York, NY (United States); Harvard Univ., Cambridge, MA (United States)}
}

@MISC{JASP2019,
AUTHOR = {{JASP Team}},
TITLE = {{JASP (Version 0.11.1)[Computer software]}},
YEAR = {2019},
URL = {https://jasp-stats.org/}
}
@article{Salvatier2016,
  doi = {10.7717/peerj-cs.55},
  url = {https://doi.org/10.7717/peerj-cs.55},
  year  = {2016},
  month = {apr},
  publisher = {{PeerJ}},
  volume = {2},
  pages = {e55},
  author = {John Salvatier and Thomas V. Wiecki and Christopher Fonnesbeck},
  title = {Probabilistic programming in Python using {PyMC}3},
  journal = {{PeerJ} Computer Science}
}

@article{spiegelhalter1994bayesian,
  title={Bayesian approaches to randomized trials},
  author={Spiegelhalter, David J. and Freedman, Laurence S. and Parmar, Mahesh K.B.},
  journal={Journal of the Royal Statistical Society. Series A (Statistics in Society)},
  pages={357--416},
  year={1994},
  volume={157},
  number={3}
}

@article{Freedman1984,
 author = {Laurence S. Freedman and D. Lowe and P. Macaskill},
 journal = {Biometrics},
 number = {3},
 pages = {575--586},
 title = {Stopping Rules for Clinical Trials Incorporating Clinical Opinion},
 volume = {40},
 year = {1984}
}


@book{lunn2012bugs,
  title={The {BUGS} book: {A} practical introduction to {B}ayesian analysis},
  author={Lunn, David J. and Jackson, Chris and Spiegelhalter, David J. and Best, Nichola G. and Thomas, Andrew},
  volume={98},
  year={2012},
  publisher={CRC Press}
}

@book{jackman2009bayesian,
  title={Bayesian analysis for the social sciences},
  author={Jackman, Simon},
  volume={846},
  year={2009},
  publisher={John Wiley \& Sons}
}


@article{lunn2000winbugs,
  title={{WinBUGS}-{A B}ayesian modelling framework: {C}oncepts, structure, and extensibility},
  author={Lunn, David J. and Thomas, Andrew and Best, Nichola G. and Spiegelhalter, David J.},
  journal={Statistics and Computing},
  volume={10},
  number={4},
  pages={325--337},
  year={2000},
  publisher={Springer}
}

@misc{richard_morey_2015_31202,
  author       = {Richard D. Morey and
                  Jeffrey N. Rouder and
                  Jonathon Love and
                  Ben Marwick},
  title        = {BayesFactor: 0.9.12-2 CRAN},
  month        = sep,
  year         = 2015,
  publisher    = {Zenodo},
  version      = {0.9.12-2},
  doi          = {10.5281/zenodo.31202},
  url          = {https://doi.org/10.5281/zenodo.31202}
}

@misc{plummer2016jags,
  Author = {Plummer, Martin},
  Title = {JAGS Version 4.2.0 user manual},
  Year = {2016}}

@Misc{rstanarm,
    title = {rstanarm: {Bayesian} applied regression modeling via
      {Stan}.},
    author = {Ben Goodrich and Jonah Gabry and Imad Ali and Sam
      Brilleman},
    note = {R package version 2.17.4},
    year = {2018},
    url = {http://mc-stan.org/}
  }


@article{hoffmanNoUTurnSamplerAdaptively2014,
  title = {The {{No}}-{{U}}-{{Turn Sampler}}: {{Adaptively}} Setting Path Lengths in {{Hamiltonian Monte Carlo}}},
  volume = {15},
  issn = {1532-4435},
  url = {http://dl.acm.org/citation.cfm?id=2627435.2638586},
  shorttitle = {The {{No}}-{{U}}-Turn {{Sampler}}},
  number = {1},
  journal = {Journal of Machine Learning Research},
  urldate = {2019-11-20},
  year = {2014},
  pages = {1593--1623},
  keywords = {adaptive Monte Carlo,Bayesian inference,dual averaging,Hamiltonian Monte Carlo,Markov chain Monte Carlo},
  author = {Hoffman, Matthew D. and Gelman, Andrew}
}



@article{duaneHybridMonteCarlo1987,
  langid = {english},
  title = {Hybrid {{Monte Carlo}}},
  volume = {195},
  issn = {0370-2693},
  url = {http://www.sciencedirect.com/science/article/pii/037026938791197X},
  doi = {10.1016/0370-2693(87)91197-X},
  abstract = {We present a new method for the numerical simulation of lattice field theory. A hybrid (molecular dynamics/Langevin) algorithm is used to guide a Monte Carlo simulation. There are no discretization errors even for large step sizes. The method is especially efficient for systems such as quantum chromodynamics which contain fermionic degrees of freedom. Detailed results are presented for four-dimensional compact quantum electrodynamics including the dynamical effects of electrons.},
  number = {2},
  journal = {Physics Letters B},
  shortjournal = {Physics Letters B},
  urldate = {2019-11-20},
  year = {1987},
  pages = {216-222},
  author = {Duane, Simon and Kennedy, A.D. and Pendleton, Brian J. and Roweth, Duncan}
}



@incollection{nealMCMCUsingHamiltonian2011,
  title = {{MCMC} using {Hamiltonian} dynamics},
  doi = {10.1201/b10905-10},
  booktitle = {Handbook of Markov Chain Monte Carlo},
  year = {2011},
  chapter = {5},
  publisher = {Taylor \& Francis},
  editor = {Steve Brooks and Andrew Gelman and Galin Jones and Xiao-Li Meng},
  author = {Neal, Radford M.}
}


@book{dead,
  title={Designing experiments and analyzing data: {A} model comparison perspective},
  author={Maxwell, Scott E and Delaney, Harold D and Kelley, Ken},
  year={2017},
  publisher={Routledge},
  address = {New York, NY}
}

@book{chambers2019seven,
  title={The seven deadly sins of psychology: {A} manifesto for reforming the culture of scientific practice},
  author={Chambers, Chris},
  year={2019},
  publisher={Princeton University Press}
}


@article{barr2013,
  title={Random effects structure for confirmatory hypothesis testing: {K}eep it maximal},
  author={Barr, Dale J. and Levy, Roger P. and Scheepers, Christoph and Tily, Harry J.},
  journal={Journal of Memory and Language},
  volume={68},
  number={3},
  pages={255--278},
  year={2013},
  publisher={Elsevier}
}

@book{spiegelhalter2004bayesian,
  title={Bayesian approaches to clinical trials and health-care evaluation},
  author={Spiegelhalter, David J. and Abrams, Keith R. and Myles, Jonathan P.},
  volume={13},
  year={2004},
  publisher={John Wiley \& Sons}
}

@article{kadane1998experiences,
  title={Experiences in elicitation},
  author={Kadane, Joseph and Wolfson, Lara J.},
  journal={Journal of the Royal Statistical Society: Series D ({T}he Statistician)},
  volume={47},
  number={1},
  pages={3--19},
  year={1998},
  doi = {https://doi.org/10.1111/1467-9884.00113},
  publisher={Wiley Online Library}
}

@article{paapevasishthmpt2022,
	Author = {Dario Paape and Shravan Vasishth},
	Title = {Estimating the true cost of garden-pathing: {A} computational model of latent cognitive processes},
	Year = {2022},
	journal = {Cognitive Science},
    volume = {46},
    issue = {8},
    pages = {e13186},
    doi = {https://doi.org/10.1111/cogs.13186},
    code = {https://osf.io/9k8wn/?view_only=4253f641077445a7ae2670d8d81088c9.
Sections},
    pdf = {https://doi.org/10.1111/cogs.13186}
}


@article{gelmanPriorCanOften2017,
  langid = {english},
  title = {The prior can often only be understood in the context of the likelihood},
  volume = {19},
  url = {https://www.mdpi.com/1099-4300/19/10/555},
  doi = {10.3390/e19100555},
  abstract = {A key sticking point of Bayesian analysis is the choice of prior distribution, and there is a vast literature on potential defaults including uniform priors, Jeffreys’ priors, reference priors, maximum entropy priors, and weakly informative priors. These methods, however, often manifest a key conceptual tension in prior modeling: a model encoding true prior information should be chosen without reference to the model of the measurement process, but almost all common prior modeling techniques are implicitly motivated by a reference likelihood. In this paper we resolve this apparent paradox by placing the choice of prior into the context of the entire Bayesian analysis, from inference to prediction to model evaluation.},
  number = {10},
  journal = {Entropy},
  urldate = {2019-11-20},
  year = {2017},
  pages = {555},
  keywords = {Bayesian inference,default priors,prior distribution},
  author = {Gelman, Andrew and Simpson, Daniel P. and Betancourt, Michael J.}
}


@article{simpsonPenalisingModelComponent2017,
  langid = {english},
  title = {Penalising Model Component Complexity: {{A}} Principled, Practical Approach to Constructing Priors},
  volume = {32},
  issn = {0883-4237, 2168-8745},
  url = {https://projecteuclid.org/euclid.ss/1491465621},
  doi = {10.1214/16-STS576},
  shorttitle = {Penalising {{Model Component Complexity}}},
  abstract = {In this paper, we introduce a new concept for constructing prior distributions. We exploit the natural nested structure inherent to many model components, which defines the model component to be a flexible extension of a base model. Proper priors are defined to penalise the complexity induced by deviating from the simpler base model and are formulated after the input of a user-defined scaling parameter for that model component, both in the univariate and the multivariate case. These priors are invariant to reparameterisations, have a natural connection to Jeffreys’ priors, are designed to support Occam’s razor and seem to have excellent robustness properties, all which are highly desirable and allow us to use this approach to define default prior distributions. Through examples and theoretical results, we demonstrate the appropriateness of this approach and how it can be applied in various situations.},
  number = {1},
  journal = {Statistical Science},
  urldate = {2019-11-20},
  year = {2017},
  pages = {1-28},
  keywords = {Bayesian theory,disease mapping,hierarchical models,information geometry,interpretable prior distributions,prior on correlation matrices},
  author = {Simpson, Daniel P. and Rue, Håvard and Riebler, Andrea and Martins, Thiago G. and Sørbye, Sigrunn H.}
}

@book{blokpoelvanrooij,
  title={Theoretical Modeling for cognitive science and psychology},
  author={Mark Blokpoel and Iris van Rooij},
  year={2021},
  howpublished = {\url{https://computationalcognitivescience.github.io/lovelace/}},
  note = {Accessed: 2022-11-25}
}


@book{winter2019statistics,
  title={Statistics for Linguists: {An Introduction Using R}},
  author={Winter, Bodo},
  year={2019},
  publisher={Routledge}
}

@book{cochrane,
	Address = {New York},
	Author = {Julian Higgins and Sally Green},
	Publisher = {Wiley-Blackwell},
	Title = {Cochrane Handbook for Systematics Reviews of Interventions},
	Year = {2008}}

@book{sutton2012evidence,
  title={Evidence synthesis for decision making in healthcare},
  author={Sutton, Alexander J. and Welton, Nicky J. and Cooper, Nicola and Abrams, Keith R. and Ades, A.E.},
  volume={132},
  year={2012},
  publisher={John Wiley \& Sons}
}

@article{abbl02,
  Author = {John R. Anderson and Dan Bothell and  Michael D. Byrne and Scott Douglass and Christian Lebiere and Yulin Qin},
  Journal = {Psychological Review},
  Number = {4},
  Pages = {1036-1060},
  Title = {An Integrated Theory of the Mind},
  Volume = {111},
  Year = {2004},
  Bdsk-Url-1 = {http://act-r.psy.cmu.edu/publications/pubinfo.php?id=526}}


@article{shiffrinSurveyModelEvaluation2008,
  langid = {english},
  title = {A Survey of Model Evaluation Approaches With a Tutorial on Hierarchical {Bayesian} Methods},
  volume = {32},
  issn = {0364-0213},
  url = {http://doi.wiley.com/10.1080/03640210802414826},
  doi = {10.1080/03640210802414826},
  abstract = {This article reviews current methods for evaluating models in the cognitive sciences, including theoretically based approaches, such as Bayes factors and minimum description length measures; simulation approaches, including model mimicry evaluations; and practical approaches, such as validation and generalization measures. This article argues that, although often useful in specific settings, most of these approaches are limited in their ability to give a general assessment of models. This article argues that hierarchical methods, generally, and hierarchical Bayesian methods, specifically, can provide a more thorough evaluation of models in the cognitive sciences. This article presents two worked examples of hierarchical Bayesian analyses to demonstrate how the approach addresses key questions of descriptive adequacy, parameter interference, prediction, and generalization in principled and coherent ways.},
  number = {8},
  journal = {Cognitive Science: A Multidisciplinary Journal},
  urldate = {2019-05-17},
  year = {2008},
  pages = {1248-1284},
  author = {Shiffrin, Richard M and Lee, Michael D. and Kim, Woojae and Wagenmakers, Eric-Jan}
}


@article{frankERPResponseAmount2015,
  langid = {english},
  title = {The {{ERP}} Response to the Amount of Information Conveyed by Words in Sentences},
  volume = {140},
  issn = {0093934X},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0093934X14001515},
  doi = {10.1016/j.bandl.2014.10.006},
  abstract = {Reading times on words in a sentence depend on the amount of information the words convey, which can be estimated by probabilistic language models. We investigate whether event-related potentials (ERPs), too, are predicted by information measures. Three types of language models estimated four different information measures on each word of a sample of English sentences. Six different ERP deflections were extracted from the EEG signal of participants reading the same sentences. A comparison between the information measures and ERPs revealed a reliable correlation between N400 amplitude and word surprisal. Language models that make no use of syntactic structure fitted the data better than did a phrase-structure grammar, which did not account for unique variance in N400 amplitude. These findings suggest that different information measures quantify cognitively different processes and that readers do not make use of a sentence’s hierarchical structure for generating expectations about the upcoming word. Ó 2014 The Authors. Published by Elsevier Inc. This is an open access article under the CC BY license (http:// creativecommons.org/licenses/by/3.0/).},
  journal = {Brain and Language},
  urldate = {2019-05-17},
  year = {2015},
  pages = {1-11},
  author = {Frank, Stefan L. and Otten, Leun J. and Galli, Giulia and Vigliocco, Gabriella}
}

@article{FrankEtAl2015,
  author = {Stefan L. Frank and Thijs Trompenaars and Shravan Vasishth},
  title = {Cross-linguistic differences in processing double-embedded relative clauses: {W}orking-memory constraints or language statistics?},
  year = {2015},
  pages = {554-578},
  volume = {40},
  doi = {10.1111/cogs.12247},
  abstract = {An English double-embedded relative clause from which the middle verb is omitted can often be processed more easily than its grammatical counterpart, a phenomenon known as the grammaticality illusion. This effect has been found to be reversed in German, suggesting that the illusion is language specific rather than a consequence of universal working memory constraints. We present results from three self-paced reading experiments which show that Dutch native speakers also do not show the grammaticality illusion in Dutch, whereas both German and Dutch native speakers do show the illusion when reading English sentences. These findings provide evidence against working memory constraints as an explanation for the observed effect in English. We propose an alternative account based on the statistical patterns of the languages involved. In support of this alternative, a single recurrent neural network model that is trained on both Dutch and English sentences indeed predicts the cross-linguistic difference in grammaticality effect.},
  journal = {Cognitive Science},
  code = {https://github.com/vasishth/StanJAGSexamples/tree/master/FrankEtAlCogSci2015},
  pdf = {http://stefanfrank.info/pubs/GrammaticalityIllusion.pdf}
}


@article{kutasBrainPotentialsReading1984,
  title = {Brain Potentials during Reading Reflect Word Expectancy and Semantic Association},
  volume = {307},
  issn = {1476-4687},
  url = {https://www.nature.com/articles/307161a0},
  doi = {10.1038/307161a0},
  abstract = {The neuroelectric activity of the human brain that accompanies linguistic processing can be studied through recordings of event-related potentials (e.r.p. components) from the scalp. The e.r.ps triggered by verbal stimuli have been related to several different aspects of language processing1. For example, the N400 component, peaking around 400 ms post-stimulus, appears to be a sensitive indicator of the semantic relationship between a word and the context in which it occurs. Words that complete sentences in a nonsensical fashion elicit much larger N400 waves than do semantically appropriate words or non-semantic irregularities in a text2,3. In the present study, e.r.ps were recorded in response to words that completed meaningful sentences. The amplitude of the N400 component of the e.r.p. was found to be an inverse function of the subject's expectancy for the terminal word as measured by its ‘Cloze probability’. In addition, unexpected words that were semantically related to highly expected words elicited lower N400 amplitudes. These findings suggest N400 may reflect processes of semantic priming or activation.},
  number = {5947},
  journal = {Nature},
  shortjournal = {Nature},
  urldate = {2019-08-08},
  year = {1984},
  pages = {161-163},
  author = {Kutas, Marta and Hillyard, Steven A.}
}

@article{kutasReadingSenselessSentences1980,
  langid = {english},
  title = {Reading Senseless Sentences: Brain Potentials Reflect Semantic Incongruity},
  volume = {207},
  issn = {0036-8075, 1095-9203},
  url = {https://science.sciencemag.org/content/207/4427/203},
  doi = {10.1126/science.7350657},
  shorttitle = {Reading Senseless Sentences},
  abstract = {In a sentence reading task, words that occurred out of context were associated with specific types of event-related brain potentials. Words that were physically aberrant (larger than normal) elecited a late positive series of potentials, whereas semantically inappropriate words elicited a late negative wave (N400). The N400 wave may be an electrophysiological sign of the "reprocessing" of semantically anomalous information.},
  number = {4427},
  journal = {Science},
  urldate = {2019-08-08},
  year = {1980},
  pages = {203-205},
  author = {Kutas, Marta and Hillyard, Steven A.},
  eprinttype = {pmid},
  eprint = {7350657}
}



@article{kutasThirtyYearsCounting2011,
  title = {Thirty Years and Counting: {Finding} Meaning in the {N400} Componentof the Event-related Brain Potential ({ERP})},
  volume = {62},
  url = {https://doi.org/10.1146/annurev.psych.093008.131123},
  doi = {10.1146/annurev.psych.093008.131123},
  number = {1},
  journal = {Annual Review of Psychology},
  year = {2011},
  pages = {621-647},
  author = {Kutas, Marta and Federmeier, Kara D.},
  eprint = {20809790}
}

@article{NicenboimPreactivation2019,
title={Are words pre-activated probabilistically during sentence comprehension? Evidence from new data and a {B}ayesian random-effects meta-analysis using publicly available data},
  author={Bruno Nicenboim and Shravan Vasishth and Frank R{\"o}sler},
  year = {2020},
  volume = {142},
  code = {https://psyarxiv.com/2atrh/},
  doi = {10.1016/j.neuropsychologia.2020.107427},
  journal = {Neuropsychologia}
  }

@article{delongProbabilisticWordPreactivation2005,
  title = {Probabilistic Word Pre-Activation during Language Comprehension Inferred from Electrical Brain Activity},
  volume = {8},
  issn = {1097-6256, 1546-1726},
  url = {http://www.nature.com/articles/nn1504},
  doi = {10.1038/nn1504},
  number = {8},
  journal = {Nature Neuroscience},
  urldate = {2019-05-17},
  year = {2005},
  pages = {1117-1121},
  author = {DeLong, Katherine A. and Urbach, Thomas P. and Kutas, Marta}
}

@article{NicenboimEtAlCogSci2018,
  year = {2018},
  author = {Bruno Nicenboim and Shravan Vasishth and Felix Engelmann and Katja Suckow},
  journal = {Cognitive Science},
  volume = {42},
  issue = {S4},
  page = {1075-1100},
  title = {Exploratory and confirmatory analyses in sentence processing: {A case study of number interference in German}},
  doi = {10.1111/cogs.12589}
}


@article{nieuwlandLargescaleReplicationStudy2018,
  title = {Large-Scale Replication Study Reveals a Limit on Probabilistic Prediction in Language Comprehension},
  volume = {7},
  issn = {2050-084X},
  url = {https://elifesciences.org/articles/33468},
  doi = {10.7554/eLife.33468},
  abstract = {Do people routinely pre-activate the meaning and even the phonological form of upcoming words? The most acclaimed evidence for phonological prediction comes from a 2005 Nature Neuroscience publication by DeLong, Urbach and Kutas, who observed a graded modulation of electrical brain potentials (N400) to nouns and preceding articles by the probability that people use a word to continue the sentence fragment (‘cloze’). In our direct replication study spanning 9 laboratories (N=334), pre-registered replication-analyses and exploratory Bayes Factor analyses successfully replicated the noun-results but, crucially, not the article-results. Pre-registered single-trial analyses also yielded a statistically significant effect for the nouns but not the articles. Exploratory Bayesian single-trial analyses showed that the article-effect may be non-zero but is likely far smaller than originally reported and too small to observe without very large sample sizes. Our results do not support the view that readers routinely pre-activate the phonological form of predictable words.},
  journal = {eLife},
  urldate = {2019-05-17},
  year = {2018},
  author = {Nieuwland, Mante S. and Politzer-Ahles, Stephen and Heyselaar, Evelien and Segaert, Katrien and Darley, Emily and Kazanina, Nina and Von Grebmer Zu Wolfsthurn, Sarah and Bartolozzi, Federica and Kogan, Vita and Ito, Aine and Mézière, Diane and Barr, Dale J. and Rousselet, Guillaume A. and Ferguson, Heather J. and Busch-Moreno, Simon and Fu, Xiao and Tuomainen, Jyrki and Kulakova, Eugenia and Husband, E. Matthew and Donaldson, David I. and Kohút, Zdenko and Rueschemeyer, Shirley-Ann and Huettig, Falk}
}


@book{GelmanHill2007,
  title={Data analysis using regression and multilevel/hierarchical models},
  author={Gelman, Andrew and Hill, Jennifer},
  year={2007},
  publisher={Cambridge University Press}
}



@Book{mcelreath2015statistical,
  Title                    = {Statistical rethinking: {A} {Bayesian} course with examples in {R} and {Stan}},
  Author                   = {McElreath, Richard},
  year={2020},
  Publisher                = {Chapman and Hall/CRC},
    address = {Boca Raton, Florida}
}


@article{rp,
  Author = {Seth Roberts and Harold Pashler},
  Journal = {Psychological Review},
  OPTMonth = {April},
  Number = {2},
  Pages = {358-367},
  Title = {How persuasive is a good fit? {A} comment on theory testing},
  Volume = {107},
  doi = {https://doi.org/10.1037/0033-295X.107.2.358},
  Year = {2000}}

@article{rodriguez2021and,
  title={Who is and is not “average”? Random effects selection with spike-and-slab priors},
  author={Rodriguez, Josue E. and Williams, Donald R. and Rast, Philippe},
  journal={Psychological Methods},
  year={2022},
  doi = {https://doi.org/10.1037/met0000535},
  publisher={American Psychological Association}
}


@ARTICLE{NicenboimVasishth2016,
   author = {Bruno Nicenboim and Shravan Vasishth},
    title = "{Statistical methods for linguistic research: {Foundational} Ideas - {Part} {II}}",
  journal = {Language and Linguistics Compass},
   eprint = {https://arxiv.org/abs/1602.00245},
  pages = {591--613},
  doi = {10.1111/lnc3.12207},
url = {http://dx.doi.org/10.1111/lnc3.12207},
  issn = {1749-818X},
  number = {11},
  volume = {10},
     year = "2016"
}

@article{HofmeisterVasishth2014,
  author = {Philip Hofmeister and Shravan Vasishth},
  title = {Distinctiveness and encoding effects in online sentence comprehension},
  year = {2014},
  pages = {1--13},
  abstract = {In explicit memory recall and recognition tasks, elaboration and contextual isolation both facilitate memory performance. Here, we investigate these effects in the context of sentence processing: targets for retrieval during online sentence processing of English object relative clause constructions differ in the amount of elaboration associated with the target noun phrase, or the homogeneity of superficial features (text color). Experiment 1 shows that greater elaboration for targets during the encoding phase reduces reading times at retrieval sites, but elaboration of non-targets has considerably weaker effects. Experiment 2 illustrates that processing isolated superficial features of target noun phrases—here, a green word in a sentence with words colored white—does not lead to enhanced memory performance, despite triggering longer encoding times. These results are interpreted in the light of the memory models of Nairne, 1990, 2001, 2006, which state that encoding remnants contribute to the set of retrieval cues that provide the basis for similarity-based interference effects.},
  doi = {doi: 10.3389/fpsyg.2014.01237},
  pdf = {http://journal.frontiersin.org/Journal/10.3389/fpsyg.2014.01237/abstract},
  volume = {5},
  journal = {Frontiers in Psychology},
  note = {Article 1237},
  code = {http://privatewww.essex.ac.uk/~phofme/Univ3-Frontiers.zip}
}

@article{HusainEtAl2014,
  title={Strong Expectations Cancel Locality Effects: {E}vidence from {H}indi},
  author={Husain, Samar and Vasishth, Shravan and Srinivasan, Narayanan},
  journal={PLoS ONE},
  volume={9},
  number={7},
  pages={1--14},
  year={2014},
  abstract = {Expectation-driven facilitation (Hale, 2001; Levy, 2008) and locality-driven retrieval difficulty (Gibson, 1998, 2000; Lewis &
Vasishth, 2005) are widely recognized to be two critical factors in incremental sentence processing; there is accumulating
evidence that both can influence processing difficulty. However, it is unclear whether and how expectations and memory
interact. We first confirm a key prediction of the expectation account: a Hindi self-paced reading study shows that when an
expectation for an upcoming part of speech is dashed, building a rarer structure consumes more processing time than
building a less rare structure. This is a strong validation of the expectation-based account. In a second study, we show that
when expectation is strong, i.e., when a particular verb is predicted, strong facilitation effects are seen when the appearance
of the verb is delayed; however, when expectation is weak, i.e., when only the part of speech ``verb'' is predicted but a
particular verb is not predicted, the facilitation disappears and a tendency towards a locality effect is seen. The interaction
seen between expectation strength and distance shows that strong expectations cancel locality effects, and that weak
expectations allow locality effects to emerge.},
  publisher={Public Library of Science},
  pdf = {http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0100986},
  code={http://www.ling.uni-potsdam.de/~vasishth/code/HusainEtAl2014PLoSONE.zip}
}

@Article{NicenboimEtAl2016Frontiersb,
  author   = {Bruno Nicenboim and Pavel Logačev and Carolina Gattei and Shravan Vasishth},
  title    = {When high-capacity readers slow down and low-capacity readers speed up: {W}orking memory and locality effects},
  doi      = {10.3389/fpsyg.2016.00280},
  issn     = {1664-1078},
  number   = {280},
  url      = {http://www.frontiersin.org/language_sciences/10.3389/fpsyg.2016.00280/abstract},
  volume   = {7},
  abstract = {We examined the effects of argument-head distance in SVO and SOV languages (Spanish and German), while taking into account readers’ working memory capacity and controlling for expectation (Levy, 2008) and other factors. We predicted only locality effects, that is, a slow-down produced by increased dependency distance (Gibson, 2000; Lewis & Vasishth, 2005). Furthermore, we expected stronger locality effects for readers with low working memory capacity. Contrary to our predictions, low-capacity readers showed faster reading with increased distance, while high-capacity readers showed locality effects. We suggest that while the locality effects are compatible with memory-based explanations, the speedup of low-capacity readers can be explained by an increased probability of retrieval failure. We present a computational model based on ACT-R built under the previous assumptions, which is able to give a qualitative account for the present data and can be tested in future research. Our results suggest that in some cases, interpreting longer RTs as indexing increased processing difficulty and shorter RTs as facilitation may be too simplistic: The same increase in processing difficulty may lead to slowdowns in high-capacity readers and speedups in low-capacity ones. Ignoring individual level capacity differences when investigating locality effects may lead to misleading conclusions.},
  journal  = {Frontiers in Psychology},
  year     = {2016},
}

@Article{Rouder2005,
  Title                    = {Are unshifted distributional models appropriate for response time?},
  Author                   = {Rouder, Jeffrey N.},
  Year                     = {2005},
  Doi                      = {10.1007/s11336-005-1297-7},
  ISSN                     = {1860-0980},
  Month                    = {Jun},
  Number                   = {2},
  Pages                    = {377–381},
  Url                      = {http://dx.doi.org/10.1007/s11336-005-1297-7},
  Volume                   = {70},

  File                     = {:Rouder-2005 shifted lognormal.pdf:PDF},
  Journal                  = {Psychometrika},
  Publisher                = {Springer Science + Business Media}
}
@Article{Lee2011,
  Title                    = {How cognitive modeling can benefit from hierarchical {Bayes}ian models},
  Author                   = {Lee, Michael D.},
  Year                     = {2011},
  Doi                      = {10.1016/j.jmp.2010.08.013},
  ISSN                     = {0022-2496},
  Month                    = {Feb},
  Number                   = {1},
  Pages                    = {1--7},
  Url                      = {http://dx.doi.org/10.1016/j.jmp.2010.08.013},
  Volume                   = {55},

  File                     = {:Lee-inpress HIERARCHICAL.pdf:PDF},
  Journal                  = {Journal of Mathematical Psychology},
  Publisher                = {Elsevier BV}
}


@book{LeeWagenmakers2014,
  title={Bayesian cognitive modeling: {A} practical course},
  author={Lee, Michael D. and Wagenmakers, Eric-Jan},
  year={2014},
  publisher={Cambridge University Press}
}

@article {LogacevVasishth2015,
author = {Logačev, Pavel and Vasishth, Shravan},
title = {A Multiple-Channel Model of Task-Dependent Ambiguity Resolution in Sentence Comprehension},
journal = {Cognitive Science},
volume = {40},
number = {2},
issn = {1551-6709},
url = {http://dx.doi.org/10.1111/cogs.12228},
doi = {10.1111/cogs.12228},
pages = {266--298},
keywords = {Sentence processing, Ambiguity, Parallel processing, Cognitive modeling, Unrestricted race model, URM, Underspecification, Good-enough processing},
year = {2016},
}

@article{grodner,
  Author = {Daniel Grodner and Edward Gibson},
  Journal = {Cognitive Science},
  Pages = {261--290},
  Title = {Consequences of the serial nature of linguistic input},
  Volume = {29},
  doi = {https://doi.org/10.1207/s15516709cog0000_7},
  Year = {2005}}

@unpublished{BatesEtAlParsimonious,
  Author = {Bates, Douglas M. and Kliegl, Reinhold and Vasishth, Shravan and Baayen, R. Harald},
  Note = {Unpublished manuscript},
	Title = {Parsimonious mixed models},
	Year = {2015},
  pdf = {http://arxiv.org/abs/1506.04967},
  	abstract = {The analysis of experimental data with mixed-effects models requires
decisions about the specification of the appropriate random-effects structure.
Recently, Barr, et al 2013, recommended  fitting `maximal'
models with all possible random effect components included.  Estimation of
maximal models, however, may not converge.  We show that failure to converge
 typically is not due to a suboptimal estimation algorithm, but is
a consequence of attempting to fit a model that is too complex to be properly
supported by the data, irrespective of whether estimation is based on maximum
likelihood or on Bayesian hierarchical modeling with uninformative or weakly
informative priors.  Importantly, even under convergence, overparameterization
may lead to uninterpretable models.  We provide diagnostic tools for detecting
overparameterization and guiding model simplification.  Finally, we clarify
that the simulations on which Barr et al. base their recommendations are
atypical for real data.  A detailed example is provided of how subject-related
attentional fluctuation across trials may further qualify
statistical inferences about fixed effects, and of how such nonlinear effects
can be accommodated within the mixed-effects modeling framework.}
}

@article{hannesBEAP,
  title={{Balancing Type I Error and Power in Linear Mixed Models}},
  author={Hannes Matuschek and Reinhold Kliegl and Shravan Vasishth and R. Harald Baayen and Douglas M. Bates},
  doi = {10.1016/j.jml.2017.01.001},
  pdf = {http://www.sciencedirect.com/science/article/pii/S0749596X17300013},
  abstract = {Linear mixed-effects models have increasingly replaced mixed-model analyses of variance for statistical inference in factorial psycholinguistic experiments. The advantages of LMMs over ANOVAs, however, come at a cost: Setting up an LMM is not as straightforward as running an ANOVA. One simple option, when numerically possible, is to fit the full variance-covariance structure of random effects (the maximal model; Barr et al., 2013), presumably to keep Type I error down to the nominal {$\alpha$} in the presence of random effects. Although it is true that fitting a model with only random intercepts may lead to higher Type I error, fitting a maximal model also has a cost: it can lead to a significant loss of power. We demonstrate this with simulations and suggest that for typical psychological and psycholinguistic data, models with a random effect structure that is supported by the data have optimal Type I error and power properties.},
  year={2017},
  volume={94},
  pages={305--315},
  journal ={Journal of Memory and Language}
}


@article{lewisvasishth:cogsci05,
  Author = {Richard L. Lewis and Shravan Vasishth},
  Journal = {Cognitive Science},
  Pages = {1--45},
  abstract = {We present a detailed process theory of the moment-by-moment working-memory retrievals and associated
  control structure that subserve sentence comprehension. The theory is derived from the application
  of independently motivated principles of memory and cognitive skill to the specialized task of sentence
  parsing. The resulting theory construes sentence processing as a series of skilled associative
  memory retrievals modulated by similarity-based interference and fluctuating activation. The cognitive
  principles are formalized in computational form in the Adaptive Control of Thought-Rational (ACT-R)
  architecture, and our process model is realized in ACT-R.We present the results of 6 sets of simulations:
  5 simulation sets provide quantitative accounts of the effects of length and structural interference on
  both unambiguous and garden-path structures. A final simulation set provides a graded taxonomy of
  double center embeddings ranging from relatively easy to extremely difficult. The explanation of center-
  embedding difficulty is a novel one that derives from the model‚Äôs complete reliance on discriminating
  retrieval cues in the absence of an explicit representation of serial order information. All fits were obtained
  with only 1 free scaling parameter fixed across the simulations; all other parameters were ACT-R
  defaults. The modeling results support the hypothesis that fluctuating activation and similarity-based interference
  are the key factors shaping working memory in sentence processing. We contrast the theory
  and empirical predictions with several related accounts of sentence-processing complexity.},
  Title = {An activation-based model of sentence processing as skilled memory retrieval},
  Volume = {29},
  Year = {2005},
  doi = { 10.1207/s15516709cog0000_25},
  pdf = {http://www.ling.uni-potsdam.de/~vasishth/pdfs/Lewis-VasishthCogSci2005.pdf},
  code = {http://www.ling.uni-potsdam.de/~vasishth/code/LewisVasishthModel05.tar.gz}
  }


@article{gibsonwu,
  title={Processing {C}hinese relative clauses in context},
  author={Gibson, Edward and Wu, H.-H. Iris},
  journal={Language and Cognitive Processes},
  volume={28},
  number={1-2},
  pages={125--155},
  doi = {https://doi.org/10.1080/01690965.2010.536656},
  year={2013},
  publisher={Taylor \& Francis}
}

@article{gibsonthomas99,
  Author = {Edward Gibson and James Thomas},
  Journal = {Language and Cognitive Processes},
  Pages = {225--248},
  Title = {Memory Limitations and Structural Forgetting: The Perception of Complex Ungrammatical Sentences as Grammatical},
  Volume = {14(3)},
  doi = {https://doi.org/10.1080/016909699386293},
  Year = {1999}}

  @incollection{meehl97,
    Address = {Mahwah, New Jersey},
    Author = {Paul E. Meehl},
    Booktitle = {What if there were no significance tests?},
    Editor = {L.L. Harlow and S.A. Mulaik and J. H. Steiger},
    Publisher = {Erlbaum},
    Title = {The Problem Is Epistemology, Not Statistics: {R}eplace Significance Tests by Confidence Intervals and Quantify Accuracy of Risky Numerical Predictions},
    Year = {1997}}


    @article{warren2002influence,
      title={The influence of referential processing on sentence complexity},
      author={Warren, Tessa and Gibson, Edward},
      journal={Cognition},
      volume={85},
      pages={79--112},
      year={2002}
    }

@incollection{gibson00,
  Address = {Cambridge, MA},
  Author = {Edward Gibson},
  Booktitle = {{Image, Language, Brain}: {Papers from the First Mind Articulation Project Symposium}},
  Editor = {Marantz, Alec and Miyashita, Yasushi and O'Neil, Wayne},
  Publisher = {MIT Press},
  Title = {Dependency Locality Theory: {A} Distance-Based Theory of Linguistic Complexity},
  Year = {2000}}

@Article{EngelmannJaegerVasishth2019,
  author  = {Engelmann, Felix and J{\"a}ger, Lena A. and Vasishth, Shravan},
  title   = {The effect of prominence and cue association in retrieval processes: {A} computational account},
  doi     = {10.1111/cogs.12800},
  issue   = {12},
  pages   = {e12800},
  volume  = {43},
  code    = {https://osf.io/b56qv/},
  journal = {Cognitive Science},
  year    = {2020},
}


@article{wahnPupilSizesScale2016,
  langid = {english},
  title = {Pupil Sizes Scale with Attentional Load and Task Experience in a Multiple Object Tracking Task},
  volume = {11},
  issn = {1932-6203},
  url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0168087},
  doi = {10.1371/journal.pone.0168087},
  abstract = {Previous studies have related changes in attentional load to pupil size modulations. However, studies relating changes in attentional load and task experience on a finer scale to pupil size modulations are scarce. Here, we investigated how these changes affect pupil sizes. To manipulate attentional load, participants covertly tracked between zero and five objects among several randomly moving objects on a computer screen. To investigate effects of task experience, the experiment was conducted on three consecutive days. We found that pupil sizes increased with each increment in attentional load. Across days, we found systematic pupil size reductions. We compared the model fit for predicting pupil size modulations using attentional load, task experience, and task performance as predictors. We found that a model which included attentional load and task experience as predictors had the best model fit while adding performance as a predictor to this model reduced the overall model fit. Overall, results suggest that pupillometry provides a viable metric for precisely assessing attentional load and task experience in visuospatial tasks.},
  number = {12},
  journal = {PLOS ONE},
  shortjournal = {PLOS ONE},
  urldate = {2019-11-26},
  year = {2016},
  pages = {e0168087},
  keywords = {Cognition,Memory,Attention,Eyes,Pupil,Reflexes,Target detection,Vision},
  author = {Wahn, Basil and Ferris, Daniel P. and Hairston, W. David and König, Peter}
}



@article{hayesMappingCorrectingInfluence2016,
  title = {Mapping and Correcting the Influence of Gaze Position on Pupil Size Measurements},
  volume = {48},
  issn = {1554-351X},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4637269/},
  doi = {10.3758/s13428-015-0588-x},
  abstract = {Pupil size is correlated with a wide variety of important cognitive variables and is increasingly being used by cognitive scientists. Pupil data can be recorded inexpensively and non-invasively by many commonly used video-based eye-tracking cameras. Despite the relative ease of data collection and increasing prevalence of pupil data in the cognitive literature, researchers often underestimate the methodological challenges associated with controlling for confounds that can result in misinterpretation of their data. One serious confound that is often not properly controlled is pupil foreshortening error (PFE)—the foreshortening of the pupil image as the eye rotates away from the camera. Here we systematically map PFE using an artificial eye model and then apply a geometric model correction. Three artificial eyes with different fixed pupil sizes were used to systematically measure changes in pupil size as a function of gaze position with a desktop EyeLink 1000 tracker. A grid-based map of pupil measurements was recorded with each artificial eye across three experimental layouts of the eye-tracking camera and display. Large, systematic deviations in pupil size were observed across all nine maps. The measured PFE was corrected by a geometric model that expressed the foreshortening of the pupil area as a function of the cosine of the angle between the eye-to-camera axis and the eye-to-stimulus axis. The model reduced the root mean squared error of pupil measurements by 82.5 \% when the model parameters were pre-set to the physical layout dimensions, and by 97.5 \% when they were optimized to fit the empirical error surface.},
  number = {2},
  journal = {Behavior research methods},
  shortjournal = {Behav Res Methods},
  urldate = {2019-11-27},
  year = {2016},
  pages = {510-527},
  author = {Hayes, Taylor R. and Petrov, Alexander A.},
  eprinttype = {pmid},
  eprint = {25953668},
  pmcid = {PMC4637269}
}



@article{wagenmakersRelationMeanVariance2005,
  langid = {english},
  title = {On the Relation between the Mean and the Variance of a Diffusion Model Response Time Distribution},
  volume = {49},
  issn = {0022-2496},
  url = {http://www.sciencedirect.com/science/article/pii/S0022249605000106},
  doi = {10.1016/j.jmp.2005.02.003},
  abstract = {Almost every empirical psychological study finds that the variance of a response time (RT) distribution increases with the mean. Here we present a theoretical analysis of the nature of the relationship between RT mean and RT variance, based on the assumption that a diffusion model (e.g., Ratcliff (1978) Psychological Review, 85, 59–108; Ratcliff (2002). Psychonomic Bulletin \& Review, 9, 278–291), adequately captures the shape of empirical RT distributions. We first derive closed-form analytic solutions for the mean and variance of a diffusion model RT distribution. Next, we study how systematic differences in two important diffusion model parameters simultaneously affect the mean and the variance of the diffusion model RT distribution. Within the range of plausible values for the drift rate parameter, the relation between RT mean and RT standard deviation is approximately linear. Manipulation of the boundary separation parameter also leads to an approximately linear relation between RT mean and RT standard deviation, but only for low values of the drift rate parameter.},
  number = {3},
  journal = {Journal of Mathematical Psychology},
  shortjournal = {Journal of Mathematical Psychology},
  urldate = {2019-11-28},
  year= {2005},
  pages = {195-204},
  author = {Wagenmakers, Eric-Jan and Grasman, Raoul P. P. P. and Molenaar, Peter C. M.}
}



@article{ulrichInformationProcessingModels1993,
  langid = {english},
  title = {Information Processing Models Generating Lognormally Distributed Reaction Times},
  volume = {37},
  issn = {00222496},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0022249683710321},
  doi = {10.1006/jmps.1993.1032},
  number = {4},
  journal = {Journal of Mathematical Psychology},
  urldate = {2019-05-17},
  year = {1993},
  pages = {513-525},
  author = {Ulrich, Rolf and Miller, Jeff}
}

@book{millermiller,
  title={John E. Freund's Mathematical Statistics with Applications},
  author={Miller, I. and Miller, M.},
  year={2004},
  publisher={Prentice Hall},
  address = {Upper Saddle River, NJ}
}

@book{RossProb,
  title={A first course in probability},
  author={Ross, Sheldon},
  year={2002},
  publisher={Pearson Education}
}


@article{limpertLognormalDistributionsSciences2001,
  langid = {english},
  title = {Log-Normal {{Distributions}} across the {{Sciences}}: {{Keys}} and {{Clues}}},
  volume = {51},
  issn = {0006-3568},
  url = {https://academic.oup.com/bioscience/article/51/5/341-352/243981},
  doi = {10.1641/0006-3568(2001)051[0341:LNDATS]2.0.CO;2},
  shorttitle = {Log-Normal {{Distributions}} across the {{Sciences}}},
  number = {5},
  journal = {BioScience},
  year = {2001},
  pages = {341},
  author = {Limpert, Eckhard and Stahel, Werner A. and Abbt, Markus}
}



@article{buzsakiLogdynamicBrainHow2014,
  langid = {english},
  title = {The Log-Dynamic Brain: How Skewed Distributions Affect Network Operations},
  volume = {15},
  issn = {1471-003X, 1471-0048},
  url = {http://www.nature.com/articles/nrn3687},
  doi = {10.1038/nrn3687},
  shorttitle = {The Log-Dynamic Brain},
  abstract = {We often assume that the variables of functional and structural brain parameters — such as synaptic weights, the firing rates of individual neurons, the synchronous discharge of neural populations, the number of synaptic contacts between neurons and the size of dendritic boutons —have a bell-shaped distribution. However, at many physiological and anatomical levels in the brain, the distribution of numerous parameters is in fact strongly skewed with a heavy tail, suggesting that skewed (typically lognormal) distributions are fundamental to structural and functional brain organization. This insight not only has implications for how we should collect and analyse data, it may also help us to understand how the different levels of skewed distributions —from synapses to cognition — are related to each other.},
  number = {4},
  journal= {Nature Reviews Neuroscience},
  urldate = {2019-11-28},
  year = {2014},
  pages = {264-278},
  author = {Buzsáki, György and Mizuseki, Kenji}
}

@book{utc,
	Address = {Cambridge: MA},
	Author = {Allen Newell},
	Publisher = {Harvard University Press},
	Title = {Unified Theories of Cognition},
	Year = {1990}}

@article{cw99,
	Author = {Caplan, David and Waters, G. S.},
	Journal = {Behavioral and Brain Science},
	Pages = {77--94},
	Title = {Verbal working memory and sentence comprehension},
	Volume = {22},
    doi = {https://doi.org/10.1017/S0140525X99001788},
	Year = {1999}}


@article{king1991individual,
  title={Individual differences in syntactic processing: The role of working memory},
  author={King, Jonathan and Just, Marcel Adam},
  journal={Journal of Memory and Language},
  volume={30},
  number={5},
  pages={580--602},
  year={1991},
  publisher={Elsevier}
}



@article{kelloScalingLawsCognitive2010,
  langid = {english},
  title = {Scaling Laws in Cognitive Sciences},
  volume = {14},
  issn = {1364-6613},
  url = {http://www.sciencedirect.com/science/article/pii/S136466131000046X},
  doi = {10.1016/j.tics.2010.02.005},
  abstract = {Scaling laws are ubiquitous in nature, and they pervade neural, behavioral and linguistic activities. A scaling law suggests the existence of processes or patterns that are repeated across scales of analysis. Although the variables that express a scaling law can vary from one type of activity to the next, the recurrence of scaling laws across so many different systems has prompted a search for unifying principles. In biological systems, scaling laws can reflect adaptive processes of various types and are often linked to complex systems poised near critical points. The same is true for perception, memory, language and other cognitive phenomena. Findings of scaling laws in cognitive science are indicative of scaling invariance in cognitive mechanisms and multiplicative interactions among interdependent components of cognition.},
  number = {5},
  journal = {Trends in Cognitive Sciences},
  shortjournal = {Trends in Cognitive Sciences},
  urldate = {2019-11-28},
  date = {2010-05-01},
  pages = {223-232},
  author = {Kello, Christopher T. and Brown, Gordon D. A. and Ferrer-i-Cancho, Ramon and Holden, John G. and Linkenkaer-Hansen, Klaus and Rhodes, Theo and Van Orden, Guy C.},
  file = {/home/bruno/Zotero/storage/Y3MM2FZ2/S136466131000046X.html}
}

@article{gordon01,
	Author = {P. C. Gordon and Randall Hendrick and Marcus Johnson},
	Journal = "Journal of Experimental Psychology: Learning, Memory, and Cognition",
	Pages = {1411--1423},
	Title = {Memory interference during language processing},
	Volume = {27},
    issue = {6},
    doi = { https://doi.org/10.1037/0278-7393.27.6.1411},
	Year = {2001}}


@book{vasishthbroe2ed,
  Address = {Heidelberg},
  Author = {Shravan Vasishth and Michael Broe},
  Publisher = {Springer},
  Year = {2021},
  Title = {The Foundations of Statistics: {A} Simulation-based Approach},
  edition = {Second},
  note = {In preparation}
  }


@article{nicenboimModelsRetrievalSentence2018,
  langid = {english},
  title = {Models of Retrieval in Sentence Comprehension: {{A}} Computational Evaluation Using {{Bayesian}} Hierarchical Modeling},
  volume = {99},
  issn = {0749596X},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0749596X16301577},
  doi = {10.1016/j.jml.2017.08.004},
  shorttitle = {Models of Retrieval in Sentence Comprehension},
  journal= {Journal of Memory and Language},
  urldate = {2019-05-17},
  year = {2018},
  pages = {1-34},
  author = {Nicenboim, Bruno and Vasishth, Shravan}
}

@article{oberauerWorkingMemoryCapacity2019,
  langid = {english},
  title = {Working Memory Capacity Limits Memory for Bindings},
  volume = {2},
  issn = {2514-4820},
  url = {http://www.journalofcognition.org/articles/10.5334/joc.86/},
  doi = {10.5334/joc.86},
  abstract = {Article: Working Memory Capacity Limits Memory for Bindings},
  number = {1},
  journal = {Journal of Cognition},
  year = {2019},
  pages = {40},
  author = {Oberauer, Klaus}
}

@Inproceedings{VasishthEtAl2017Modelling,
  Title                    = {Modelling dependency completion in sentence comprehension as a {Bayesian} hierarchical mixture process: {A} case study involving {Chinese} relative clauses},
  Author                   = {Shravan Vasishth and Nicolas Chopin and Robin Ryder and Bruno Nicenboim},
  eprint = {https://arxiv.org/abs/1702.00564v2},
       year = 2017,
    Booktitle={Proceedings of Cognitive Science Conference},
Location={London, UK},
  url                      = {https://arxiv.org/abs/1702.00564v2}
}


@Inproceedings{VasishthEtAl2017Feature,
   author = {Vasishth, Shravan and J{\"a}ger, Lena A. and Nicenboim, Bruno},
    title = "{Feature overwriting as a finite mixture process: {Evidence} from comprehension data}",
archivePrefix = "arXiv",
   eprint = {https://arxiv.org/abs/1703.04081},
     Booktitle={Proceedings of {MathPsych/ICCM} Conference},
Location={Warwick, UK},
      year = 2017,
  url = {https://arxiv.org/abs/1703.04081}

}


@article{mcclellandPlaceModelingCognitive2009,
  langid = {english},
  title = {The place of Modeling in {Cognitive Science}},
  volume = {1},
  issn = {17568757, 17568765},
  url = {http://doi.wiley.com/10.1111/j.1756-8765.2008.01003.x},
  doi = {10.1111/j.1756-8765.2008.01003.x},
  number = {1},
  journal= {Topics in Cognitive Science},
  urldate = {2019-05-17},
  year= {2009},
  pages = {11-38},
  author = {McClelland, James L.}
}



@article{mathotPupillometryPsychologyPhysiology2018,
  langid = {english},
  title = {Pupillometry: {Psychology}, Physiology, and Function},
  volume = {1},
  issn = {2514-4820},
  url = {http://www.journalofcognition.org/articles/10.5334/joc.18/},
  doi = {10.5334/joc.18},
  shorttitle = {Pupillometry},
  abstract = {Article: Pupillometry: Psychology, Physiology, and Function},
  number = {1},
  journal = {Journal of Cognition},
  urldate = {2019-12-12},
  year = {2018},
  pages = {16},
  author = {Mathot, Sebastiaan}
}

@Article{oberauerkliegel2001,
  author    = {Klaus Oberauer and Reinhold Kliegl},
  title     = {Beyond resources: {Formal} models of complexity effects and age differences in working memory},
  doi       = {10.1080/09541440042000278},
  eprint    = {https://doi.org/10.1080/09541440042000278},
  number    = {1-2},
  pages     = {187-215},
  url       = {https://doi.org/10.1080/09541440042000278},
  volume    = {13},
  journal   = {European Journal of Cognitive Psychology},
  publisher = {Routledge},
  year      = {2001},
}


@article{pylyshynTrackingMultipleIndependent1988,
  langid = {english},
  title = {Tracking Multiple Independent Targets: {{Evidence}} for a Parallel Tracking Mechanism},
  volume = {3},
  issn = {0169-1015, 1568-5683},
  url = {https://brill.com/view/journals/sv/3/3/article-p179_3.xml},
  doi = {10.1163/156856888X00122},
  shorttitle = {Tracking Multiple Independent Targets},
  abstract = {"Tracking multiple independent targets: Evidence for a parallel tracking mechanism*" published on 01 Jan 1988 by Brill.},
  number = {3},
  journal= {Spatial Vision},
  urldate = {2019-12-13},
  year = {1988},
  pages = {179-197},
  author = {Pylyshyn, Zenon W. and Storm, Ron W.}
}


@ARTICLE{Blumberg2015,

AUTHOR={Blumberg, Eric J. and Peterson, Matthew S. and Parasuraman, Raja},

TITLE={Enhancing multiple object tracking performance with noninvasive brain stimulation: a causal role for the anterior intraparietal sulcus},

JOURNAL={Frontiers in Systems Neuroscience},

VOLUME={9},

PAGES={3},

YEAR={2015},

URL={https://www.frontiersin.org/article/10.3389/fnsys.2015.00003},

DOI={10.3389/fnsys.2015.00003},

ISSN={1662-5137},

ABSTRACT={Multiple object tracking (MOT) is a complex task recruiting a distributed network of brain regions. There are also marked individual differences in MOT performance. A positive causal relationship between the anterior intraparietal sulcus (AIPS), an integral region in the MOT attention network and inter-individual variation in MOT performance has not been previously established. The present study used transcranial direct current stimulation (tDCS), a form of non-invasive brain stimulation, in order to examine such a causal link. Active anodal stimulation was applied to the right AIPS and the left dorsolateral prefrontal cortex (DLPFC) (and sham stimulation), an area associated with working memory (but not MOT) while participants completed a MOT task. Stimulation to the right AIPS significantly improved MOT accuracy more than the other two conditions. The results confirm a causal role of the AIPS in the MOT task and illustrate that tDCS has the ability to improve MOT performance.}
}

@article{hsiao03,
  Author = {Fanny Pai-Fang Hsiao and Edward Gibson},
  Journal = {Cognition},
  Pages = {3--27},
  Title = {Processing relative clauses in {C}hinese},
  Volume = {90},
  doi = {https://doi.org/10.1016/S0010-0277(03)00124-0},
  Year = {2003}}

@incollection{spectorPupils1990,
  title = {The {{Pupils}}},
  booktitle = {Clinical {{Methods}}: {{The History}}, {{Physical}}, and {{Laboratory Examinations}}},
  author = {Spector, Robert H.},
  editor = {Walker, H. Kenneth and Hall, W. Dallas and Hurst, J. Willis},
  year = {1990},
  edition = {3rd},
  publisher = {{Butterworths}},
  address = {{Boston}},
  abstract = {The normal pupil size in adults varies from 2 to 4 mm in diameter in bright light to 4 to 8 mm in the dark. The pupils are generally equal in size. They constrict to direct illumination (direct response) and to illumination of the opposite eye (conseqnsual response). The pupil dilates in the dark. Both pupils constrict when the eye is focused on a near object (accommodative response). The pupil is abnormal if it fails to dilate to the dark or fails to constrict to light or accommodation. The popular acronym PERRLA\textemdash{}pupils equal, round, and reactive to light and accommodation\textemdash{}is a convenient but incomplete description of pupillomotor function. It specifically omits important clinical data such as the actual size and shape of each pupil, the speed and extent of pupillary constriction, and the results of determining an afferent pupillary defect.},
  copyright = {Copyright \textcopyright{} 1990, Butterworth Publishers, a division of Reed Publishing.},
  file = {/home/bruno/ownCloud/Zotero/storage/CVS3DR6Z/NBK381.html},
  isbn = {978-0-409-90077-4},
  language = {eng},
  lccn = {NBK381},
  pmid = {21250222}
}

@book{kerns2014introduction,
  title={Introduction to Probability and Statistics Using R},
  author={Kerns, G.J.},
  edition = {Second Edition},
  year={2014}
  }


@Article{VSLK08,
  author =   {Shravan Vasishth and Katja Suckow and Richard L. Lewis and Sabine Kern},
  title =    {Short-term forgetting in sentence comprehension: {C}rosslinguistic evidence from head-final structures},
  journal =    {Language and Cognitive Processes},
  year =   {2011},
  OPTkey =   {},
  volume =   {25},
  OPTnumber =    {4},
  pages =    {533-567},
  OPTmonth =   {},
  OPTannote =    {},
  pdf = {http://www.ling.uni-potsdam.de/~vasishth/pdfs/Vasishth-Suckow-Lewis-Kern-LCP2010.pdf},
  abstract = {Seven experiments using self-paced reading and eyetracking suggest that omitting the middle verb in a double centre embedding leads to easier processing in English but leads to greater difficulty in German. One commonly accepted explanation for the English pattern‚Äîbased on data from offline acceptability ratings and due to Gibson and Thomas (1999)‚Äîis that working-memory overload leads the comprehender to forget the prediction of the upcoming verb phrase (VP), which reduces working-memory load. We show that this VP-forgetting hypothesis does an excellent job of explaining the English data, but cannot account for the German results. We argue that the English and German results can be explained by the parser's adaptation to the grammatical properties of the languages; in contrast to English, German subordinate clauses always have the verb in clause-final position, and this property of German may lead the German parser to maintain predictions of upcoming VPs more robustly compared to English. The evidence thus argues against language-independent forgetting effects in online sentence processing; working-memory constraints can be conditioned by countervailing influences deriving from grammatical properties of the language under study.},
  code = {https://osf.io/r3cg9/}
}


@article{breeDistributionProblemsolvingTimes1975,
  title = {The Distribution of Problem-Solving Times: {{An}} Examination of the Stages Model},
  shorttitle = {{{THE DISTRIBUTION OF PROBLEM}}-{{SOLVING TIMES}}},
  author = {Br{\'e}e, David S.},
  year = {1975},
  month = nov,
  volume = {28},
  pages = {177--200},
  issn = {00071102},
  doi = {10/cnx3q7},
  file = {/home/bruno/ownCloud/Zotero/storage/FQYQ4DL6/Brée - 1975 - THE DISTRIBUTION OF PROBLEM-SOLVING TIMES AN EXAM.pdf},
  journal = {British Journal of Mathematical and Statistical Psychology},
  language = {en},
  number = {2}
}

@article{ulrichEffectsTruncationReaction1994,
  title = {Effects of Truncation on Reaction Time Analysis.},
  author = {Ulrich, Rolf and Miller, Jeff},
  year = {1994},
  volume = {123},
  pages = {34--80},
  issn = {1939-2222, 0096-3445},
  doi = {10/b8tsnh},
  file = {/home/bruno/ownCloud/Zotero/storage/IFVQAFVX/Ulrich and Miller - Effects of Truncation on Reaction Time Analysis.pdf},
  journal = {Journal of Experimental Psychology: General},
  language = {en},
  number = {1}
}

@article{picton_etal_2000,
author = {Picton, T.W. and Bentin, S. and Berg, P. and Donchin, E. and Hillyard, Steven A. and Johnson J.R., R. and Miller, G.A. and Ritter, W. and Ruchkin, D.S. and Rugg, M.D. and Taylor, M.J.},
title = {Guidelines for using human event-related potentials to study cognition: {R}ecording standards and publication criteria},
journal = {Psychophysiology},
volume = {37},
number = {2},
pages = {127-152},
keywords = {Event-related potentials, Methods, Artifacts, Measurement, Statistics},
doi = {10.1111/1469-8986.3720127},
year={2000}
}


@article{mitchellEffectsContextContent1978,
  title = {The Effects of Context and Content on Immediate Processing in Reading},
  author = {Mitchell, Don C and Green, D. W.},
  year = {1978},
  month = nov,
  volume = {30},
  pages = {609--636},
  issn = {0033-555X},
  doi = {10.1080/14640747808400689},
  file = {/home/bruno/ownCloud/bib_papers/Mitchell and Green - 1978 - The effects of context and content on immediate pr.pdf},
  journal = {Quarterly Journal of Experimental Psychology},
  language = {en},
  number = {4}
}


@article{aaronsonPerformanceTheoriesSentence1976,
  title = {Performance Theories for Sentence Coding: {{Some}} Quantitative Evidence},
  shorttitle = {Performance Theories for Sentence Coding},
  author = {Aaronson, Doris and Scarborough, Hollis S.},
  year = {1976},
  volume = {2},
  pages = {56--70},
  issn = {0096-1523},
  doi = {10/bjfzn4},
  journal = {Journal of Experimental Psychology: Human Perception and Performance},
  language = {en},
  number = {1}
}


@article{schadHowCapitalizePriori2020,
  title = {How to Capitalize on a Priori Contrasts in Linear (Mixed) Models: {{A}} Tutorial},
  shorttitle = {How to Capitalize on a Priori Contrasts in Linear (Mixed) Models},
  author = {Schad, Daniel J. and Vasishth, Shravan and Hohenstein, Sven and Kliegl, Reinhold},
  year = {2020},
  month = feb,
  volume = {110},
  pages = {104038},
  issn = {0749-596X},
  doi = {10/gf9tjp},
  abstract = {Factorial experiments in research on memory, language, and in other areas are often analyzed using analysis of variance (ANOVA). However, for effects with more than one numerator degrees of freedom, e.g., for experimental factors with more than two levels, the ANOVA omnibus F-test is not informative about the source of a main effect or interaction. Because researchers typically have specific hypotheses about which condition means differ from each other, a priori contrasts (i.e., comparisons planned before the sample means are known) between specific conditions or combinations of conditions are the appropriate way to represent such hypotheses in the statistical model. Many researchers have pointed out that contrasts should be ``tested instead of, rather than as a supplement to, the ordinary `omnibus' F test'' (Hays, 1973, p. 601). In this tutorial, we explain the mathematics underlying different kinds of contrasts (i.e., treatment, sum, repeated, polynomial, custom, nested, interaction contrasts), discuss their properties, and demonstrate how they are applied in the R System for Statistical Computing (R Core Team, 2018). In this context, we explain the generalized inverse which is needed to compute the coefficients for contrasts that test hypotheses that are not covered by the default set of contrasts. A detailed understanding of contrast coding is crucial for successful and correct specification in linear models (including linear mixed models). Contrasts defined a priori yield far more useful confirmatory tests of experimental hypotheses than standard omnibus F-tests. Reproducible code is available from https://osf.io/7ukf6/.},
  file = {/home/bruno/ownCloud/Zotero/storage/9RW8825A/Schad et al. - 2020 - How to capitalize on a priori contrasts in linear .pdf;/home/bruno/ownCloud/Zotero/storage/KVXSVX27/S0749596X19300695.html},
  journal = {Journal of Memory and Language},
  keywords = {A priori hypotheses,Contrasts,Linear models,Null hypothesis significance testing},
  language = {en}
}


@book{faraway2002practical,
  title={Practical regression and {ANOVA using R}},
  author={Faraway, Julian James},
  volume={168},
  year={2002},
  publisher={Citeseer}
}

@book{faraway2016extending,
  title={Extending the linear model with R: generalized linear, mixed effects and nonparametric regression models},
  author={Faraway, Julian James},
  year={2016},
  publisher={Chapman and Hall/CRC}
}

@book{monty,
  title={An introduction to linear regression analysis},
  author={D. C. Montgomery and E. A. Peck and G. G. Vining},
  edition = {5th},
  year={2012},
  publisher={Wiley},
  address = {Hoboken, NJ}
}

@book{seber,
  title={Linear Regression Analysis},
  author={George A. F. Seber and Allen J. Lee},
  edition = {2nd Edition},
  year={2003},
  publisher={John Wiley and Sons},
  address = {Hoboken, NJ}
}

@book{salas2003calculus,
  title={Calculus: {O}ne and several variables},
  author={Salas, Saturnino L. and Etgen, Garret J. and Hille, Einar},
  year={2003},
  edition = {Ninth},
  publisher={John Wiley \& Sons}
}


@book{lynch2007introduction,
  title={Introduction to applied {B}ayesian statistics and estimation for social scientists},
  author={Lynch, Scott Michael},
  year={2007},
  publisher={Springer},
  address = {New York, NY}
}


@Inproceedings{Nicenboim2018StanCon,
  Title                    = {The implementation of a model of choice: {The} (truncated) linear ballistic accumulator},
  Author                   = {Nicenboim, Bruno},
  Booktitle                = {{StanCon}},
  Year                     = {2018},
doi = {10.5281/zenodo.1465990},
  Location                 = {Aalto University, Helsinki, Finland},
  Month                    = {8},
customa = {https://htmlpreview.github.io/?https://github.com/stan-dev/stancon_talks/blob/master/2018-helsinki/Contributed-Talks/nicenboim/LBA_stancon2018.html},
  customd = {https://www.youtube.com/watch?v=wcpjZC9AV84&t=1h44m33s}
}

@article{macleod1991half,
  title={Half a century of research on the {Stroop} effect: {An} integrative review.},
  author={MacLeod, Colin M},
  journal={Psychological bulletin},
  volume={109},
  number={2},
  pages={163},
  year={1991},
  doi = {https://doi.org/10.1037/0033-2909.109.2.163},
  publisher={American Psychological Association}
}
@article{stroop1935studies,
  title={Studies of interference in serial verbal reactions.},
  author={Stroop, J. Ridley},
  journal={Journal of experimental psychology},
  volume={18},
  number={6},
  pages={643},
  year={1935},
  publisher={Psychological Review Company}
}
@article{ManyLabs3,
title = "{Many Labs 3}: {Evaluating} participant pool quality across the academic semester via replication",
journal = "Journal of Experimental Social Psychology",
volume = "67",
pages = {68 - 82},
year = {2016},
note = "Special Issue: Confirmatory",
issn = "0022-1031",
doi = "https://doi.org/10.1016/j.jesp.2015.10.012",
url = "http://www.sciencedirect.com/science/article/pii/S0022103115300123",
author = "Charles R. Ebersole and Olivia E. Atherton and Aimee L. Belanger and Hayley M. Skulborstad and Jill M. Allen and Jonathan B. Banks and Erica Baranski and Michael J. Bernstein and Diane B.V. Bonfiglio and Leanne Boucher and Elizabeth R. Brown and Nancy I. Budiman and Athena H. Cairo and Colin A. Capaldi and Christopher R. Chartier and Joanne M. Chung and David C. Cicero and Jennifer A. Coleman and John G. Conway and William E. Davis and Thierry Devos and Melody M. Fletcher and Komi German and Jon E. Grahe and Anthony D. Hermann and Joshua A. Hicks and Nathan Honeycutt and Brandon Humphrey and Matthew Janus and David J. Johnson and Jennifer A. Joy-Gaba and Hannah Juzeler and Ashley Keres and Diana Kinney and Jacqeline Kirshenbaum and Richard A. Klein and Richard E. Lucas and Christopher J.N. Lustgraaf and Daniel Martin and Madhavi Menon and Mitchell Metzger and Jaclyn M. Moloney and Patrick J. Morse and Radmila Prislin and Timothy Razza and Daniel E. Re and Nicholas O. Rule and Donald F. Sacco and Kyle Sauerberger and Emily Shrider and Megan Shultz and Courtney Siemsen and Karin Sobocko and R. [Weylin Sternglanz] and Amy Summerville and Konstantin O. Tskhay and Zack [van Allen] and Leigh Ann Vaughn and Ryan J. Walker and Ashley Weinberg and John Paul Wilson and James H. Wirth and Jessica Wortman and Brian A. Nosek",
keywords = "Social psychology, Cognitive psychology, Replication, Participant pool, Individual differences, Sampling effects, Situational effects",
abstract = "The university participant pool is a key resource for behavioral research, and data quality is believed to vary over the course of the academic semester. This crowdsourced project examined time of semester variation in 10 known effects, 10 individual differences, and 3 data quality indicators over the course of the academic semester in 20 participant pools (N=2696) and with an online sample (N=737). Weak time of semester effects were observed on data quality indicators, participant sex, and a few individual differences—conscientiousness, mood, and stress. However, there was little evidence for time of semester qualifying experimental or correlational effects. The generality of this evidence is unknown because only a subset of the tested effects demonstrated evidence for the original result in the whole sample. Mean characteristics of pool samples change slightly during the semester, but these data suggest that those changes are mostly irrelevant for detecting effects."
}

@misc{betancourt2016identifying,
    title={Identifying the Optimal Integration Time in Hamiltonian Monte Carlo},
    author={Michael J. Betancourt},
    year={2016},
    eprint={1601.00225},
    archivePrefix={arXiv},
    primaryClass={stat.ME}
}

@misc{betancourt2017conceptual,
    title={A Conceptual Introduction to Hamiltonian Monte Carlo},
    author={Michael J. Betancourt},
    year={2017},
    eprint={1701.02434},
    archivePrefix={arXiv},
    primaryClass={stat.ME}
}

@article{monnahanFasterEstimationBayesian2017,
  title = {Faster Estimation of {{Bayesian}} Models in Ecology Using {{Hamiltonian Monte Carlo}}},
  author = {Monnahan, Cole C. and Thorson, James T. and Branch, Trevor A.},
  editor = {O'Hara, Robert B.},
  journal = {Methods in Ecology and Evolution},
  volume = {8},
  pages = {339--348},
  issn = {2041210X},
  doi = {10.1111/2041-210X.12681},
  url = {http://doi.wiley.com/10.1111/2041-210X.12681},
  year = {2017},
  langid = {english},
  number = {3}
}

@misc{vehtari2019ranknormalization,
    title={Rank-normalization, folding, and localization: {An} improved $\widehat{R}$ for assessing convergence of {MCMC}},
    author={Aki Vehtari and Andrew Gelman and Daniel P. Simpson and Bob Carpenter and Paul-Christian Bürkner},
    year={2019},
    eprint={1903.08008},
    archivePrefix={arXiv},
    primaryClass={stat.CO}
}

@article{RabePaapeJML2023,
  title={{SEAM}: {A}n Integrated Activation-Coupled Model of Sentence Processing and Eye Movements in Reading},
  author={Maximilian M. Rabe and Dario Paape and Daniela Metzen and Shravan Vasishth and Ralf Engbert},
  year={2024},
  journal = {Journal of Memory and Language},
  doi = {https://doi.org/10.1016/j.jml.2023.104496},
  pdf = {https://arxiv.org/abs/2303.05221},
  code = {https://osf.io/8zrxb/}
}

@article{engbertetal06,
	Author = {Ralf Engbert and Antje Nuthmann and Eike M. Richter and Reinhold Kliegl},
	Journal = {Psychological Review},
	Pages = {777-813},
	Title = {{SWIFT}: {A} dynamical model of saccade generation during reading},
	Volume = {112},
    doi={https://doi.org/10.1037/0033-295X.112.4.777},
	Year = {2005}}


@article{Rabe2019,
  Author = {Maximilian M. Rabe and Johan Chandra and Andr{\'e} Kr{\"u}gel and Stefan A. Seelig and Shravan Vasishth and Ralf Engbert},
  journal = {Psychological Review},
  Title = {A {B}ayesian Approach to Dynamical Modeling of Eye-movement Control in Reading of Normal, Mirrored, and Scrambled Texts},
  Year = {2021},
  doi = {10.1037/rev0000268},
  url = {https://psyarxiv.com/nw2pb/},
  code = {https://osf.io/t9sbf/}
  }

  @article{pvals,
  author = {Ronald L. Wasserstein and Nicole A. Lazar},
  title = {{The ASA's Statement on p-Values: Context, Process, and Purpose}},
  journal = {The American Statistician},
  volume = {70},
  number = {2},
  pages = {129-133},
  year  = {2016},
  publisher = {Taylor & Francis},
  OPTdoi = {10.1080/00031305.2016.1154108}
  }


  @article{plummer2022simulation,
    title={Simulation-Based Bayesian Analysis},
    author={Plummer, Martin},
    year={2022},
    journal ={Annual Review of Statistics and Its Application}
  }


  @article{tabor1999dynamical,
    title={Dynamical models of sentence processing},
    author={Tabor, Whitney and Tanenhaus, Michael K.},
    journal={Cognitive Science},
    volume={23},
    number={4},
    pages={491--515},
    year={1999},
    publisher={Wiley Online Library}
  }


@article{smith2021encoding,
  title={Encoding interference effects support self-organized sentence processing},
  author={Smith, Garrett and Franck, Julie and Tabor, Whitney},
  journal={Cognitive Psychology},
  volume={124},
  pages={101356},
  year={2021},
  publisher={Elsevier}
}


@article{yadavindiff2021,
title={Individual differences in cue weighting in sentence comprehension: {An evaluation using Approximate Bayesian Computation}},
  author={Himanshu Yadav and Dario Paape and Garrett Smith and Brian W. Dillon and Shravan Vasishth},
  year = {2022},
  journal = {Open Mind},
  pdf = {https://psyarxiv.com/4jdu5/},
  doi = {https://doi.org/10.1162/opmi_a_00052},
  code = {https://osf.io/3na9q/}
  }


@article{papaspiliopoulos2007,
author = "Papaspiliopoulos, Omiros and Roberts, Gareth O. and Sköld, Martin",
doi = "10.1214/088342307000000014",
fjournal = "Statistical Science",
journal = "Statistical Science",
month = "02",
number = "1",
pages = "59--73",
publisher = "The Institute of Mathematical Statistics",
title = "A General Framework for the Parametrization of Hierarchical Models",
url = "https://doi.org/10.1214/088342307000000014",
volume = "22",
year = "2007"
}

@article{neal2003,
author = "Neal, Radford M.",
doi = "10.1214/aos/1056562461",
journal = "Annals of Statistics",
month = "06",
number = "3",
pages = "705--767",
publisher = "The Institute of Mathematical Statistics",
title = "Slice sampling",
url = "https://doi.org/10.1214/aos/1056562461",
volume = "31",
year = "2003"
}

@book{lambert2018student,
  title={A Student’s Guide to {B}ayesian Statistics},
  author={Lambert, Ben},
  year={2018},
  publisher={Sage},
  address = {London, UK}
}


@article{henrich_heine_norenzayan_2010,
title={The weirdest people in the world?},
volume={33},
DOI={10.1017/S0140525X0999152X},
number={2-3},
journal={Behavioral and Brain Sciences}, publisher={Cambridge University Press},
author={Henrich, Joseph and Heine, Steven J. and Norenzayan, Ara},
year={2010},
pages={61–83}}

@book{jaynes2003probability,
  title={Probability theory: {T}he logic of science},
  author={Jaynes, Edwin T.},
  year={2003},
  publisher={Cambridge University Press}
}

@phdthesis{AnnaLphd,
  Type = {dissertation},
  Title = {Similarity-based interference and faulty encoding accounts of sentence processing},
  Author = {Anna Laurinavichyute},
  School = {University of Potsdam},
  url = {https://publishup.uni-potsdam.de/frontdoor/index/index/docId/50966},
  Year = {2020},
}


@article{lisson_2020,
  Author = {Paula Lissón and Dorothea Pregla and Bruno Nicenboim and Dario Paape and Mick van het Nederend and Frank Burchert and Nicole Stadie and David Caplan and Shravan Vasishth},
  journal = {Cognitive Science},
  Title = {A computational evaluation of two models of retrieval processes in sentence processing in aphasia},
  url = {https://onlinelibrary.wiley.com/doi/full/10.1111/cogs.12956},
  volume = {45},
  issue = {4},
  pages = {e12956},
  Year = {2021}
  }

  @article{tendeiro,
  title={Diagnosing the misuse of the Bayes factor in Applied Research},
  author={Tendeiro, Jorge N. and Kiers, Henk A.L. and Hoekstra, Rink and Wong, Tsz Keung and Morey, Richard D.},
  journal={Advances in Methods and Practices in Psychological Science},
  year={2023},
  publisher={SAGE Publications}
}


  @article{LissonEtAlInterference2021,
    author      = {Paula Liss{\'o}n and Dario Paape and Dorothea Pregla and Frank Burchert and Nicole Stadie and Shravan Vasishth},
    title       = {Similarity-based interference in sentence comprehension in aphasia: {A} computational evaluation of two models of cue-based retrieval},
    year        = {2023},
    journal = {Computational Brain and Behavior},
    doi = {10.1007/s42113-023-00168-3},
    osf = {https://osf.io/2aetr/}
  }

 @article{spector1991potential,
    title={The potential and limitations of meta-analysis},
    author={Spector, Tim D. and Thompson, Simon G.},
    journal={Journal of Epidemiology and Community Health},
    volume={45},
    number={2},
    pages={89},
    year={1991},
    publisher={BMJ Publishing Group}
  }


@article{YadavetalJML2022,
  title={Number feature distortion modulates cue-based retrieval in reading},
    author={Himanshu Yadav and Garrett Smith and Sebastian Reich and Shravan Vasishth},
    year = {2023},
    code = {https://osf.io/gqj3p/},
    volume = {129},
    doi = {10.1016/j.jml.2022.104400},
    journal = {Journal of Memory and Language}
    }


@article{yadavencret2021,
  Author = {Himanshu Yadav and Garrett Smith and Shravan Vasishth},
  journal = {Proceedings of the Cognitive Science conference},
  Title = {Feature encoding modulates cue-based retrieval: {M}odeling interference effects in both grammatical and ungrammatical sentences},
  Year = {2021},
  pdf = {https://psyarxiv.com/76aex/}
  }

@article{yadaviccm2021,
  Author = {Himanshu Yadav and Garrett Smith and Shravan Vasishth},
  journal = {Proceedings of the International Conference on Cognitive Modeling},
  Title = {Is similarity-based interference caused by lossy compression or cue-based retrieval? {A} computational evaluation},
  Year = {2021},
  pdf = {https://psyarxiv.com/3et95/}
  }

@unpublished{yadavindiff2021,
  Author = {Himanshu Yadav and Dario Paape and Garrett Smith and Brian W. Dillon and Shravan Vasishth},
  Note = {Under revision with Open Mind},
  Title = {Individual differences in cue-weighting in sentence comprehension: An evaluation using Approximate Bayesian Computation},
  Year = {2021},
  pdf = {https://psyarxiv.com/4jdu5/}
  }

@article{PaapeEtAlMPT2020,
  Author = {Dario Paape and Serine Avetisyan and Sol Lago and Shravan Vasishth},
  Title = {Modeling misretrieval and feature substitution in agreement attraction: {A} computational evaluation},
  Year = {2021},
  journal = {Cognitive Science},
  volume = {45},
  issue = {8},
  doi = {https://doi.org/10.1111/cogs.13019}
  }


@article{SorensenVasishthTutorial,
  title = {{B}ayesian linear mixed models using {S}tan: {A} tutorial for psychologists, linguists,
and cognitive scientists},
  year = {2016},
  author = {Tanner Sorensen and Sven Hohenstein and Shravan Vasishth},
  journal = {Quantitative Methods for Psychology},
  pdf={http://www.tqmp.org/RegularArticles/vol12-3/p175/p175.pdf},
  abstract = {With the arrival of the R packages \texttt{nlme} and \texttt{lme4}, linear mixed models (LMMs) have come to be widely used in experimentally-driven areas like psychology, linguistics, and cognitive science. This tutorial provides a practical introduction to fitting LMMs in a Bayesian framework using the probabilistic programming language Stan. We choose Stan (rather than WinBUGS or JAGS) because it provides an elegant and scalable framework for fitting models in most of the standard applications of LMMs. We ease the reader into fitting increasingly complex LMMs, using a two-condition repeated measures self-paced reading study.},
  volume={12},
  number={3},
  pages={175-200}
}

@book{busemeyer2010cognitive,
  title={Cognitive modeling},
  author={Busemeyer, Jerome R and Diederich, Adele},
  year={2010},
  publisher={Sage}
}

@book{farrell2018computational,
  title={Computational modeling of cognition and behavior},
  author={Farrell, Simon and Lewandowsky, Stephan},
  year={2018},
  publisher={Cambridge University Press}
}


@inproceedings{logacev-dokudan-2021-multinomial,
    title = "A Multinomial Processing Tree Model of {RC} Attachment",
    author = "Loga{\v c}ev, Pavel  and Dokudan, Noyan",
    booktitle = "Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics",
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2021.cmcl-1.4",
    pages = "39--47"
}


@article{navarroDevilDeepBlue2018,
  title = {Between the {{Devil}} and the {{Deep Blue Sea}}: {{Tensions Between Scientific Judgement}} and {{Statistical Model Selection}}},
  author = {Navarro, Danielle J.},
  year = {2019},
  journal = {Computational Brain \& Behavior},
  issn = {2522-0861, 2522-087X},
  doi = {10.1007/s42113-018-0019-z},
  url = {http://link.springer.com/10.1007/s42113-018-0019-z},
  volume={2},
  number={1},
  pages={28--34}
 }

@article{GneitingRaftery2007,
author = {Tilmann Gneiting and Adrian E. Raftery},
title = {Strictly Proper Scoring Rules, Prediction, and Estimation},
journal = {Journal of the American Statistical Association},
volume = {102},
number = {477},
pages = {359-378},
year  = {2007},
publisher = {Taylor & Francis},
doi = {10.1198/016214506000001437},
URL = {https://doi.org/10.1198/016214506000001437},
eprint = {https://doi.org/10.1198/016214506000001437}
}

@article{Good1952,
 ISSN = {00359246},
 URL = {http://www.jstor.org/stable/2984087},
 abstract = {This paper deals first with the relationship between the theory of probability and the theory of rational behaviour. A method is then suggested for encouraging people to make accurate probability estimates, a connection with the theory of information being mentioned. Finally Wald's theory of statistical decision functions is summarised and generalised and its relation to the theory of rational behaviour is discussed.},
 author = {I. J. Good},
 journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
 number = {1},
 pages = {107--114},
 publisher = {[Royal Statistical Society, Wiley]},
 title = {Rational Decisions},
 volume = {14},
 year = {1952}
}
@article{piironenComparisonBayesianPredictive2017,
  title = {Comparison of {{Bayesian}} Predictive Methods for Model Selection},
  author = {Piironen, Juho and Vehtari, Aki},
  year = {2017},
  journal = {Statistics and Computing},
  volume = {27},
  pages = {711--735},
  issn = {0960-3174, 1573-1375},
  doi = {10.1007/s11222-016-9649-y},
  url = {http://link.springer.com/10.1007/s11222-016-9649-y},
   number = {3}
}

@article{gronauRejoinderMoreLimitations,
  title={Rejoinder: {M}ore limitations of {B}ayesian leave-one-out cross-validation},
  author={Gronau, Quentin F. and Wagenmakers, Eric-Jan},
  journal={Computational Brain \& Behavior},
  volume={2},
  number={1},
  pages={35--47},
  year={2019},
  doi = { https://doi.org/10.1007/s42113-018-0022-4},
  publisher={Springer}
}


@article{DutilhEtAl2011,
author = {Dutilh, Gilles and Wagenmakers, Eric-Jan and Visser, Ingmar and van der Maas, Han L.J.},
title = {A Phase Transition Model for the Speed-Accuracy Trade-Off in Response Time Experiments},
journal = {Cognitive Science},
volume = {35},
number = {2},
pages = {211-250},
keywords = {Reponse times, Speed-accuracy trade-off, Sequential sampling models, Fast guess model, Phase transitions, Catastrophe theory},
doi = {10.1111/j.1551-6709.2010.01147.x},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1551-6709.2010.01147.x},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1551-6709.2010.01147.x},
abstract = {Abstract Most models of response time (RT) in elementary cognitive tasks implicitly assume that the speed-accuracy trade-off is continuous: When payoffs or instructions gradually increase the level of speed stress, people are assumed to gradually sacrifice response accuracy in exchange for gradual increases in response speed. This trade-off presumably operates over the entire range from accurate but slow responding to fast but chance-level responding (i.e., guessing). In this article, we challenge the assumption of continuity and propose a phase transition model for RTs and accuracy. Analogous to the fast guess model (Ollman, 1966), our model postulates two modes of processing: a guess mode and a stimulus-controlled mode. From catastrophe theory, we derive two important predictions that allow us to test our model against the fast guess model and against the popular class of sequential sampling models. The first prediction—hysteresis in the transitions between guessing and stimulus-controlled behavior—was confirmed in an experiment that gradually changed the reward for speed versus accuracy. The second prediction—bimodal RT distributions—was confirmed in an experiment that required participants to respond in a way that is intermediate between guessing and accurate responding.},
year = {2011}
}

@article{Dutilh2019quality,
  title={The quality of response time data inference: {A} blinded, collaborative assessment of the validity of cognitive models},
  author={Dutilh, Gilles and Annis, Jeffrey and Brown, Scott D. and Cassey, Peter and Evans, Nathan J. and Grasman, Raoul P. P. P. and Hawkins, Guy E. and Heathcote, Andrew and Holmes, William R. and Krypotos, Angelos-Miltiadis and  Colin N. Kupitz and Fábio P. Leite and Veronika Lerche and Yi-Shin Lin and Gordon D. Logan and Thomas J. Palmeri and Jeffrey J. Starns and Jennifer S. Trueblood and Leendert van Maanen and Don van Ravenzwaaij and Joachim Vandekerckhove and Ingmar Visser and Andreas Voss and Corey N. White and Thomas V. Wiecki and J{\"o}rg Rieskamp and Christopher Donkin},
  journal={Psychonomic Bulletin \& Review},
  volume={26},
  number={4},
  doi = {https://doi.org/10.3758/s13423-017-1417-2},
  pages={1051--1069},
  year={2019},
  publisher={Springer}
}
@article{Cooketal2006,
author = {Samantha R. Cook and Andrew Gelman and Donald B. Rubin},
title = {Validation of Software for {Bayesian} Models Using Posterior Quantiles},
journal = {Journal of Computational and Graphical Statistics},
volume = {15},
number = {3},
pages = {675-692},
year  = {2006},
publisher = {Taylor & Francis},
doi = {10.1198/106186006X136976},
URL = {https://doi.org/10.1198/106186006X136976}, eprint = {https://doi.org/10.1198/106186006X136976}

}

@article{talts2018validating,
  title={Validating {B}ayesian inference algorithms with simulation-based calibration},
  author={Talts, Sean and Betancourt, Michael J. and Simpson, Daniel P. and Vehtari, Aki and Gelman, Andrew},
  journal={arXiv preprint arXiv:1804.06788},
  year={2018}
}
@article{wickelgren1977speed,
  title={Speed-accuracy tradeoff and information processing dynamics},
  author={Wickelgren, Wayne A},
  journal={Acta psychologica},
  volume={41},
  number={1},
  pages={67--85},
  year={1977}
}
@article{britten_shadlen_newsome_movshon_1993,
title={Responses of neurons in macaque {MT} to stochastic motion signals},
volume={10}, DOI={10.1017/S0952523800010269},
number={6},
journal={Visual Neuroscience},
publisher={Cambridge University Press},
author={Britten, Kenneth H. and Shadlen, Michael N. and Newsome, William T. and Movshon, J. Anthony},
year={1993}, pages={1157–1169}
}

@article{Ollman1966,
  title={Fast guesses in choice reaction time},
  author={Ollman, Robert},
  journal={Psychonomic Science},
  volume={6},
  number={4},
  pages={155--156},
  year={1966},
  doi = {https://doi.org/10.3758/BF03328004},
  publisher={Springer}
}
@article{Mcelree2000,
  title={Sentence comprehension is mediated by content-addressable memory structures},
  author={McElree, Brian},
  journal={Journal of Psycholinguistic Research},
  volume={29},
  number={2},
  pages={111--123},
  year={2000},
  doi = {https://doi.org/10.1023/A:1005184709695},
  publisher={Springer}
}
@article{Ratcliff1978,
  title={A theory of memory retrieval.},
  author={Ratcliff, Roger},
  journal={Psychological review},
  volume={85},
  number={2},
  pages={59},
  year={1978},
  doi = { https://doi.org/10.1037/0033-295X},
  publisher={American Psychological Association}
}
@article{brownSimplestCompleteModel2008,
  title = {The Simplest Complete Model of Choice Response Time: {{Linear}} Ballistic Accumulation},
  shorttitle = {The Simplest Complete Model of Choice Response Time},
  author = {Brown, Scott D. and Heathcote, Andrew},
  year = {2008},
  journal = {Cognitive Psychology},
  volume = {57},
  pages = {153--178},
  issn = {00100285},
  doi = {10.1016/j.cogpsych.2007.12.002},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0010028507000722},
  urldate = {2019-05-17},
  abstract = {We propose a linear ballistic accumulator (LBA) model of decision making and reaction time. The LBA is simpler than other models of choice response time, with independent accumulators that race towards a common response threshold. Activity in the accumulators increases in a linear and deterministic manner. The simplicity of the model allows complete analytic solutions for choices between any number of alternatives. These solutions (and freely-available computer code) make the model easy to apply to both binary and multiple choice situations. Using data from five previously published experiments, we demonstrate that the LBA model successfully accommodates empirical phenomena from binary and multiple choice tasks that have proven difficult for other theoretical accounts. Our results are encouraging in a field beset by the tradeoff between complexity and completeness.},
  file = {/home/bruno/ownCloud/bib_papers/Brown_Heathcote_2008_The simplest complete model of choice response time.pdf},
  langid = {english},
  number = {3}
}
@article{Ratcliff2016,
title = "Diffusion Decision Model: Current Issues and History",
journal = "Trends in Cognitive Sciences",
volume = "20",
number = "4",
pages = "260 - 281",
year = "2016",
issn = "1364-6613",
doi = "https://doi.org/10.1016/j.tics.2016.01.007",
url = "http://www.sciencedirect.com/science/article/pii/S1364661316000255",
author = "Roger Ratcliff and Philip L. Smith and Scott D. Brown and Gail McKoon",
keywords = "diffusion model, response time, optimality, nonstationarity",
abstract = "There is growing interest in diffusion models to represent the cognitive and neural processes of speeded decision making. Sequential-sampling models like the diffusion model have a long history in psychology. They view decision making as a process of noisy accumulation of evidence from a stimulus. The standard model assumes that evidence accumulates at a constant rate during the second or two it takes to make a decision. This process can be linked to the behaviors of populations of neurons and to theories of optimality. Diffusion models have been used successfully in a range of cognitive tasks and as psychometric tools in clinical research to examine individual differences. In this review, we relate the models to both earlier and more recent research in psychology."
}

@article{levy2013expectation,
  title={Expectation and locality effects in {G}erman verb-final structures},
  author={Levy, Roger P. and Keller, Frank},
  journal={Journal of Memory and Language},
  volume={68},
  number={2},
  pages={199--222},
  year={2013},
  publisher={Elsevier}
}


@article{levy1993eye,
  title={Eye tracking dysfunction and schizophrenia: {A} critical perspective},
  author={Levy, Deborah L. and Holzman, Philip S. and Matthysse, Steven and Mendell, Nancy R.},
  journal={Schizophrenia Bulletin},
  volume={19},
  number={3},
  pages={461--536},
  year={1993},
  doi = {https://doi.org/10.1093/schbul/19.3.461},
  publisher={Oxford University Press}
}
@article{DingEtAl,
    author = {Han, Ding and Wegrzyn, Jana and Bi, Hua and Wei, Ruihua and Zhang, Bin and Li, Xiaorong},
    journal = {PLOS ONE},
    publisher = {Public Library of Science},
    title = {Practice makes the deficiency of global motion detection in people with pattern-related visual stress more apparent},
    year = {2018},
    month = {02},
    volume = {13},
    url = {https://doi.org/10.1371/journal.pone.0193215},
    pages = {1-13},
    abstract = {Aims Pattern-related visual stress (PRVS) refers to the perceptual difficulties experienced by some individuals when exposed to high contrast striped patterns. People with PRVS were reported to have reduced sensitivity to global motion at baseline testing and the difference disappears at a second estimate. The present study was to investigate the effect of practice on global motion threshold in adults with and without PRVS.   Methods A total of 101 subjects were recruited and the Wilkins & Evans Pattern Glare Test was used to determine if a subject had PRVS. The threshold to detect global motion was measured with a random dot kinematogram. Each subject was measured 5 times at the first visit and again a month later. Receiver operating characteristic (ROC) curve analysis was applied to show the agreement between the two tests.   Results Twenty-nine subjects were classified as having PRVS and 72 were classified as normal. At baseline, the threshold to detect global motion was significantly higher in subjects with PRVS (0.832 ± 0.098 vs. 0.618 ± 0.228, p < 0.001). After 5 sessions, the difference between the normal and subjects with PRVS increased (0.767 ± 0.170 vs. 0.291 ± 0.149, p < 0.001). In ROC analysis, the area under the curve (AUC) improved from 0.792 at baseline to 0.964 at the fifth session. After a one-month break, the difference between normal and subjects with PRVS was still significant (0.843 ± 0.169 vs. 0.407 ± 0.216, p < 0.001) and the AUC was 0.875.   Conclusion The ability to detect global motion is impaired in persons with PRVS and the difference increased after additional sessions of practice.},
    number = {2},
    doi = {10.1371/journal.pone.0193215}
}

@article{TverskyKahneman1983,
  title={Extensional versus intuitive reasoning: The conjunction fallacy in probability judgment.},
  author={Tversky, Amos and Kahneman, Daniel},
  journal={Psychological review},
  volume={90},
  number={4},
  pages={293},
  year={1983},
  publisher={American Psychological Association}
}
@article{Paolaccietal,
  title={Running experiments on amazon mechanical turk},
  author={Paolacci, Gabriele and Chandler, Jesse and Ipeirotis, Panagiotis G},
  journal={Judgment and Decision making},
  volume={5},
  number={5},
  pages={411--419},
  doi = {10.1017/S1930297500002205},
  year={2010}
}

@InProceedings{turing,
  author    = {Hong Ge and Kai Xu and Zoubin Ghahramani},
  booktitle = {Proceedings of Machine Learning Research},
  title     = {Turing: {A} Language for Flexible Probabilistic Inference},
  editor    = {Amos Storkey and Fernando Perez-Cruz},
  pages     = {1682--1690},
  publisher = {PMLR},
  url       = {http://proceedings.mlr.press/v84/ge18b.html},
  volume    = {84},
  abstract  = {Probabilistic programming promises to simplify and democratize probabilistic machine learning, but successful probabilistic programming systems require flexible, generic and efficient inference engines. In this work, we present a system called Turing for building MCMC algorithms for probabilistic programming inference. Turing has a very simple syntax and makes full use of the numerical capabilities in the Julia programming language, including all implemented probability distributions, and automatic differentiation. Turing supports a wide range of popular Monte Carlo algorithms, including Hamiltonian Monte Carlo (HMC), HMC with No-U-Turns (NUTS), Gibbs sampling, sequential Monte Carlo (SMC), and several particle MCMC (PMCMC) samplers. Most importantly, Turing inference is composable: it combines MCMC operations on subsets of variables, for example using a combination of an HMC engine and a particle Gibbs (PG) engine. We explore several combinations of inference methods with the aim of finding approaches that are both efficient and universal, i.e. applicable to arbitrary probabilistic models. NUTS—a popular variant of HMC that adapts Hamiltonian simulation path length automatically, although quite powerful for exploring differentiable target distributions, is however not universal. We identify some failure modes for the NUTS engine, and demonstrate that composition of PG (for discrete variables) and NUTS (for continuous variables) can be useful when the NUTS engine is either not applicable, or simply does not work well. Our aim is to present Turing and its composable inference engines to the world and encourage other researchers to build on this system to help advance the field of probabilistic machine learning.},
  address   = {Playa Blanca, Lanzarote, Canary Islands},
  pdf       = {http://proceedings.mlr.press/v84/ge18b/ge18b.pdf},
  year      = {2018},
}

@article{batchelder1990multinomial,
  title={Multinomial processing models of source monitoring},
  author={Batchelder, William H. and Riefer, David M.},
  journal={Psychological review},
  volume={97},
  number={4},
  pages={548},
  year={1990},
  publisher={American Psychological Association}
}
@article{wickelmaier2018using,
  title={Using recursive partitioning to account for parameter heterogeneity in multinomial processing tree models},
  author={Wickelmaier, Florian and Zeileis, Achim},
  journal={Behavior research methods},
  volume={50},
  number={3},
  pages={1217--1233},
  year={2018},
  publisher={Springer}
}
@unpublished{haines2020learning,
  title={Learning from the Reliability Paradox: {H}ow Theoretically Informed Generative Models Can Advance the Social, Behavioral, and Brain Sciences},
  author={Haines, Nathaniel and Kvam, Peter D. and Irving, Louis H. and Smith, Colin and Beauchaine, Theodore P. and Pitt, Mark A. and Ahn, Woo-Young and Turner, Brandon M},
  year={2020},
  url = {https://ccs-lab.github.io/pdfs/papers/haines2020_reliability.pdf}
}
@Article{Koster2017,
  author     = {Koster, Jeremy and McElreath, Richard},
  title      = {Multinomial analysis of behavior: {Statistical} methods},
  doi        = {10.1007/s00265-017-2363-8},
  issn       = {1432-0762},
  language   = {en},
  number     = {9},
  pages      = {138},
  url        = {https://doi.org/10.1007/s00265-017-2363-8},
  urldate    = {2021-01-31},
  volume     = {71},
  abstract   = {Behavioral ecologists frequently use observational methods, such as instantaneous scan sampling, to record the behavior of animals at discrete moments in time. We develop and apply multilevel, multinomial logistic regression models for analyzing such data. These statistical methods correspond to the multinomial character of the response variable while also accounting for the repeated observations of individuals that characterize behavioral datasets. Correlated random effects potentially reveal individual-level trade-offs across behaviors, allowing for models that reveal the extent to which individuals who regularly engage in one behavior also exhibit relatively more or less of another behavior. Using an example dataset, we demonstrate the estimation of these models using Hamiltonian Monte Carlo algorithms, as implemented in the RStan package in the R statistical environment. The supplemental files include a coding script and data that demonstrate auxiliary functions to prepare the data, estimate the models, summarize the posterior samples, and generate figures that display model predictions. We discuss possible extensions to our approach, including models with random slopes to allow individual-level behavioral strategies to vary over time and the need for models that account for temporal autocorrelation. These models can potentially be applied to a broad class of statistical analyses by behavioral ecologists, focusing on other polytomous response variables, such as behavior, habitat choice, or emotional states.},
  journal    = {Behavioral Ecology and Sociobiology},
  month      = aug,
  shorttitle = {Multinomial analysis of behavior},
  year       = {2017},
}
@article{yarkoni_2020, title={The generalizability crisis}, DOI={10.1017/S0140525X20001685}, journal={Behavioral and Brain Sciences}, publisher={Cambridge University Press}, author={Yarkoni, Tal}, year={2020}, pages={1–37}}


@article{grassi_two_2021,
	title = {Two replications of {Raymond}, {Shapiro}, and {Arnell} (1992), {The} {Attentional} {Blink}},
	volume = {53},
	issn = {1554-3528},
	url = {https://doi.org/10.3758/s13428-020-01457-6},
	doi = {10.3758/s13428-020-01457-6},
	abstract = {In order to improve the trustworthiness of our science, several new research practices have been suggested, including preregistration, large statistical power, availability of research data and materials, new statistical standards, and the replication of experiments. We conducted a replication project on an original phenomenon that was discovered more than 25 years ago, namely the attentional blink (Raymond, Shapiro, \& Arnell, Human Perception and Performance, 18(3), 849–860, 1992), which has been conceptually replicated hundreds of times with major variations. Here, we ran two identical experiments, adopting the new practices and closely reproducing the original experiment. The two experiments were run by different research groups in different countries and laboratories with different participants. Experiment 1 shared remarkable similarities (in magnitude and duration of the effect) with the original study, but also some differences (the overall accuracy of participants, the timing of the effect, and lag-1 sparing). Experts interviewed to evaluate our results stressed the similarities rather than the differences. Experiment 2 replicated nearly identically the results observed in Experiment 1. These findings show that the adoption of new research practices improves the replicability of experimental research and opens the door for a quantitative and direct comparison of the results collected across different laboratories and countries.},
	language = {en},
	number = {2},
	urldate = {2021-06-13},
	journal = {Behavior Research Methods},
	author = {Grassi, Massimo and Crotti, Camilla and Giofrè, David and Boedker, Ingrid and Toffalini, Enrico},
	month = apr,
	year = {2021},
	pages = {656--668}
}

@article{raymond1992temporary,
  title={Temporary suppression of visual processing in an {RSVP} task: {An} attentional blink?},
  author={Raymond, Jane E. and Shapiro, Kimron L. and Arnell, Karen M.},
  journal={Journal of Experimental Psychology: Human Perception and Performance},
  volume={18},
  number={3},
  pages={849},
  year={1992},
  doi = {https://doi.org/10.1037/0096-1523.18.3.849},
  publisher={American Psychological Association}
}
@Article{Broadbent1987,
  author       = {Broadbent, Donald E. and Broadbent, Margaret H. P.},
  date         = {1987},
  journaltitle = {Perception \& Psychophysics},
  title        = {From detection to identification: {Response} to multiple targets in rapid serial visual presentation},
  doi          = {10.3758/BF03210498},
  issn         = {1532-5962},
  language     = {en},
  number       = {2},
  pages        = {105--113},
  volume       = {42},
  abstract     = {In four experiments, words were presented visually at a high rate; as has been found previously, subjects could identify individual target words and must therefore have gathered some information even about the unreportable nontargets. The novel feature of this study was that there were frequently two targets in the list; the occurrence of the first target disrupted identification of the second for a subsequent period of more than half a second. This happened whether the target word was designated by a single physical feature or by the semantic characteristic of belonging to a specified category. The two situations did differ, however, in that unidentified targets of the first type still disturbed an accompanying second target, whereas those of the second type did not. The results are interpreted as meaning that a simple undemanding process of detection triggers other and more demanding processes of identification, so that the occurrence of the latter for one target interferes with their occurrence for another.},
  shorttitle   = {From detection to identification}
}


@article{sivula2020uncertainty,
      title={Uncertainty in Bayesian Leave-One-Out Cross-Validation Based Model Comparison},
      author={Tuomas Sivula and Måns Magnusson and Aki Vehtari},
      year={2020},
      eprint={2008.10296},
      journal={arXiv preprint},
      primaryClass={stat.ME}
}

@article{Piironenetal2020,
author = {Juho Piironen and Markus Paasiniemi and Aki Vehtari},
title = {{Projective inference in high-dimensional problems: Prediction and feature selection}},
volume = {14},
journal = {Electronic Journal of Statistics},
number = {1},
publisher = {Institute of Mathematical Statistics and Bernoulli Society},
pages = {2155 -- 2197},
keywords = {Feature selection, Post-selection inference, prediction, projection, Sparsity},
year = {2020},
doi = {10.1214/20-EJS1711},
URL = {https://doi.org/10.1214/20-EJS1711}
}


@article{Paananen_2021,
   title={Implicitly adaptive importance sampling},
   volume={31},
   ISSN={1573-1375},
   url={http://dx.doi.org/10.1007/s11222-020-09982-2},
   DOI={10.1007/s11222-020-09982-2},
   number={2},
   journal={Statistics and Computing},
   publisher={Springer Science and Business Media LLC},
   author={Paananen, Topi and Piironen, Juho and Bürkner, Paul-Christian and Vehtari, Aki},
   year={2021},
   month={Feb}
}
@article{damasio1992aphasia,
  title={Aphasia},
  author={Damasio, Antonio R},
  journal={New England Journal of Medicine},
  volume={326},
  number={8},
  pages={531--539},
  year={1992},
  doi = {10.1056/NEJM199202203260806},
  publisher={Mass Medical Soc}
}
@article{YackulicEtAl2020,
author = {Yackulic, Charles B. and Dodrill, Michael and Dzul, Maria and Sanderlin, Jamie S. and Reid, Janice A.},
title = {A need for speed in Bayesian population models: a practical guide to marginalizing and recovering discrete latent states},
journal = {Ecological Applications},
volume = {30},
number = {5},
pages = {e02112},
keywords = {augmentation, autologistic, closed conditional, density dependence, forward conditional, fully conditional, hidden Markov model, mark–recapture, N-occupancy, unconditional},
doi = {https://doi.org/10.1002/eap.2112},
url = {https://esajournals.onlinelibrary.wiley.com/doi/abs/10.1002/eap.2112},
eprint = {https://esajournals.onlinelibrary.wiley.com/doi/pdf/10.1002/eap.2112},
abstract = {Abstract Bayesian population models can be exceedingly slow due, in part, to the choice to simulate discrete latent states. Here, we discuss an alternative approach to discrete latent states, marginalization, that forms the basis of maximum likelihood population models and is much faster. Our manuscript has two goals: (1) to introduce readers unfamiliar with marginalization to the concept and provide worked examples and (2) to address topics associated with marginalization that have not been previously synthesized and are relevant to both Bayesian and maximum likelihood models. We begin by explaining marginalization using a Cormack-Jolly-Seber model. Next, we apply marginalization to multistate capture–recapture, community occupancy, and integrated population models and briefly discuss random effects, priors, and pseudo-R2. Then, we focus on recovery of discrete latent states, defining different types of conditional probabilities and showing how quantities such as population abundance or species richness can be estimated in marginalized code. Last, we show that occupancy and site-abundance models with auto-covariates can be fit with marginalized code with minimal impact on parameter estimates. Marginalized code was anywhere from five to >1,000 times faster than discrete code and differences in inferences were minimal. Discrete latent states and fully conditional approaches provide the best estimates of conditional probabilities for a given site or individual. However, estimates for parameters and derived quantities such as species richness and abundance are minimally affected by marginalization. In the case of abundance, marginalized code is both quicker and has lower bias than an N-augmentation approach. Understanding how marginalization works shrinks the divide between Bayesian and maximum likelihood approaches to population models. Some models that have only been presented in a Bayesian framework can easily be fit in maximum likelihood. On the other hand, factors such as informative priors, random effects, or pseudo-R2 values may motivate a Bayesian approach in some applications. An understanding of marginalization allows users to minimize the speed that is sacrificed when switching from a maximum likelihood approach. Widespread application of marginalization in Bayesian population models will facilitate more thorough simulation studies, comparisons of alternative model structures, and faster learning.},
year = {2020}
}
@misc{robert2021raoblackwellization,
      title={Rao-Blackwellization in the MCMC era},
      author={Christian P. Robert and Gareth O. Roberts},
      year={2021},
      eprint={2101.01011},
      archivePrefix={arXiv},
      primaryClass={stat.CO}
}

@misc{pullin2021statistical,
      title={Statistical Models of Repeated Categorical Ratings: The R package rater},
      author={Jeffrey Pullin and Lyle Gurrin and Damjan Vukcevic},
      year={2021},
      eprint={2010.09335},
      archivePrefix={arXiv},
      primaryClass={stat.ME}
}
@ARTICLE{HeathcoteLove2012,
AUTHOR={Heathcote, Andrew and Love, Jonathon},
TITLE={Linear Deterministic Accumulator Models of Simple Choice},
JOURNAL={Frontiers in Psychology},
VOLUME={3},
PAGES={292},
YEAR={2012},
URL={https://www.frontiersin.org/article/10.3389/fpsyg.2012.00292},
DOI={10.3389/fpsyg.2012.00292},
ISSN={1664-1078}

}
@Article{RouderEtAl2015,
  author       = {Rouder, Jeffrey N. and Province, Jordan M. and Morey, Richard D. and Gomez, Pablo and Heathcote, Andrew},
  year         = {2015},
  journal = {Psychometrika},
  title        = {The Lognormal Race: {A} Cognitive-Process Model of Choice and Latency with Desirable Psychometric Properties},
  doi          = {10.1007/s11336-013-9396-3},
  issn         = {1860-0980},
  language     = {en},
  number       = {2},
  pages        = {491--513},
  url          = {https://doi.org/10.1007/s11336-013-9396-3},
  urldate      = {2021-07-17},
  volume       = {80}
}
@Misc{Stan2023,
  Author = {{Stan Development Team}},
  Title = {Stan Modeling Language Users Guide and Reference Manual, version 2.32},
  url ={https://mc-stan.org/docs/2_32/reference-manual/index.html; https://mc-stan.org/docs/2_32/stan-users-guide/index.html},
  Year = {2023}
}
@article{pittWhenGoodFit2002,
  title = {When a Good Fit Can Be Bad},
  author = {Pitt, Mark A. and Myung, In Jae},
  date = {2002-10},
  journaltitle = {Trends in Cognitive Sciences},
  volume = {6},
  pages = {421--425},
  issn = {13646613},
  doi = {10.1016/S1364-6613(02)01964-2},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1364661302019642},
  urldate = {2019-05-17},
  file = {/home/bruno/ownCloud/bib_papers/Pitt_Myung_2002_When a good fit can be bad.pdf},
  langid = {english},
  number = {10}
}
@book{luce1986response,
  title={Response Times: {Their} Role in Inferring Elementary Mental Organization},
  author={Luce, R. Duncan},
  year={1991},
  publisher={Oxford University Press}
}
@Article{Yellott1967,
  author       = {Yellott, John I.},
  date         = {1967-08},
  journaltitle = {Psychonomic Science},
  title        = {Correction for guessing in choice reaction time},
  doi          = {10.3758/BF03331682},
  issn         = {0033-3131},
  language     = {en},
  number       = {8},
  pages        = {321--322},
  url          = {https://doi.org/10.3758/BF03331682},
  urldate      = {2021-07-26},
  volume       = {8},
  abstract     = {Additional theoretical and experimental results are presented for a choice reaction time performance model described by Oilman (1966). A formula is given for estimating the latency distribution of true recognition responses from the results of a single session; the estimate is invariant with respect to changes in the proportion of “guess” responses and with respect to fluctuations in the latency distribution of guesses.}
}
@Article{Yellott1971,
  author       = {Yellott, John I.},
  date         = {1971-05},
  journaltitle = {Journal of Mathematical Psychology},
  title        = {Correction for fast guessing and the speed-accuracy tradeoff in choice reaction time},
  doi          = {10.1016/0022-2496(71)90011-3},
  issn         = {0022-2496},
  language     = {en},
  number       = {2},
  pages        = {159--199},
  url          = {https://www.sciencedirect.com/science/article/pii/0022249671900113},
  urldate      = {2021-07-26},
  volume       = {8},
  abstract     = {In choice reaction time tasks, response latency varies as the subject changes his bias for speed vs accuracy; this is the speed-accuracy tradeoff. Ollman's Fast Guess model provides a mechanism for this tradeoff by allowing the subject to vary his probability of making a guess response rather than a stimulus controlled response (SCR). It is shown that the mean latency of SCR's (μs) in two-choice experiments can be estimated from a single session, regardless of how the subject adjusts his guessing probability. Three experiments are reported in which μs apparently remained virtually constant despite tradeoffs in which accuracy varied from chance to near-perfect. From the standpoint of the Fast Guess model, this result is interpreted to mean that the tradeoff here was produced almost entirely by mixing different proportions of fast guesses and constant (mean) latency SCR's. The final sections of the paper discuss the question of what other models might be compatible with μs invariance.}
}
@ARTICLE{HarrisEtAl2014,
AUTHOR={Harris, Christopher M. and Waddington, Jonathan and Biscione, Valerio and Manzi, Sean},
TITLE={Manual choice reaction times in the rate-domain},
JOURNAL={Frontiers in Human Neuroscience},
VOLUME={8},
PAGES={418},
YEAR={2014},
URL={https://www.frontiersin.org/article/10.3389/fnhum.2014.00418},
DOI={10.3389/fnhum.2014.00418},
ISSN={1662-5161},
ABSTRACT={Over the last 150 years, human manual reaction times (RTs) have been recorded countless times. Yet, our understanding of them remains remarkably poor. RTs are highly variable with positively skewed frequency distributions, often modeled as an inverse Gaussian distribution reflecting a stochastic rise to threshold (diffusion process). However, latency distributions of saccades are very close to the reciprocal Normal, suggesting that “rate” (reciprocal RT) may be the more fundamental variable. We explored whether this phenomenon extends to choice manual RTs. We recorded two-alternative choice RTs from 24 subjects, each with 4 blocks of 200 trials with two task difficulties (easy vs. difficult discrimination) and two instruction sets (urgent vs. accurate). We found that rate distributions were, indeed, very close to Normal, shifting to lower rates with increasing difficulty and accuracy, and for some blocks they appeared to become left-truncated, but still close to Normal. Using autoregressive techniques, we found temporal sequential dependencies for lags of at least 3. We identified a transient and steady-state component in each block. Because rates were Normal, we were able to estimate autoregressive weights using the Box-Jenkins technique, and convert to a moving average model using z-transforms to show explicit dependence on stimulus input. We also found a spatial sequential dependence for the previous 3 lags depending on whether the laterality of previous trials was repeated or alternated. This was partially dissociated from temporal dependency as it only occurred in the easy tasks. We conclude that 2-alternative choice manual RT distributions are close to reciprocal Normal and not the inverse Gaussian. This is not consistent with stochastic rise to threshold models, and we propose a simple optimality model in which reward is maximized to yield to an optimal rate, and hence an optimal time to respond. We discuss how it might be implemented.}
}
@article{Harris2012,
title = {On the convergence of time interval moments: caveat sciscitator},
journal = {Journal of Neuroscience Methods},
volume = {205},
number = {2},
pages = {345-356},
year = {2012},
issn = {0165-0270},
doi = {https://doi.org/10.1016/j.jneumeth.2012.01.017},
url = {https://www.sciencedirect.com/science/article/pii/S0165027012000325},
author = {Christopher M. Harris and Jonathan Waddington},
keywords = {Reaction time, Saccade latency, Optokinetic nystagmus, Inter-spike intervals, Firing rate, Moment convergence, Partial moments},
abstract = {The frequency distributions of biological time-intervals (TIs) have been measured in innumerable behavioural and neurophysiological studies including, for example, reaction time experiments and studies of neuronal spike timing. Regardless of context, TI distributions tend to be significantly positively skewed, and many studies have attempted to characterise these distributions by their mean or higher moments (variance, skewness, and kurtosis). It is, however, not widely appreciated in the neural/behavioural literature that highly skewed distributions may not have moments: they may not converge to a finite value. For example, the reciprocal Normal distribution, frequently used as a model of saccade latency and manual reaction time, does not have a finite mean or higher moments. We explore this non-trivial phenomenon. We introduce ‘cumulative’ moments and show that moment convergence can be easily related to the distribution of the reciprocal of time-intervals (rate). The behaviour of the rate distribution near zero determines which moments in the time domain converge. Non-convergence may be very slow leading to the appearance of convergence, but after infinite time they become infinite (pseudo-convergence). Experiments take place in finite time and cannot reconcile pseudo-convergence. Nevertheless, cumulative sample moments can provide insight into the convergence of the parent moments, but this depends on sample size. We illustrate convergence issues with three empirical examples: manual reaction times, neural inter-spike intervals, and optokinetic inter-saccadic intervals. We conclude that estimating moments of skewed TI distributions is at best questionable and propose that rate (reciprocal TI) usually (but not always) provides more information.}
}

@inproceedings{belin1990,
  title={Analysis of a finite mixture model with variance components},
  author={Belin, T.R. and Rubin, Donald B.},
  booktitle={Proceedings of the Social Statistics Section},
  pages={211--215},
  year={1990}
}

@article{McGill1965,
title = {The general-gamma distribution and reaction times},
journal = {Journal of Mathematical Psychology},
volume = {2},
number = {1},
pages = {1-18},
year = {1965},
issn = {0022-2496},
doi = {https://doi.org/10.1016/0022-2496(65)90014-3},
url = {https://www.sciencedirect.com/science/article/pii/0022249665900143},
author = {William J. McGill and John Gibbon},
abstract = {The general-gamma distribution describes input-output times in a multistage process consisting of exponential components whose constants are all different. The distribution and its unique history are examined. A stochastic process that leads to it is presented. The conditional density (hazard) function is studied as a means for estimating parameters. Finally, the multistage process model is applied to simple reaction times in an effort to reveal underlying detection and response components.}
}

@article{ashby1980decomposing,
  title={Decomposing the reaction time distribution: {Pure} insertion and selective influence revisited},
  author={Ashby, F. Gregory and Townsend, James T.},
  journal={Journal of Mathematical Psychology},
  volume={21},
  number={2},
  pages={93--123},
  year={1980},
  publisher={Elsevier}
}
@article{ashby1982testing,
  title={Testing the assumptions of exponential, additive reaction time models},
  author={Ashby, F. Gregory},
  journal={Memory \& Cognition},
  volume={10},
  number={2},
  pages={125--134},
  year={1982},
  publisher={Springer}
}
@article{keuleers2012british,
  title={The {British Lexicon Project}: {Lexical} decision data for 28,730 monosyllabic and disyllabic English words},
  author={Keuleers, Emmanuel and Lacey, Paula and Rastle, Kathleen and Brysbaert, Marc},
  journal={Behavior Research Methods},
  volume={44},
  number={1},
  pages={287--304},
  doi = {https://doi.org/10.3758/s13428-011-0118-4},
  year={2012},
  publisher={Springer}
}
@article{brown2005ballistic,
  title={A ballistic model of choice response time},
  author={Brown, Scott D. and Heathcote, Andrew},
  journal={Psychological review},
  volume={112},
  number={1},
  pages={117},
  year={2005},
  publisher={American Psychological Association}
}
@article{heathcote2019dynamic,
  title={Dynamic models of choice},
  author={Heathcote, Andrew and Lin, Yi-Shin and Reynolds, Angus and Strickland, Luke and Gretton, Matthew and Matzke, Dora},
  journal={Behavior Research Methods},
  volume={51},
  number={2},
  pages={961--985},
  doi = {https://doi.org/10.3758/s13428-018-1067-y},
  year={2019},
  publisher={Springer}
}
@article{turner2013method,
  title={A method for efficiently sampling from distributions with correlated dimensions},
  author={Turner, Brandon M and Sederberg, Per B. and Brown, Scott D. and Steyvers, Mark},
  journal={Psychological Methods},
  volume={18},
  number={3},
  pages={368},
  year={2013},
  publisher={American Psychological Association}
}
@article {WilsonEtAl2019,
article_type = {journal},
title = {Ten simple rules for the computational modeling of behavioral data},
author = {Wilson, Robert C. and Collins, Anne G.E.},
editor = {Behrens, Timothy E},
volume = 8,
year = 2019,
month = {nov},
pub_date = {2019-11-26},
pages = {e49547},
citation = {eLife 2019;8:e49547},
doi = {10.7554/eLife.49547},
url = {https://doi.org/10.7554/eLife.49547},
abstract = {Computational modeling of behavior has revolutionized psychology and neuroscience. By fitting models to experimental data we can probe the algorithms underlying behavior, find neural correlates of computational variables and better understand the effects of drugs, illness and interventions. But with great power comes great responsibility. Here, we offer ten simple rules to ensure that computational modeling is used with care and yields meaningful insights. In particular, we present a beginner-friendly, pragmatic and details-oriented introduction on how to relate models to data. What, exactly, can a model tell us about the mind? To answer this, we apply our rules to the simplest modeling techniques most accessible to beginning modelers and illustrate them with examples and code available online. However, most rules apply to more advanced techniques. Our hope is that by following our guidelines, researchers will avoid many pitfalls and unleash the power of computational modeling on their own data.},
keywords = {computational modeling, model fitting, validation, reproducibility},
journal = {eLife},
issn = {2050-084X},
publisher = {eLife Sciences Publications, Ltd},
}

@article{BrysbaertEtAl2018,
author = {Marc Brysbaert and Paweł Mandera and Emmanuel Keuleers},
title ={The Word Frequency Effect in Word Processing: An Updated Review},
journal = {Current Directions in Psychological Science},
volume = {27},
number = {1},
pages = {45-50},
year = {2018},
doi = {10.1177/0963721417727521},

URL = {
        https://doi.org/10.1177/0963721417727521

},
eprint = {
        https://doi.org/10.1177/0963721417727521

}
,
    abstract = { The word frequency effect refers to the observation that high-frequency words are processed more efficiently than low-frequency words. Although the effect was first described over 80 years ago, in recent years it has been investigated in more detail. It has become clear that considerable quality differences exist between frequency estimates and that we need a new standardized frequency measure that does not mislead users. Research also points to consistent individual differences in the word frequency effect, meaning that the effect will be present at different word frequency ranges for people with different degrees of language exposure. Finally, a few ongoing developments point to the importance of semantic diversity rather than mere differences in the number of times words have been encountered and to the importance of taking into account word prevalence in addition to word frequency. }
}
@article{Nelson1981,
author = { Peter R.   Nelson },
title = {The Algebra of Random Variables},
journal = {Technometrics},
volume = {23},
number = {2},
pages = {197-198},
year  = {1981},
publisher = {Taylor & Francis},
doi = {10.1080/00401706.1981.10486266},

URL = {
        https://www.tandfonline.com/doi/abs/10.1080/00401706.1981.10486266

},
eprint = {
        https://www.tandfonline.com/doi/pdf/10.1080/00401706.1981.10486266

}

}
@misc{balota1994visual,
  title={Visual word recognition},
  author={Balota, David A},
  journal={Handbook of psycholinguistics},
  pages={303--358},
  year={1994},
  publisher={Academic Press San Diego, CA}
}

@article{ratcliff2002estimating,
  title={Estimating parameters of the diffusion model: Approaches to dealing with contaminant reaction times and parameter variability},
  author={Ratcliff, Roger and Tuerlinckx, Francis},
  journal={Psychonomic Bulletin \& Review},
  volume={9},
  number={3},
  pages={438--481},
  year={2002},
  doi = {https://doi.org/10.3758/BF03196302},
  publisher={Springer}
}

@article{audley1965some,
  title={Some alternative stochastic models of choice 1},
  author={Audley, R.J. and Pike, A.R.},
  journal={British Journal of Mathematical and Statistical Psychology},
  volume={18},
  number={2},
  pages={207--225},
  year={1965},
  publisher={Wiley Online Library}
}
@article{dufau2012say,
  title={How to say “no” to a nonword: {A} leaky competing accumulator model of lexical decision},
  author={Dufau, St{\'e}phane and Grainger, Jonathan and Ziegler, Johannes C.},
  journal={Journal of Experimental Psychology: Learning, Memory, and Cognition},
  volume={38},
  number={4},
  doi = {https://doi.org/10.1037/a0026948},
  pages={1117},
  year={2012},
  publisher={American Psychological Association}
}

@ARTICLE{Goldfarb2014,

AUTHOR={Goldfarb, Stephanie and Leonard, Naomi E. and Simen, Patrick and Caicedo-Núñez, Carlos H. and Holmes, Philip},

TITLE={A comparative study of drift diffusion and linear ballistic accumulator models in a reward maximization perceptual choice task},

JOURNAL={Frontiers in Neuroscience},

VOLUME={8},

PAGES={148},

YEAR={2014},

URL={https://www.frontiersin.org/article/10.3389/fnins.2014.00148},

DOI={10.3389/fnins.2014.00148},

ISSN={1662-453X},

ABSTRACT={We present new findings that distinguish drift diffusion models (DDMs) from the linear ballistic accumulator (LBA) model as descriptions of human behavior in a two-alternative forced-choice reward maximization (Rmax) task. Previous comparisons have not considered Rmax tasks, and differences identified between the models' predictions have centered on practice effects. Unlike the parameter-free optimal performance curves of the pure DDM, the extended DDM and LBA predict families of curves depending on their additional parameters, and those of the LBA show significant differences from the DDMs, especially for poorly discriminable stimuli that incur high error rates. Moreover, fits to behavior reveal that the LBA and DDM provide different interpretations of behavior as stimulus discriminability increases. Trends for threshold setting (caution) in the DDMs are consistent between fits, while in the corresponding LBA fits, thresholds interact with distributions of starting points in a complex manner that depends upon parameter constraints. Our results suggest that reinterpretation of LBA parameters may be necessary in modeling the Rmax paradigm.}
}
@misc{evans_2020,
 title={Same model, different conclusions: An identifiability issue in the linear ballistic accumulator model of decision-making},
 url={psyarxiv.com/2xu7f},
 DOI={10.31234/osf.io/2xu7f},
 publisher={PsyArXiv},
 author={Evans, Nathan J.},
 year={2020},
 month={Jun}
}

@article{clark1994identification,
  title={Identification of early visual evoked potential generators by retinotopic and topographic analyses},
  author={Clark, Vincent P. and Fan, Silu and Hillyard, Steven A.},
  journal={Human brain mapping},
  volume={2},
  number={3},
  pages={170--187},
  year={1994},
  publisher={Wiley Online Library}
}
@article{PiironenVehtari2017,
author = {Juho Piironen and Aki Vehtari},
title = {Sparsity information and regularization in the horseshoe and other shrinkage priors},
volume = {11},
journal = {Electronic Journal of Statistics},
number = {2},
publisher = {Institute of Mathematical Statistics and Bernoulli Society},
pages = {5018 -- 5051},
keywords = {Bayesian inference, horseshoe prior, shrinkage priors, Sparse estimation},
year = {2017},
doi = {10.1214/17-EJS1337SI},
URL = {https://doi.org/10.1214/17-EJS1337SI}
}
@inproceedings{yao2018yes,
  title={Yes, but did it work?: {Evaluating} variational inference},
  author={Yao, Yuling and Vehtari, Aki and Simpson, Daniel P. and Gelman, Andrew},
  booktitle={International Conference on Machine Learning},
  pages={5581--5590},
  year={2018},
  organization={PMLR}
}
@article{sailynoja2022graphical,
  title={Graphical test for discrete uniformity and its applications in goodness-of-fit evaluation and multiple sample comparison},
  author={Säilynoja, Teemu and Bürkner, Paul-Christian and Vehtari, Aki},
  journal={Statistics and Computing},
  volume={32},
  number={2},
  pages={1--21},
  year={2022},
  doi = {https://doi.org/10.1007/s11222-022-10090-6},
  publisher={Springer}
}

@article{Klaueretal2018,
title = {{RT-MPTs}: {Process} models for response-time distributions based on multinomial processing trees with applications to recognition memory},
journal = {Journal of Mathematical Psychology},
volume = {82},
pages = {111-130},
year = {2018},
issn = {0022-2496},
doi = {https://doi.org/10.1016/j.jmp.2017.12.003},
url = {https://www.sciencedirect.com/science/article/pii/S0022249617301980},
author = {Karl Christoph Klauer and David Kellen},
keywords = {Multinomial models, Response times, Hierarchical models},
abstract = {Multinomial processing tree models have been widely used for characterizing categorical responses in terms of a finite set of discrete latent states, and a number of processes arranged serially in a processing tree. We extend the scope of this model class by proposing a method for incorporating response times. This extension enables the estimation of the completion times of each process and the testing of alternative process orderings. In line with previous developments, the proposed method is hierarchical and implemented using Bayesian methods. We apply our method to the two-high-threshold model of recognition memory, using previously published data. The results provide interesting insights into the ordering of memory-retrieval and guessing processes and show that the model performs at least as well as established benchmarks such as the diffusion model.}
}

@article{hartmann2020rtmpt,
  title={{rtmpt}: An {R} package for fitting response-time extended multinomial processing tree models},
  author={Hartmann, Raphael and Johannsen, Lea and Klauer, Karl Christoph},
  journal={Behavior Research Methods},
  volume={52},
  number={3},
  pages={1313--1338},
  year={2020},
  publisher={Springer}
}
@article{heathcote2004fitting,
  title={Fitting {Wald} and {ex-Wald} distributions to response time data: {An} example using functions for the S-PLUS package},
  author={Heathcote, Andrew},
  journal={Behavior Research Methods, Instruments, \& Computers},
  volume={36},
  number={4},
  pages={678--694},
  doi = {https://doi.org/10.3758/BF03206550},
  year={2004},
  publisher={Springer}
}


@book{steyer2017probability,
  title={Probability and conditional expectation: Fundamentals for the empirical sciences},
  author={Steyer, Rolf and Nagel, Werner},
  volume={5},
  year={2017},
  publisher={John Wiley \& Sons}
}


@article{conway2005working,
  title={Working memory span tasks: {A} methodological review and user’s guide},
  author={Conway, Andrew R.A. and Kane, Michael J. and Bunting, Michael F. and Hambrick, D. Zach and Wilhelm, Oliver and Engle, Randall W.},
  journal={Psychonomic bulletin \& Review},
  volume={12},
  number={5},
  pages={769--786},
  year={2005},
  publisher={Springer}
}

@article{DenklaRudel1976,
  title={Rapid ‘automatized’ naming ({R.A.N.}): {Dyslexia} differentiated from other learning disabilities},
  author={Denckla, Martha Bridge and Rudel, Rita G.},
  journal={Neuropsychologia},
  volume={14},
  number={4},
  doi = {https://doi.org/10.1016/0028-3932(76)90075-0},
  pages={471--479},
  year={1976},
  publisher={Elsevier}
}


@article{GelmanEtAl2008,
author = {Andrew Gelman and Aleks Jakulin and Maria Grazia Pittau and Yu-Sung Su},
title = {A weakly informative default prior distribution for logistic and other regression models},
volume = {2},
journal = {The Annals of Applied Statistics},
number = {4},
publisher = {Institute of Mathematical Statistics},
pages = {1360 -- 1383},
keywords = {Bayesian inference, generalized linear model, hierarchical model, least squares, Linear regression, logistic regression, multilevel model, noninformative prior distribution, weakly informative prior distribution},
year = {2008},
doi = {10.1214/08-AOAS191},
URL = {https://doi.org/10.1214/08-AOAS191}
}


@book{johnson1995continuous,
title={Continuous univariate distributions, volume 2},
author={Johnson, Norman L. and Kotz, Samuel and Balakrishnan, Narayanaswamy},
volume={289},
year={1995},
publisher={John Wiley & Sons}
}


@periodical{mathpsych2011,
  editor = {Lee, Michael D.},
  year = {2011},
  title = {Journal of Mathematical Psychology},
  issuetitle = {Special Issue on Hierarchical {Bayesian} Models},
  volume = {55},
  number = {1},
  url = {https://www.sciencedirect.com/journal/journal-of-mathematical-psychology/vol/55/issue/1},
}
@book{luce1986response,
  title={Response times: {Their} role in inferring elementary mental organization},
  author={Luce, R. Duncan and others},
  number={8},
  year={1986},
  publisher={Oxford University Press on Demand}
}


@misc{FAQCV,
author = {Vehtari, Aki},
year = {2022},
  title = {{Cross-validation FAQ}},
  howpublished = {\url{https://web.archive.org/web/20221219223947/https://avehtari.github.io/modelselection/CV-FAQ.html}},
  note = {Accessed: 2022-12-19}
}

@Article{hubel2013computerized,
title={Computerized measures of finger tapping: {Effects} of hand dominance, age, and sex},
author={Hubel, Kerry A. and Reed, Bruce and Yund, E. William and Herron, Timothy J. and Woods, David L.},
journal={Perceptual and Motor Skills},
volume={116},
number={3},
pages={929--952},
year={2013},
doi = {https://doi.org/10.2466/25.29.PMS.116.3.929-952},
publisher={SAGE Publications Sage},
address = {Los Angeles, CA}
}
@article{newall2023evaluation,
  title={Evaluation of the 'take time to think' safer gambling message: {A} randomised, online experimental study},
  author={Newall, Philip W.S. and Hayes, Taylor R. and Singmann, Henrik and Weiss-Cohen, Leonardo and Ludvig, Elliot A. and Walasek, Lukasz},
  journal={Behavioural Public Policy},
  pages={1--18},
  year={2023},
  doi = {10.1017/bpp.2023.2},
  publisher={Cambridge University Press}
}


@article{betancourt2013hamiltonian,
  title={Hamiltonian Monte Carlo for hierarchical models},
  author={Betancourt, Michael J. and Girolami, Mark},
  journal={Current trends in Bayesian methodology with applications},
  volume={79},
  number={30},
  pages={2--4},
  year={2015},
  publisher={CRC Press Boca Raton, FL}
}
@article{ModrakEtAl2023,
author = {Martin Modr{\'a}k and Angie H. Moon and Shinyoung Kim and Paul B{\"u}rkner and Niko Huurre and Kateřina Faltejskov{\'a} and Andrew Gelman and Aki Vehtari},
title = {Simulation-Based Calibration Checking for {Bayesian} Computation: {The} Choice of Test Quantities Shapes Sensitivity},
journal = {Bayesian Analysis},
publisher = {International Society for Bayesian Analysis},
pages = {1 -- 28},
keywords = {Calibration, probabilistic programming, Software testing},
year = {2023},
doi = {10.1214/23-BA1404},
URL = {https://doi.org/10.1214/23-BA1404}
}
